{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO9kyTII9NjtZlrkDkY4OuL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hmehryar/Hm.JetscapeMl/blob/ml5/jet_ml_pointnet_classifier_all_datasets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hk_dCbkKm-4k",
        "outputId": "3df33511-d60f-45d3-df19-e4918a7de76f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import sys\n",
        "sys.path.insert(1,'/wsu/home/gy/gy40/gy4065/hm.jetscapeml.source')\n",
        "sys.path.insert(1,'/content/drive/MyDrive/Projects/110_JetscapeMl/hm.jetscapeml.source')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade keras tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_o3efLKxrkUq",
        "outputId": "4fbb021f-d1ad-45a6-f0a9-1a3e6fa59148"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Collecting keras\n",
            "  Downloading keras-3.0.2-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Collecting tensorflow\n",
            "  Downloading tensorflow-2.15.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (475.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m475.2/475.2 MB\u001b[0m \u001b[31m707.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras) (1.23.5)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras) (13.7.0)\n",
            "Collecting namex (from keras)\n",
            "  Downloading namex-0.0.7-py3-none-any.whl (5.8 kB)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras) (3.9.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from keras) (0.1.8)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.35.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.60.0)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.42.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.5.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
            "Installing collected packages: tensorflow\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.15.0\n",
            "    Uninstalling tensorflow-2.15.0:\n",
            "      Successfully uninstalled tensorflow-2.15.0\n",
            "Successfully installed tensorflow-2.15.0.post1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# loading libraries\n",
        "import jet_ml_dataset_builder.jet_ml_dataset_builder_utilities as util\n",
        "from jet_ml_dataset_builder.jet_ml_dataset_builder_utilities import set_directory_paths\n",
        "from jet_ml_dataset_builder.jet_ml_dataset_builder_utilities import parse_parameters\n",
        "from jet_ml_dataset_builder.jet_ml_dataset_builder_utilities import load_dataset\n",
        "from jet_ml_dataset_builder.jet_ml_dataset_builder_utilities import install\n",
        "install(\"trimesh\")\n",
        "import os\n",
        "import glob\n",
        "import trimesh\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "tf.random.set_seed(1234)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V5FNXhmJqtdZ",
        "outputId": "e6266a32-3cb8-4a61-cc4d-1695d308e9cd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing pickle5\n",
            "Installed pickle5\n",
            "\n",
            "Installing trimesh\n",
            "Installed trimesh\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Loading/Installing Package => Begin\\n\\n')\n",
        "\n",
        "import jet_ml_dataset_builder.jet_ml_dataset_builder_utilities as util\n",
        "\n",
        "print('\\n########################################################################')\n",
        "print('Checking the running platforms\\n')\n",
        "\n",
        "from jet_ml_dataset_builder.jet_ml_dataset_builder_utilities import set_directory_paths\n",
        "# Call the function and retrieve the dataset_directory_path and simulation_directory_path\n",
        "dataset_directory_path, simulation_directory_path = set_directory_paths()\n",
        "\n",
        "# Access the dataset_directory_path and simulation_directory_path\n",
        "print(\"Dataset Directory Path:\", dataset_directory_path)\n",
        "print(\"Simulation Directory Path:\", simulation_directory_path)\n",
        "print('########################################################################\\n')\n",
        "\n",
        "\n",
        "print('\\nLoading/Installing Package => End\\n\\n')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IrDIuS_1sqb6",
        "outputId": "f73cd3eb-387d-4efb-9a1e-f1582c5b4fd1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading/Installing Package => Begin\n",
            "\n",
            "\n",
            "\n",
            "########################################################################\n",
            "Checking the running platforms\n",
            "\n",
            "Python version: 3.10.12\n",
            "OS: Linux\n",
            "OS version: 6.1.58+\n",
            "running on Colab: True\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Dataset Directory Path: /content/drive/My Drive/Projects/110_JetscapeMl/hm.jetscapeml.data/\n",
            "Simulation Results Path: /content/drive/My Drive/Projects/110_JetscapeMl/hm.jetscapeml.data/simulation_results/\n",
            "Dataset Directory Path: /content/drive/My Drive/Projects/110_JetscapeMl/hm.jetscapeml.data/\n",
            "Simulation Directory Path: /content/drive/My Drive/Projects/110_JetscapeMl/hm.jetscapeml.data/simulation_results/\n",
            "########################################################################\n",
            "\n",
            "\n",
            "Loading/Installing Package => End\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from jet_ml_dataset_builder.jet_ml_dataset_builder_utilities import  parse_parameters\n",
        "\n",
        "# Call the function and retrieve the tokenized parameters\n",
        "tokenized_arguments, tokenized_values = parse_parameters()\n",
        "\n",
        "# Access the tokenized arguments and values\n",
        "print(\"Tokenized Arguments:\")\n",
        "for argument in tokenized_arguments:\n",
        "    print(argument)\n",
        "\n",
        "print(\"\\nTokenized Values:\")\n",
        "for argument, value in tokenized_values.items():\n",
        "    print(f\"{argument}: {value}\")\n",
        "\n",
        "y_class_label_items=['MMAT','MLBT']\n",
        "alpha_s_items=[0.2 ,0.3 ,0.4]\n",
        "q0_items=[1.5 ,2.0 ,2.5]\n",
        "\n",
        "print(\"y_class_label_items:\",y_class_label_items)\n",
        "print(\"alpha_s_items:\",alpha_s_items)\n",
        "print(\"q0_items:\",q0_items)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jX15cxNSs6o3",
        "outputId": "9e5198c5-b9c6-4f9b-b070-640d35ff6cfa"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "option -f not recognized\n",
            "Tokenized Arguments:\n",
            "\n",
            "Tokenized Values:\n",
            "y_class_label_items: ['MMAT', 'MLBT']\n",
            "alpha_s_items: [0.2, 0.3, 0.4]\n",
            "q0_items: [1.5, 2.0, 2.5]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Building required params for the loading the dataset file\")\n",
        "\n",
        "class_labels_str = '_'.join(y_class_label_items)\n",
        "alpha_s_items_str='_'.join(map(str, alpha_s_items))\n",
        "q0_items_str='_'.join(map(str, q0_items))\n",
        "total_size=9*1200000\n",
        "# for shuffled_y_processed\n",
        "# dataset_file_name = f\"jet_ml_benchmark_config_01_to_09_alpha_{alpha_s_items_str}_q0_{q0_items_str}_{class_labels_str}_size_{total_size}_split_train_datasets/train_split_0.pkl\"\n",
        "# for shuffled\n",
        "dataset_file_name = f\"jet_ml_benchmark_config_01_to_09_alpha_{alpha_s_items_str}_q0_{q0_items_str}_{class_labels_str}_size_{1000}_shuffled.pkl\"\n",
        "# dataset_file_name = f\"jet_ml_benchmark_config_01_to_09_alpha_{alpha_s_items_str}_q0_{q0_items_str}_{class_labels_str}_size_{10000}_shuffled.pkl\"\n",
        "# dataset_file_name = f\"jet_ml_benchmark_config_01_to_09_alpha_{alpha_s_items_str}_q0_{q0_items_str}_{class_labels_str}_size_{100000}_shuffled.pkl\"\n",
        "# dataset_file_name = f\"jet_ml_benchmark_config_01_to_09_alpha_{alpha_s_items_str}_q0_{q0_items_str}_{class_labels_str}_size_{1000000}_shuffled.pkl\"\n",
        "# dataset_file_name = f\"jet_ml_benchmark_config_01_to_09_alpha_{alpha_s_items_str}_q0_{q0_items_str}_{class_labels_str}_size_{total_size}_shuffled.pkl\"\n",
        "\n",
        "dataset_file_name=simulation_directory_path+dataset_file_name\n",
        "print(\"dataset_file_name:\",dataset_file_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aTojIHZMtGcg",
        "outputId": "6930034d-7967-4ed7-c56d-e9f5b29c84ad"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building required params for the loading the dataset file\n",
            "dataset_file_name: /content/drive/My Drive/Projects/110_JetscapeMl/hm.jetscapeml.data/simulation_results/jet_ml_benchmark_config_01_to_09_alpha_0.2_0.3_0.4_q0_1.5_2.0_2.5_MMAT_MLBT_size_1000_shuffled.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# loading dataset by size and getting just the first column\n",
        "\n",
        "from jet_ml_dataset_builder.jet_ml_dataset_builder_utilities import load_dataset\n",
        "# Function to load datasets of different sizes\n",
        "def get_dataset(size):\n",
        "    dataset_file_name = f\"jet_ml_benchmark_config_01_to_09_alpha_{alpha_s_items_str}_q0_{q0_items_str}_{class_labels_str}_size_{size}_shuffled.pkl\"\n",
        "\n",
        "    dataset_file_name=simulation_directory_path+dataset_file_name\n",
        "    print(\"dataset_file_name:\",dataset_file_name)\n",
        "\n",
        "    dataset=load_dataset(dataset_file_name,has_test=False)\n",
        "    (dataset_x, dataset_y) = dataset\n",
        "    # Extract the first column for binary classification\n",
        "    dataset_y = dataset_y[:, 0]\n",
        "    print(\"dataset.x:\",type(dataset_x), dataset_x.size, dataset_x.shape)\n",
        "    print(\"dataset.y:\",type(dataset_y), dataset_y.size,dataset_y.shape)\n",
        "    return dataset_x, dataset_y"
      ],
      "metadata": {
        "id": "VUyAMovFsgDO"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from time import time\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, History\n",
        "import json\n",
        "\n",
        "def conv_bn(x, filters):\n",
        "    x = layers.Conv1D(filters, kernel_size=1, padding=\"valid\")(x)\n",
        "    x = layers.BatchNormalization(momentum=0.0)(x)\n",
        "    return layers.Activation(\"relu\")(x)\n",
        "\n",
        "\n",
        "def dense_bn(x, filters):\n",
        "    x = layers.Dense(filters)(x)\n",
        "    x = layers.BatchNormalization(momentum=0.0)(x)\n",
        "    return layers.Activation(\"relu\")(x)\n",
        "\n",
        "\n",
        "class OrthogonalRegularizer(keras.regularizers.Regularizer):\n",
        "    def __init__(self, num_features, l2reg=0.001):\n",
        "        self.num_features = num_features\n",
        "        self.l2reg = l2reg\n",
        "        self.eye = tf.eye(num_features)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        x = tf.reshape(x, (-1, self.num_features, self.num_features))\n",
        "        xxt = tf.tensordot(x, x, axes=(2, 2))\n",
        "        xxt = tf.reshape(xxt, (-1, self.num_features, self.num_features))\n",
        "        return tf.reduce_sum(self.l2reg * tf.square(xxt - self.eye))\n",
        "\n",
        "    def get_config(self):\n",
        "        return {'num_features': self.num_features, 'l2reg': self.l2reg}\n",
        "\n",
        "def tnet(inputs, num_features):\n",
        "\n",
        "    # Initalise bias as the indentity matrix\n",
        "    bias = keras.initializers.Constant(np.eye(num_features).flatten())\n",
        "    reg = OrthogonalRegularizer(num_features)\n",
        "\n",
        "    x = conv_bn(inputs, 32)\n",
        "    x = conv_bn(x, 64)\n",
        "    x = conv_bn(x, 512)\n",
        "    x = layers.GlobalMaxPooling1D()(x)\n",
        "    x = dense_bn(x, 256)\n",
        "    x = dense_bn(x, 128)\n",
        "    x = layers.Dense(\n",
        "        num_features * num_features,\n",
        "        kernel_initializer=\"zeros\",\n",
        "        bias_initializer=bias,\n",
        "        activity_regularizer=reg,\n",
        "    )(x)\n",
        "    feat_T = layers.Reshape((num_features, num_features))(x)\n",
        "    # Apply affine transformation to input features\n",
        "    return layers.Dot(axes=(2, 1))([inputs, feat_T])\n",
        "\n",
        "def build_pointnet_binary_classifier_model(NUM_POINTS,NUM_CLASSES):\n",
        "    inputs = keras.Input(shape=(NUM_POINTS, 3))\n",
        "    x = tnet(inputs, 3)\n",
        "    x = conv_bn(x, 32)\n",
        "    x = conv_bn(x, 32)\n",
        "    x = tnet(x, 32)\n",
        "    x = conv_bn(x, 32)\n",
        "    x = conv_bn(x, 64)\n",
        "    x = conv_bn(x, 512)\n",
        "    x = layers.GlobalMaxPooling1D()(x)\n",
        "    x = dense_bn(x, 256)\n",
        "    x = layers.Dropout(0.3)(x)\n",
        "    x = dense_bn(x, 128)\n",
        "    x = layers.Dropout(0.3)(x)\n",
        "\n",
        "    # outputs = layers.Dense(NUM_CLASSES, activation=\"softmax\")(x)\n",
        "    outputs = layers.Dense(NUM_CLASSES, activation=\"sigmoid\")(x)\n",
        "\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs, name=\"pointnet\")\n",
        "    model.summary()\n",
        "    return model"
      ],
      "metadata": {
        "id": "7wlkSWaHks0z"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# NUM_POINTS = 2048\n",
        "NUM_POINTS = 1024\n",
        "# NUM_CLASSES = 10\n",
        "NUM_CLASSES = 2\n",
        "BATCH_SIZE = 32\n",
        "model=build_pointnet_binary_classifier_model(NUM_POINTS,NUM_CLASSES)\n",
        "\n",
        "learning_rate=0.001\n",
        "monitor='val_loss' #'val_accuracy' or 'val_loss'\n",
        "n_epochs=20\n",
        "\n",
        "best_model_file_path = simulation_directory_path+f'jetml_pointnet_classification_eloss_{class_labels_str}_size_{1000}_best_model.keras'\n",
        "# if not path.exists(best_model_file_path):\n",
        "#     makedirs(best_model_file_path)\n",
        "print('File Path to Save Best Model : {}'.format(best_model_file_path))"
      ],
      "metadata": {
        "id": "xveklusUk1T9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def compile_pointnet_binary_classifier_model_with_hyperparam(model,learning_rate,monitor,best_model_file_path):\n",
        "  optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "\n",
        "  model.compile(\n",
        "      # loss=\"sparse_categorical_crossentropy\",\n",
        "      loss=\"binary_crossentropy\",\n",
        "      optimizer=optimizer,\n",
        "      # metrics=[\"sparse_categorical_accuracy\"],\n",
        "      metrics=[\"accuracy\"],\n",
        "  )\n",
        "  return model"
      ],
      "metadata": {
        "id": "UgTjO5munWUX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pointnet=compile_pointnet_binary_classifier_model_with_hyperparam(model,learning_rate,monitor,best_model_file_path)"
      ],
      "metadata": {
        "id": "o6sK2bSktpOO"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# defining dataset sizes and classifiers\n",
        "\n",
        "# Sizes of datasets\n",
        "# dataset_sizes = [1000]\n",
        "# dataset_sizes = [1000, 10000]\n",
        "dataset_sizes = [1000, 10000, 100000, 1000000]\n",
        "\n",
        "\n",
        "# Classifiers\n",
        "classifiers = {\n",
        "    'Pointnet': pointnet,\n",
        "}"
      ],
      "metadata": {
        "id": "hXGQAvpJtN8t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from time import time\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, History\n",
        "import json\n",
        "\n",
        "\n",
        "# Function to train and evaluate classifiers\n",
        "def train_and_evaluate_classifier(model, x_train, y_train, x_test, y_test,n_epochs):\n",
        "    # Assuming x and y are defined\n",
        "    # x should be a 2D array (e.g., (1000, 32*32))\n",
        "    # y should be a 2D array with three columns (e.g., (1000, 3))\n",
        "\n",
        "    # Flatten the 32x32 images to 1D arrays for LogisticRegression, DecisionTreeClassifier, LinearSVM, KNN, RandomForests\n",
        "    x_train_flatten = x_train.reshape(x_train.shape[0], -1)\n",
        "    x_test_flatten = x_test.reshape(x_test.shape[0], -1)\n",
        "\n",
        "    # Include both ModelCheckpoint and History callbacks in the callbacks list\n",
        "    # callbacks=[checkpoint_callback]\n",
        "    # Use ModelCheckpoint callback to save the best model\n",
        "    checkpoint_callback = ModelCheckpoint(\n",
        "        best_model_file_path,\n",
        "        monitor=monitor,\n",
        "        save_best_only=True,\n",
        "        model='min',\n",
        "        # mode=\"max\" if monitor == \"val_accuracy\" else \"min\",\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # Use History callback to retrieve the training history\n",
        "    history_callback = History()\n",
        "\n",
        "    start = time()\n",
        "    history=model.fit(train_points,\n",
        "              y_train_categorical_encoded,\n",
        "              epochs=n_epochs,\n",
        "              validation_data=(test_points,y_test_categorical_encoded),\n",
        "              callbacks=[checkpoint_callback]\n",
        "              )\n",
        "    train_time = (time()-start)/60.0\n",
        "    y_pred = model.predict(x_test_flatten)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    return history, accuracy, cm"
      ],
      "metadata": {
        "id": "a70gqMr_rYyA"
      },
      "execution_count": 11,
      "outputs": []
    }
  ]
}