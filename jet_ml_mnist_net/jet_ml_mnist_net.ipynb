{"cells":[{"cell_type":"code","execution_count":null,"id":"b77cdc04","metadata":{},"outputs":[],"source":["#!/usr/bin/env python\n","# coding: utf-8"]},{"cell_type":"markdown","id":"ab82510c","metadata":{},"source":["## Part 0: Prerequisites:\n","\n","We recommend that you run this this notebook in the cloud on Google Colab (see link with icon at the top) if you're not already doing so. It's the simplest way to get started. You can also [install TensorFlow locally](https://www.tensorflow.org/install/).\n","\n","Note that there's [tf.keras](https://www.tensorflow.org/guide/keras) (comes with TensorFlow) and there's [Keras](https://keras.io/) (standalone). You should be using [tf.keras](https://www.tensorflow.org/guide/keras) because (1) it comes with TensorFlow so you don't need to install anything extra and (2) it comes with powerful TensorFlow-specific features.\n"]},{"cell_type":"code","execution_count":null,"id":"95bd9498","metadata":{},"outputs":[],"source":["\n","print('Loading/Installing Package => Begin\\n\\n')\n","# Commonly used modules\n","import numpy as np\n","import os\n","from os import path, makedirs\n","import time\n","from time import time\n","import subprocess\n","import sys\n","\n","\n","def install(package):\n","  print(\"Installing \"+package) \n","  subprocess.check_call([sys.executable,\"-m\" ,\"pip\", \"install\", package])\n","  print(\"Installed \"+package+\"\\n\") \n","# TensorFlow and tf.keras\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense\n","from tensorflow.keras.models import load_model\n","\n","# !pip3 install sklearn\n","install(\"sklearn\")\n","from sklearn.metrics import confusion_matrix, classification_report\n","\n","# !pip3 install seaborn\n","install(\"seaborn\")\n","import seaborn as sns\n","\n","\n","# Images, plots, display, and visualization\n","import matplotlib.cm as cm\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","\n","#reading/writing into files\n","# !pip3 install pickle5\n","install(\"pickle5\")\n","import pickle5 as pickle\n","\n","\n","#import cv2\n","install(\"IPython\")\n","import IPython\n","from six.moves import urllib\n","\n","\n","print('\\n########################################################################')\n","print('Checking the running platforms\\n')\n","import platform\n","running_os=platform.system()\n","print(\"OS: \"+running_os)\n","print(\"OS version: \"+platform.release())\n","\n","try:\n","  from google.colab import drive\n","  COLAB = True\n","except:\n","  COLAB = False\n","print(\"running on Colab: \"+str(COLAB))\n","\n","# if 'google.colab' in str(get_ipython()):\n","#   print('Running on CoLab')\n","#   install(\"google.colab\")\n","#   from google.colab import drive\n","#   drive.mount('/content/drive')\n","# else:\n","#   print('Not running on CoLab')\n","\n","\n","print(\"Python version: \"+platform.python_version())\n","print(\"Tensorflow version: \"+tf.__version__)\n","\n","dataset_directory_path=''\n","simulation_directory_path=''\n","\n","if COLAB == True:\n","  drive.mount('/content/drive')\n","  dataset_directory_path='/content/drive/MyDrive/Projects/110_JetscapeMl/hm.jetscapeml.data/simulation_results/'\n","  simulation_directory_path=dataset_directory_path+'simulation-results-nn-mnist-1200K-config-09/'\n","elif 'Linux' in running_os:\n","  dataset_directory_path='/wsu/home/gy/gy40/gy4065/hm.jetscapeml.data/simulation_results/'\n","  simulation_directory_path=dataset_directory_path+'simulation-results-nn-mnist-1200K-config-09/'\n","else:\n","  dataset_directory_path= 'G:\\\\My Drive\\\\Projects\\\\110_JetscapeMl\\\\hm.jetscapeml.data\\\\simulation_results\\\\'\n","  simulation_directory_path=dataset_directory_path+'simulation-results-nn-mnist-1200K-config-09\\\\'\n","print('Dataset Directory Path: '+dataset_directory_path)\n","\n","#dataset_file_name='jetscape-ml-benchmark-dataset-2k-randomized.pkl'\n","# dataset_file_name='jetscape-ml-benchmark-dataset-matter-vs-lbt-2000.pkl'\n","# dataset_file_name='jetscape-ml-benchmark-dataset-matter-vs-lbt-200k-shuffled-01.pkl'\n","# dataset_file_name='jetscape-ml-benchmark-dataset-matter-vs-lbt-1200k-momentum-shuffled.pkl'\n","# dataset_file_name='config-01-matter-vs-lbt-simulationsize1200000-dataset-momentum-shuffled.pkl'\n","# dataset_file_name='config-02-matter-vs-lbt-simulationsize1200000-dataset-momentum-shuffled.pkl'\n","# dataset_file_name='config-03-matter-vs-lbt-simulationsize1200000-dataset-momentum-shuffled.pkl'\n","# dataset_file_name='config-04-matter-vs-lbt-simulationsize1200000-dataset-momentum-shuffled.pkl'\n","# dataset_file_name='config-05-matter-vs-lbt-simulationsize1200000-dataset-momentum-shuffled.pkl'\n","# dataset_file_name='config-06-matter-vs-lbt-simulationsize1200000-dataset-momentum-shuffled.pkl'\n","# dataset_file_name='config-07-matter-vs-lbt-simulationsize1200000-dataset-momentum-shuffled.pkl'\n","# dataset_file_name='config-08-matter-vs-lbt-simulationsize1200000-dataset-momentum-shuffled.pkl'\n","dataset_file_name='config-09-matter-vs-lbt-simulationsize1200000-dataset-momentum-shuffled.pkl'\n","print(\"Dataset file name: \"+dataset_file_name)\n","\n","if not path.exists(simulation_directory_path):\n","    makedirs(simulation_directory_path)\n","print('Simulation Results Path: '+simulation_directory_path)\n","print('########################################################################\\n')\n","print('\\nLoading/Installing Package => End\\n\\n')\n"]},{"cell_type":"markdown","id":"ae5e7e31","metadata":{},"source":["\n","## 1. Load Data into a Numpy Array  \n","Downloading the data file onto my desktop/server and loading it locally.  \n","\n","(x_train, y_train), (x_test, y_test) = jetscapeMl.load_data()  \n","\n","**After the load:**   \n","x_train contains an array of 32x32.  \n","The y_train vector contains the corresponding labels for these.  \n","x_test contains an arrays of 32x32.  \n","The y_test vector contains the corresponding labels for these.\n","\n","##Saving and Loading Dataset Methods Implementation"]},{"cell_type":"code","execution_count":null,"id":"c172e360","metadata":{},"outputs":[],"source":["\n","def save_dataset(file_name,dataset):\n","    with open(file_name, 'wb') as dataset_file:\n","        pickle.dump(dataset,dataset_file, protocol=pickle.HIGHEST_PROTOCOL)\n","\n","        \n","def load_dataset(file_name):\n","    with open(file_name, 'rb') as dataset_file:\n","        (x_train, y_train), (x_test, y_test) = pickle.load(dataset_file, encoding='latin1')\n","        dataset=((x_train, y_train), (x_test, y_test))\n","        return dataset\n"]},{"cell_type":"markdown","id":"5766e17f","metadata":{},"source":["\n","## 2. Use Matplotlib to visualize one record.  \n","I set the colormap to Grey and ColorMap. There are a bunch of other colormap choices if you like bright visualizations. Try magma or any of the other  choice in the [docs](https://matplotlib.org/tutorials/colors/colormaps.html).\n"]},{"cell_type":"code","execution_count":null,"id":"d59374c9","metadata":{},"outputs":[],"source":["\n","\n","def plot_event(image_frame_size,event_matrix,file_name):\n","  plt.imshow(event_matrix.reshape(image_frame_size, image_frame_size), cmap=cm.Greys)\n","  cb = plt.colorbar()\n","  cb.set_label(\"Hit Frequency\")\n","  \n","  file_path=simulation_directory_path+file_name\n","  plt.savefig(file_path)\n","# # Funtionality Testing\n","# # Plotting sample Random Event Histogram on gray scale\n","# image_frame_size=32\n","# plot_event(image_frame_size,counts,file_name='sample_random_event_histogram_32x32_grayscale.png')\n","\n"]},{"cell_type":"markdown","id":"cee85493","metadata":{},"source":["\n","#Loading Dataset\n","**First learning step**"]},{"cell_type":"code","execution_count":null,"id":"e976a4a6","metadata":{},"outputs":[],"source":["class JetscapeMlCnn:\n","   # class attribute\n","  \n","    # Instance attribute\n","    def __init__(self, x_train,y_train,x_test,y_test):\n","        self.x_train=x_train\n","        self.y_train=y_train\n","        self.x_test=x_test\n","        self.y_test=y_test\n","\n","\n","#Loading Dataset Phase\n","\n","\n","dataset_file_path=dataset_directory_path+dataset_file_name\n","print(\"Dataset file path: \"+dataset_file_path)\n","(x_train, y_train), (x_test, y_test) =load_dataset(dataset_file_path)\n","\n","oJetscapeMlCnn=JetscapeMlCnn(x_train, y_train, x_test, y_test)\n","print(\"\\n#############################################################\")\n","print(\"Post-Load: DataType Checkpoint: Begin\")\n","print(type(oJetscapeMlCnn.x_train), oJetscapeMlCnn.x_train.size, oJetscapeMlCnn.x_train.shape)\n","print(type(oJetscapeMlCnn.y_train), oJetscapeMlCnn.y_train.size, oJetscapeMlCnn.y_train.shape)\n","print(type(oJetscapeMlCnn.x_test), oJetscapeMlCnn.x_test.size, oJetscapeMlCnn.x_test.shape)\n","print(type(oJetscapeMlCnn.y_test), oJetscapeMlCnn.y_test.size, oJetscapeMlCnn.y_test.shape)\n","print(oJetscapeMlCnn.y_train[1500], oJetscapeMlCnn.y_test[99])\n","print(oJetscapeMlCnn.y_train[1:500])\n","print(\"Post-Load: DataType Checkpoint: End\")\n","print(\"#############################################################\\n\")\n"]},{"cell_type":"markdown","id":"05b31971","metadata":{},"source":["\n","## 3. Plot a bunch of records to see sample data  \n","Basically, use the same Matplotlib commands above in a for loop to show 20 records from the train set in a subplot figure. We also make the figsize a bit bigger and remove the tick marks for readability.\n","** TODO: try to make the subplot like the below from the first project meeting\n"]},{"cell_type":"code","execution_count":null,"id":"0bb77b27","metadata":{},"outputs":[],"source":["def plot_20_sample_events(events_matrix_items):\n","  # images = x_train[0:18]\n","  # fig, axes = plt.subplots(3, 6, figsize=[9,5])\n","  images = events_matrix_items\n","  fig, axes = plt.subplots(2, 10, figsize=[15,5])\n","\n","  for i, ax in enumerate(axes.flat):\n","      current_plot= ax.imshow(x_train[i].reshape(32, 32), cmap=cm.Greys)\n","      ax.set_xticks([])\n","      ax.set_yticks([])     \n","  \n","\n","  file_name='hm_jetscape_ml_plot_20_sample_events.png'\n","  file_path=simulation_directory_path+file_name\n","  plt.savefig(file_path)\n","\n","  plt.show()\n","  plt.close()\n","#Plotting 20 Sample Events Phase\n","events_matrix_items=[x_train[0:10],x_train[1500:10]]\n","plot_20_sample_events(events_matrix_items)\n","\n"]},{"cell_type":"markdown","id":"d6ded342","metadata":{},"source":["## 4. Show distribution of training data labels   \n","The training data is about evenly distributed across all nine digits."]},{"cell_type":"code","execution_count":null,"id":"25134792","metadata":{},"outputs":[],"source":["\n","\n","def plot_y_train_dataset_distribution(y_train):\n","  unique_class_labels,positions = np.unique(y_train,return_inverse=True)\n","\n","\n","  counts = np.bincount(positions)\n","  plt.bar(unique_class_labels, counts)\n","  plt.title(\"Dataset classification vector's distribution\")\n","  \n","  \n","  file_name='hm_jetscape_ml_plot_y_train_dataset_distribution.png'\n","  file_path=simulation_directory_path+file_name\n","  plt.savefig(file_path)\n","\n","  plt.show()\n","  plt.close()\n","  print(\"\\n#############################################################\")\n","  print(\"Classification vector statistics:\")\n","  print(unique_class_labels)\n","  unique_class_labels,positions = np.unique(y_train,return_inverse=True)\n","  print(counts)\n","  print(unique_class_labels)\n","  print(\"Sample 20 head labels:\")\n","  print(positions[:20])\n","  print(\"#############################################################\\n\")\n","\n","#Checking Train Dataset Y Distribution\n","plot_y_train_dataset_distribution(y_train)\n","\n"]},{"cell_type":"markdown","id":"b1d7cc38","metadata":{},"source":["# Changing classification labels from Literal to Numeric\n","\n","---\n","\n"]},{"cell_type":"code","execution_count":null,"id":"c2806f94","metadata":{},"outputs":[],"source":["\n","def convertDatasetYFromLiteralToNumeric(y_dataset):\n","  y_train=y_dataset[0]\n","  y_test=y_dataset[1]\n","  y_train_unique_class_labels,y_train_positions = np.unique(y_train,return_inverse=True)\n","  y_test_unique_class_labels,y_test_positions = np.unique(y_test,return_inverse=True)\n","  \n","  print(y_train_unique_class_labels)\n","  print(y_test_unique_class_labels)\n","  \n","  y_train=y_train_positions\n","  y_test=y_test_positions\n","  \n","  return ((y_train,y_test))\n","\n","\n","print(\"\\n#############################################################\")\n","print(\"Changing classification labels from Literal to Numeric:\")\n","print(\"\\nBefore conversion:\")\n","print(type(y_train), y_train.size, y_train.shape)\n","print(type(y_test), y_test.size, y_test.shape)\n","print(type(y_train[0]))\n","print(type(y_test[0]))\n","\n","y_train,y_test =convertDatasetYFromLiteralToNumeric((y_train,y_test))\n","\n","print(\"\\nAfter conversion:\")\n","print(type(y_train), y_train.size, y_train.shape)\n","print(type(y_test), y_test.size, y_test.shape)\n","\n","print(type(y_train[0]))\n","print(type(y_test[0]))\n","print(\"#############################################################\\n\")\n","\n","\n"]},{"cell_type":"markdown","id":"cd25f22a","metadata":{},"source":["\n","## Normalizing the Dataset X\n","For training the model, the dataset needs to be normalized, meaning all of the dataset values should me between zero and one. This can be done by finding the maximum values over the dataset X side values and devide all the element by it.\n"]},{"cell_type":"code","execution_count":null,"id":"08bc661e","metadata":{},"outputs":[],"source":["\n","def calculate_dataset_x_max_value(x_dataset):\n","  x_train=x_dataset[0]\n","  x_test=x_dataset[1]\n","  max_x=np.amax([np.amax(x_train), np.amax(x_test)])\n","  return max_x\n","\n","def normalize_dataset_x_value_range_between_0_and_1(x_dataset,max_x):\n","  x_train=x_dataset[0]\n","  x_test=x_dataset[1]\n","\n","  # Normalize the data to a 0.0 to 1.0 scale for faster processing\n","  x_train, x_test = x_train / max_x, x_test / max_x\n","  return (x_train, x_test)\n","\n","\n","#Normalizing Phase\n","x_dataset=(x_train,x_test)\n","max_x=calculate_dataset_x_max_value(x_dataset)\n","x_train,x_test=normalize_dataset_x_value_range_between_0_and_1(x_dataset,max_x)\n","\n","image_frame_size=32\n","\n","print(\"\\n#############################################################\")\n","\n","print(\"Normalizing Dataset X: maximum sum of transfer momentum in the dataset: \")\n","print(max_x)\n","\n","print(\"#############################################################\\n\")\n","\n"]},{"cell_type":"markdown","id":"23ae3892","metadata":{},"source":["## Defining Validation Dataset from Train Dataset"]},{"cell_type":"code","execution_count":null,"id":"9a255846","metadata":{},"outputs":[],"source":["\n","\n","# Reserve 20% samples for validation dataset\n","def calculate_validation_dataset_size(dataset_train_size,dataset_test_size):\n","  dataset_size= dataset_train_size+dataset_test_size\n","  dataset_validation_size=dataset_size*.2\n","  return int(dataset_validation_size)\n","\n","def set_validation_dataset(x_train,y_train,validation_dataset_size):\n","  \n","  x_val = x_train[-validation_dataset_size:]\n","  y_val = y_train[-validation_dataset_size:]\n","  x_train = x_train[:-validation_dataset_size]\n","  y_train = y_train[:-validation_dataset_size]\n","  \n","  \n","  return (x_train, y_train), (x_val, y_val)\n","\n","validation_dataset_size= calculate_validation_dataset_size(y_train.size,y_test.size)\n","(x_train, y_train), (x_val, y_val)=set_validation_dataset(x_train,y_train,validation_dataset_size)\n","print(\"\\n#############################################################\")\n","print(\"Defining Validation Dataset from Train Dataset:\")\n","\n","print(\"\\nTrain data info:\")\n","print(type(y_train), y_train.size, y_train.shape)\n","print(type(x_train), x_train.size, x_train.shape)\n","\n","print(\"\\nValidation data info:\")\n","print(type(y_val), y_val.size, y_val.shape)\n","print(type(x_val), x_val.size, x_val.shape)\n","\n","print(\"\\nTest data info:\")\n","print(type(y_test), y_test.size, y_test.shape)\n","print(type(x_test), x_test.size, x_test.shape)\n","print(\"#############################################################\\n\")\n"]},{"cell_type":"markdown","id":"59064d30","metadata":{},"source":["\n","## 5.1 Apply Keras/TensorFlow neural network  \n","Use tensorflow to train the model with 1600/180K training records, compile the model, and classify 400/20K test records and calcualte the accuracy accuracy.  \n","**Create the model**  \n","Build the keras model by stacking layers into the network. Our model here has four layers:\n","- Flatten reshapes the data into a 1-dimensional array\n","- [Dense](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense) tells the model to use output arrays of shape (*, 512) and sets rectified linear [activation function](https://keras.io/activations/). \n","- [Dropout](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dropout) applies dropout to the input to help avoid overfitting.\n","- The next Dense line condenses the ouput into probabilities for each of the 2 Energy Loss Classes.\n","\n","**Compile the model**   \n","- [Adam](https://keras.io/optimizers/) is an optimization algorithm that uses stochastic gradient descent to update network weights.\n","- Sparse categorical crossentropy is a [loss function](https://keras.io/losses/) that is required to compile the model. The loss function measures how accurate the model is during training. We want to minimize this function to steer the model in the right direction.\n","- A metric is a function that is used to judge the performance of your model. We're using accuracy of our predictions as compared to y_test as our metric.  \n","Lastly, we fit our training data into the model, with several training repetitions (epochs), then evaluate our test data. \n","\n","Our final result is about ?% accuracy in classifying 400/20K events in the test set. You can try tweaking this model with different settings to get a better score. An easy tweak is increasing the epochs, which improves accuracy at the expense of time. Follow the links to the Keras layer docs above and try different options for Dense output, activation functions, optimization algorithms and loss functions.\n","\n","### Train the model\n","\n","Training the neural network model requires the following steps:\n","\n","1. Feed the training data to the model—in this example, the `x_train` and `y_train` arrays, which are the images and labels.\n","2. The model learns to associate images and labels.\n","3. We ask the model to make predictions about a test set—in this example, the `x_test` array. We verify that the predictions match the labels from the `y_test` array. \n","\n","To start training,  call the `model.fit` method—the model is \"fit\" to the training data:\n","*Setting number of Epochs to 1000*\n"]},{"cell_type":"code","execution_count":null,"id":"ad8d7796","metadata":{},"outputs":[],"source":["\n","# this helps makes our output less verbose but still shows progress\n","class PrintDot(keras.callbacks.Callback):\n","    def on_epoch_end(self, epoch, logs):\n","        if epoch % 100 == 0: print('')\n","        print('.', end='')\n","\n","def build_and_train_model():\n","  # model = tf.keras.models.Sequential([\n","  #   tf.keras.layers.Flatten(input_shape=(image_frame_size, image_frame_size)),\n","  #   tf.keras.layers.Dense(max_x+1, activation=tf.nn.relu),\n","  #   tf.keras.layers.Dropout(0.25),\n","  #   tf.keras.layers.Dense(2, activation=tf.nn.softmax)\n","  # ])\n","  model = tf.keras.models.Sequential([\n","  tf.keras.layers.Flatten(input_shape=(image_frame_size, image_frame_size)),\n","  tf.keras.layers.Dense(max_x+1, activation=tf.nn.relu),\n","  tf.keras.layers.Dropout(0.25),\n","  tf.keras.layers.Dense(1, activation=tf.nn.sigmoid)\n","  ])\n","  model.summary()\n","\n","  model.compile(optimizer=tf.optimizers.Adam(), \n","                loss='binary_crossentropy',\n","                metrics=['acc'])\n","  print(\"Fit model on training data\")\n","  # fit(object, x = NULL, y = NULL, batch_size = NULL, epochs = 10,\n","  #   verbose = getOption(\"keras.fit_verbose\", default = 1),\n","  #   callbacks = NULL, view_metrics = getOption(\"keras.view_metrics\",\n","  #   default = \"auto\"), validation_split = 0, validation_data = NULL,\n","  #   shuffle = TRUE, class_weight = NULL, sample_weight = NULL,\n","  #   initial_epoch = 0, steps_per_epoch = NULL, validation_steps = NULL,\n","  #   ...)\n","  # -> object : the model to train.      \n","  # -> X : our training data. Can be Vector, array or matrix      \n","  # -> Y : our training labels. Can be Vector, array or matrix       \n","  # -> Batch_size : it can take any integer value or NULL and by default, it will\n","  # be set to 32. It specifies no. of samples per gradient.      \n","  # -> Epochs : an integer and number of epochs we want to train our model for.      \n","  # -> Verbose : specifies verbosity mode(0 = silent, 1= progress bar, 2 = one\n","  # line per epoch).      \n","  # -> Shuffle : whether we want to shuffle our training data before each epoch.      \n","  # -> steps_per_epoch : it specifies the total number of steps taken before\n","  # one epoch has finished and started the next epoch. By default it values is set to NULL.\n","\n","  start_time =time.perf_counter()\n","\n","  no_epoch=2\n","  history = model.fit(\n","      x_train,\n","      y_train,\n","      batch_size=64,\n","      epochs=no_epoch,\n","      # We pass some validation for\n","      # monitoring validation loss and metrics\n","      # at the end of each epoch\n","      validation_data=(x_val, y_val))\n","\n","  file_name='trained_model.h5'\n","  file_path=simulation_directory_path+file_name\n","  model.save(file_path)\n","  acc_train = history.history['acc']\n","  acc_val = history.history['val_acc']\n","  print(acc_train)\n","  print(acc_train)\n","  epochs = range(1,no_epoch+1)\n","  plt.plot(epochs,acc_train, 'g', label='training accuracy')\n","  plt.plot(epochs, acc_val, 'b', label= 'validation accuracy')\n","  plt.title('Training and Validation accuracy')\n","  plt.xlabel('Epochs')\n","  plt.ylabel('Accuracy')\n","  plt.legend()\n","  plt.show()\n","  plt.close()\n","  elapsed_time=time.perf_counter() - start_time\n","  print('Elapsed %.3f seconds.' % elapsed_time)\n","\n","# early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=50)\n","# , verbose=0, validation_split = 0.1,\n","#                     callbacks=[early_stop, PrintDot()]\n"]},{"cell_type":"markdown","id":"4da4a861","metadata":{},"source":["## 5.2 Apply Keras/TensorFlow with MNIST pretrained model\n"]},{"cell_type":"code","execution_count":null,"id":"ab931e33","metadata":{},"outputs":[],"source":["## event info\n","collision = 'PbPb'\n","energy = 5020\n","centrality = '0_10'\n","Modules = ['MATTER','LBT']\n","JetptMinMax = '100_110'\n","#observables = ['pt','charge','mass']\n","observables = ['pt']\n","kind = 'Hadron'\n"]},{"cell_type":"markdown","id":"bd5baa45","metadata":{},"source":["# create a directory to save the best model"]},{"cell_type":"code","execution_count":null,"id":"42a44942","metadata":{},"outputs":[],"source":["save_dir = (simulation_directory_path+'models_{}_vs_{}_{}_ch{}').format(Modules[0], Modules[1], kind, len(observables))\n","if not path.exists(save_dir):\n","    makedirs(save_dir)\n","print('Directory to save models: {}'.format(save_dir))\n","\n"]},{"cell_type":"code","execution_count":null,"id":"286cf5a6","metadata":{},"outputs":[],"source":["from tensorflow.keras.models import Sequential, load_model\n","from tensorflow.keras.layers import Dense, Dropout, LeakyReLU\n","from tensorflow.keras.layers import Conv2D, MaxPool2D, BatchNormalization, Flatten\n","from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n","from tensorflow.keras.regularizers import l2\n","from tensorflow.keras.optimizers import Adam, Adagrad\n","\n","\n","def get_callbacks(monitor, save_dir):\n","    mode = None\n","    if 'loss' in monitor:\n","        mode = 'min'\n","    elif 'accuracy' in monitor:\n","        mode = 'max'\n","    assert mode != None, 'Check the monitor parameter!'\n","\n","    es = EarlyStopping(monitor=monitor, mode=mode, patience=10,\n","                      min_delta=0., verbose=1)\n","    rlp = ReduceLROnPlateau(monitor=monitor, mode=mode, factor=0.2, patience=5,\n","                            min_lr=0.001, verbose=1)\n","    mcp = ModelCheckpoint(path.join(save_dir, 'hm_jetscape_ml_model_best.h5'), monitor=monitor, \n","                          save_best_only=True, mode=mode, verbose=1)\n","    \n","    return [es, rlp, mcp]\n","\n","\n","\n","\n","def CNN_model(input_shape, lr, dropout):\n","    model = Sequential()\n","    \n","    model.add(Flatten(input_shape=(image_frame_size, image_frame_size)))\n","    model.add(Dense(max_x+1, activation=tf.nn.relu))\n","    model.add(Dropout(dropout))\n","    model.add(Dense(1, activation=tf.nn.sigmoid))\n","    \n","    optimizer = Adam(learning_rate=lr)\n","    model.compile(loss='binary_crossentropy', optimizer=optimizer,\n","                  metrics=['accuracy'])\n","    \n","    return model"]},{"cell_type":"code","execution_count":null,"id":"1520eb0a","metadata":{},"outputs":[],"source":["\n","\n","# print(\"\\n#############################################################\")\n","# print(\"Reshaping dataset X-side dimension to be fit in the defined convolutional :\")\n","\n","# print(\"\\nX train:\")\n","# print(x_train.shape)\n","# print (x_train.shape[0],x_train.shape[1],x_train.shape[2])\n","# x_train_reshaped=x_train.reshape(x_train.shape[0],x_train.shape[1],x_train.shape[2],1)\n","# print(x_train_reshaped.shape)\n","\n","# print(\"\\nX val:\")\n","# print(x_val.shape)\n","# print (x_val.shape[0],x_val.shape[1],x_val.shape[2])\n","# x_val_reshaped=x_val.reshape(x_val.shape[0],x_val.shape[1],x_val.shape[2],1)\n","# print(x_train_reshaped.shape)\n","\n","# print(\"\\nX test:\")\n","# print(x_test.shape)\n","# print (x_test.shape[0],x_test.shape[1],x_test.shape[2])\n","# x_test_reshaped=x_test.reshape(x_test.shape[0],x_test.shape[1],x_test.shape[2],1)\n","# print(x_test_reshaped.shape)\n","# print(\"#############################################################\\n\")"]},{"cell_type":"code","execution_count":null,"id":"2f959fc0","metadata":{},"outputs":[],"source":["\n","## parameers for training\n","n_epochs = 50\n","# n_epochs=2\n","batch_size = 256\n","# input_shape = x_train_reshaped.shape[1:]\n","input_shape = x_train.shape[1:]\n","monitor='val_accuracy' #'val_accuracy' or 'val_loss'\n","lr = 5e-6\n","dropout= 0.25\n"]},{"cell_type":"code","execution_count":null,"id":"2e4a33cf","metadata":{},"outputs":[],"source":["from time import time\n","def train_network(train_set, val_set, n_epochs, lr, batch_size, monitor):\n","    tf.keras.backend.clear_session()\n","    X_train = train_set[0]\n","    Y_train = train_set[1]\n","    model = CNN_model(input_shape, lr, dropout)\n","    callbacks = get_callbacks(monitor, save_dir)\n","    \n","    model.summary()\n","    \n","    start = time()\n","    history = model.fit(X_train, Y_train, epochs=n_epochs, verbose=1, batch_size=batch_size, \n","                        validation_data=val_set, shuffle=True, callbacks=callbacks)\n","    train_time = (time()-start)/60.\n","    return history, train_time\n"]},{"cell_type":"code","execution_count":null,"id":"a768c074","metadata":{},"outputs":[],"source":["\n","\n","# training and validation sets\n","# train_set, val_set = (x_train_reshaped, y_train), (x_val_reshaped, y_val)\n","train_set, val_set = (x_train, y_train), (x_val, y_val)\n","\n","# train the network\n","history, train_time = train_network(train_set, val_set, n_epochs, lr, batch_size, monitor)\n","file_name='hm_jetscape_ml_model_history.csv'\n","file_path=simulation_directory_path+file_name\n","pd.DataFrame.from_dict(history.history).to_csv(file_path,index=False)\n","\n","\n","file_name='hm_jetscape_ml_model_history.npy'\n","file_path=simulation_directory_path+file_name\n","np.save(file_path,history.history)\n"]},{"cell_type":"code","execution_count":null,"id":"3cf1173b","metadata":{},"outputs":[],"source":["\n","# This section shall be just used after training or for stand alone evaluations\n","# Building a dictionary which is accessable by dot\n","class dotdict(dict):\n","    \"\"\"dot.notation access to dictionary attributes\"\"\"\n","    __getattr__ = dict.get\n","    __setattr__ = dict.__setitem__\n","    __delattr__ = dict.__delitem__\n","\n","#Loading learning history after training \n","file_name='hm_jetscape_ml_model_history.npy'\n","file_path=simulation_directory_path+file_name\n","\n","\n","history=dict({'history':np.load(file_path,allow_pickle='TRUE').item()})\n","history=dotdict(history)\n","print(history)"]},{"cell_type":"code","execution_count":null,"id":"cd83c300","metadata":{},"outputs":[],"source":["\n","from matplotlib import pyplot as plt\n","def plot_train_history(history):\n","\n","    color_list = ['red','blue','black','green']\n","\n","    plt.figure(figsize=(8, 2.5), dpi=100)\n","\n","    plt.subplot(121)\n","    plt.plot(history.history['loss'], label='loss')\n","    plt.plot(history.history['val_loss'], label='val_loss')\n","    plt.yscale('log')\n","    plt.xlabel('Epoch')\n","    plt.title('Loss history')\n","    plt.legend()\n","    \n","    plt.subplot(122)\n","    plt.plot(history.history['accuracy'], label='accuracy')\n","    plt.plot(history.history['val_accuracy'], label='val_accuracy')\n","    plt.xlabel('Epoch')\n","    plt.title('Accuracy history')\n","    plt.legend()\n","    file_name='hm_jetscape_ml_plot_train_history.png'\n","    file_path=simulation_directory_path+file_name\n","    plt.savefig(file_path)\n","    plt.show()\n","    plt.close()\n","# plot the training history for each fold\n","plot_train_history(history)\n"]},{"cell_type":"code","execution_count":null,"id":"c765368a","metadata":{},"outputs":[],"source":["\n","from tensorflow.keras.models import load_model\n","## load the best model\n","best_model = load_model(path.join(save_dir,'hm_jetscape_ml_model_best.h5'))\n","\n","outputStr='Train   | Validation | Test sets\\n'\n","\n","## evaluate the model on train/val/test sets and append the results to lists\n","_, train_acc = best_model.evaluate(x_train, y_train, verbose=0)\n","_, val_acc = best_model.evaluate(x_val, y_val, verbose=0)\n","_, test_acc = best_model.evaluate(x_test, y_test, verbose=0)\n","    \n","## print out the accuracy\n","outputStr+='{:.4f}%  {:.4f}%     {:.4f}%\\n'.format(train_acc * 100, val_acc * 100, test_acc * 100)\n","print(outputStr)\n","\n","file_name=\"hm_jetscape_ml_model_evaluation.txt\"\n","file_path=simulation_directory_path+file_name\n","evaluation_file = open(file_path, \"w\")\n","evaluation_file.write(outputStr)\n","evaluation_file.close()\n"]},{"cell_type":"code","execution_count":null,"id":"000c38a9","metadata":{},"outputs":[],"source":["\n","from sklearn.metrics import confusion_matrix, classification_report\n","import seaborn as sns\n","## plot confution matrix\n","y_pred = best_model.predict_classes(x_test)\n","\n","conf_mat = confusion_matrix(y_pred, y_test)\n","sns.heatmap(conf_mat, annot=True, cmap='Blues', \n","            xticklabels=Modules, yticklabels=Modules, fmt='g')\n","plt.xlabel('True Label', fontsize=15)\n","plt.ylabel('Prediction', fontsize=15)\n","file_name='hm_jetscape_ml_model_confision_matrix.png'\n","file_path=simulation_directory_path+file_name\n","plt.savefig(file_path)\n","plt.show()\n","plt.close()\n","\n","classification_report_str= classification_report(y_test,y_pred)\n","\n","print (classification_report_str)\n","file_name=\"hm_jetscape_ml_model_evaluation.txt\"\n","file_path=simulation_directory_path+file_name\n","evaluation_file = open(file_path, \"a\")\n","evaluation_file.write(classification_report_str)\n","evaluation_file.close()"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"jetscape-ml-tensorflow-cnn.ipynb","provenance":[]},"interpreter":{"hash":"ada828d16365d2b22d3899327f52f8feba3feb56b4fde7279c1cd0b9201605e0"},"kernelspec":{"display_name":"Python 3.7.6 ('tensorflow_env')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"nbformat":4,"nbformat_minor":5}
