{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hmehryar/Hm.JetscapeMl/blob/309-implementingtraining-pointnet-for-alpha_s-with-various-epochs-and-folds-and-finding-the-best-learning-rate/jet_ml_pointnet_alpha_s/jet_ml_pointnet_classifier_all_datasets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VT8MlMaW_zlo"
      },
      "source": [
        "[link text](https://)# Point cloud classification with PointNet for Heavy Ion Collisions for diffrent dataset size and K-Folding\n",
        "\n",
        "**Authors:** [Haydar Mehryar](https://github.com/hmehryar) <br>\n",
        "**Date created:** 2023/12/01<br>\n",
        "**Last modified:** 2024/04/28<br>\n",
        "**Description:** Implementation of PointNet for heavy ion colllisions classifiction, based on the code from [David Griffiths](https://dgriffiths3.github.io)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "T4UgitN8Jch3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3cb46157-8586-42cb-aa52-d3b9968efc61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u0zR34byjZ9D",
        "outputId": "9e49571d-9b6b-47f4-d071-67ed82055f8c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n",
              " PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "tf.config.experimental.list_physical_devices()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hk_dCbkKm-4k",
        "outputId": "72c1138d-cbeb-448e-d589-c0d9ee2e69d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import sys\n",
        "sys.path.insert(1,'/wsu/home/gy/gy40/gy4065/hm.jetscapeml.source')\n",
        "sys.path.insert(1,'/content/drive/My Drive/Projects/110_JetscapeMl/hm.jetscapeml.source')\n",
        "sys.path.insert(1,'/content/drive/MyDrive/Projects/110_JetscapeMl/hm.jetscapeml.source')\n",
        "sys.path.insert(1,'/g/My Drive/Projects/110_JetscapeMl/hm.jetscapeml.source')\n",
        "sys.path.insert(1,'G:\\\\My Drive\\\\Projects\\\\110_JetscapeMl\\\\hm.jetscapeml.source')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILBWgUOo_tQz"
      },
      "source": [
        "## Setup\n",
        "\n",
        "If using colab first install trimesh with `!pip install trimesh`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "_o3efLKxrkUq"
      },
      "outputs": [],
      "source": [
        "# pip install --upgrade keras tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V5FNXhmJqtdZ",
        "outputId": "91866087-71b2-4a86-e3d7-0ef288e349fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading/Installing Package => Begin\n",
            "\n",
            "\n",
            "\n",
            "Loading/Installing Package => End\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# loading libraries\n",
        "print('Loading/Installing Package => Begin\\n\\n')\n",
        "import jet_ml_dataset_builder.jet_ml_dataset_builder_utilities as util\n",
        "from jet_ml_dataset_builder.jet_ml_dataset_builder_utilities import set_directory_paths\n",
        "from jet_ml_dataset_builder.jet_ml_dataset_builder_utilities import parse_parameters\n",
        "from jet_ml_dataset_builder.jet_ml_dataset_builder_utilities import load_dataset\n",
        "from jet_ml_dataset_builder.jet_ml_dataset_builder_utilities import install\n",
        "# install(\"trimesh\")\n",
        "import os\n",
        "from time import time\n",
        "import glob\n",
        "# import trimesh\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, History\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "import json\n",
        "\n",
        "\n",
        "tf.random.set_seed(1234)\n",
        "print('\\nLoading/Installing Package => End\\n\\n')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IrDIuS_1sqb6",
        "outputId": "c73b03e0-0d96-46da-9cfa-a23c40f89f61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "########################################################################\n",
            "Checking the running platforms\n",
            "\n",
            "Python version: 3.10.12\n",
            "OS: Linux\n",
            "OS version: 6.1.58+\n",
            "running on Colab: True\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Dataset Directory Path: /content/drive/MyDrive/Projects/110_JetscapeMl/hm.jetscapeml.data/\n",
            "Simulation Results Path: /content/drive/MyDrive/Projects/110_JetscapeMl/hm.jetscapeml.data/simulation_results/\n",
            "Dataset Directory Path: /content/drive/MyDrive/Projects/110_JetscapeMl/hm.jetscapeml.data/\n",
            "Simulation Directory Path: /content/drive/MyDrive/Projects/110_JetscapeMl/hm.jetscapeml.data/simulation_results/\n",
            "########################################################################\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print('\\n########################################################################')\n",
        "print('Checking the running platforms\\n')\n",
        "\n",
        "# Call the function and retrieve the dataset_directory_path and simulation_directory_path\n",
        "dataset_directory_path, simulation_directory_path = set_directory_paths()\n",
        "\n",
        "# Access the dataset_directory_path and simulation_directory_path\n",
        "print(\"Dataset Directory Path:\", dataset_directory_path)\n",
        "print(\"Simulation Directory Path:\", simulation_directory_path)\n",
        "print('########################################################################\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jX15cxNSs6o3",
        "outputId": "2ebe87cf-52d3-4753-f8eb-f6c05667a638"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "option -f not recognized\n",
            "Tokenized Arguments:\n",
            "\n",
            "Tokenized Values:\n"
          ]
        }
      ],
      "source": [
        "# Call the function and retrieve the tokenized parameters\n",
        "tokenized_arguments, tokenized_values = parse_parameters()\n",
        "\n",
        "# Access the tokenized arguments and values\n",
        "print(\"Tokenized Arguments:\")\n",
        "for argument in tokenized_arguments:\n",
        "    print(argument)\n",
        "\n",
        "print(\"\\nTokenized Values:\")\n",
        "for argument, value in tokenized_values.items():\n",
        "    print(f\"{argument}: {value}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_label_items():\n",
        "    print ('Aggregatring all parameters values')\n",
        "    y_class_label_items=['MMAT','MLBT']\n",
        "    alpha_s_items=[0.2 ,0.3 ,0.4]\n",
        "    q0_items=[1.5 ,2.0 ,2.5]\n",
        "    data_dict = {\n",
        "        \"y_class_label_items\": y_class_label_items,\n",
        "        \"alpha_s_items\": alpha_s_items,\n",
        "        \"q0_items\": q0_items\n",
        "    }\n",
        "    return data_dict"
      ],
      "metadata": {
        "id": "AYSArbI8qhE5"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "aTojIHZMtGcg"
      },
      "outputs": [],
      "source": [
        "def get_labels_str(label_items_dict):\n",
        "  print(\"Building required params for the loading the dataset file\")\n",
        "\n",
        "  data_dict = {\n",
        "      \"class_labels_str\":'_'.join(label_items_dict['y_class_label_items']),\n",
        "      \"alpha_s_items_str\":'_'.join(map(str, label_items_dict['alpha_s_items'])),\n",
        "      \"q0_items_str\":'_'.join(map(str, label_items_dict['q0_items'])),\n",
        "  }\n",
        "  return data_dict"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "    label_items_dict = get_label_items()\n",
        "    print(label_items_dict)\n",
        "\n",
        "    label_str_dict=get_labels_str(label_items_dict)\n",
        "    print(label_str_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q6lVAuYg1lAM",
        "outputId": "11150d6c-c3f2-4e58-c712-393debd16854"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Aggregatring all parameters values\n",
            "{'y_class_label_items': ['MMAT', 'MLBT'], 'alpha_s_items': [0.2, 0.3, 0.4], 'q0_items': [1.5, 2.0, 2.5]}\n",
            "Building required params for the loading the dataset file\n",
            "{'class_labels_str': 'MMAT_MLBT', 'alpha_s_items_str': '0.2_0.3_0.4', 'q0_items_str': '1.5_2.0_2.5'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "VUyAMovFsgDO"
      },
      "outputs": [],
      "source": [
        "# loading dataset by size and getting just the first column\n",
        "# Function to load datasets of different sizes\n",
        "def get_dataset(size,working_column=0):\n",
        "\n",
        "    dataset_file_name = f\"jet_ml_benchmark_config_01_to_09_alpha_{label_str_dict['alpha_s_items_str']}_q0_{label_str_dict['q0_items_str']}_{label_str_dict['class_labels_str']}_size_{size}_shuffled.pkl\"\n",
        "\n",
        "    dataset_file_name=dataset_directory_path+dataset_file_name\n",
        "    print(\"dataset_file_name:\",dataset_file_name)\n",
        "\n",
        "    dataset=load_dataset(dataset_file_name,has_test=False)\n",
        "    (dataset_x, dataset_y) = dataset\n",
        "    print(f'Extract the working column#{working_column} for classification')\n",
        "    dataset_y = dataset_y[:, working_column]\n",
        "    print(\"dataset.x:\",type(dataset_x), dataset_x.size, dataset_x.shape)\n",
        "    print(\"dataset.y:\",type(dataset_y), dataset_y.size,dataset_y.shape)\n",
        "    print(\"dataset.y(working_column) sample\",dataset_y[:10])\n",
        "    return dataset_x, dataset_y"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_x, dataset_y=get_dataset(1000,1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YFQ5UNE9tjZj",
        "outputId": "1525db9c-f2ef-40bf-b4ef-3dd3b0c2c8e4"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataset_file_name: /content/drive/MyDrive/Projects/110_JetscapeMl/hm.jetscapeml.data/jet_ml_benchmark_config_01_to_09_alpha_0.2_0.3_0.4_q0_1.5_2.0_2.5_MMAT_MLBT_size_1000_shuffled.pkl\n",
            "dataset.x: <class 'numpy.ndarray'> 1024000 (1000, 32, 32)\n",
            "dataset.y: <class 'numpy.ndarray'> 3000 (1000, 3)\n",
            "Extract the working column#1 for classification\n",
            "dataset.x: <class 'numpy.ndarray'> 1024000 (1000, 32, 32)\n",
            "dataset.y: <class 'numpy.ndarray'> 1000 (1000,)\n",
            "dataset.y(working_column) sample ['0.4' '0.4' '0.2' '0.4' '0.4' '0.3' '0.4' '0.3' '0.4' '0.4']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HcmpjB3C-HH2"
      },
      "source": [
        "Building and Compiling the Classifier model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "7wlkSWaHks0z"
      },
      "outputs": [],
      "source": [
        "def conv_bn(x, filters):\n",
        "    x = layers.Conv1D(filters, kernel_size=1, padding=\"valid\")(x)\n",
        "    x = layers.BatchNormalization(momentum=0.0)(x)\n",
        "    return layers.Activation(\"relu\")(x)\n",
        "\n",
        "\n",
        "def dense_bn(x, filters):\n",
        "    x = layers.Dense(filters)(x)\n",
        "    x = layers.BatchNormalization(momentum=0.0)(x)\n",
        "    return layers.Activation(\"relu\")(x)\n",
        "\n",
        "\n",
        "class OrthogonalRegularizer(keras.regularizers.Regularizer):\n",
        "    def __init__(self, num_features, l2reg=0.001):\n",
        "        self.num_features = num_features\n",
        "        self.l2reg = l2reg\n",
        "        self.eye = tf.eye(num_features)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        x = tf.reshape(x, (-1, self.num_features, self.num_features))\n",
        "        xxt = tf.tensordot(x, x, axes=(2, 2))\n",
        "        xxt = tf.reshape(xxt, (-1, self.num_features, self.num_features))\n",
        "        return tf.reduce_sum(self.l2reg * tf.square(xxt - self.eye))\n",
        "\n",
        "    def get_config(self):\n",
        "        return {'num_features': self.num_features, 'l2reg': self.l2reg}\n",
        "\n",
        "def tnet(inputs, num_features):\n",
        "\n",
        "    # Initalise bias as the indentity matrix\n",
        "    bias = keras.initializers.Constant(np.eye(num_features).flatten())\n",
        "    reg = OrthogonalRegularizer(num_features)\n",
        "\n",
        "    x = conv_bn(inputs, 32)\n",
        "    x = conv_bn(x, 64)\n",
        "    x = conv_bn(x, 512)\n",
        "    x = layers.GlobalMaxPooling1D()(x)\n",
        "    x = dense_bn(x, 256)\n",
        "    x = dense_bn(x, 128)\n",
        "    x = layers.Dense(\n",
        "        num_features * num_features,\n",
        "        kernel_initializer=\"zeros\",\n",
        "        bias_initializer=bias,\n",
        "        activity_regularizer=reg,\n",
        "    )(x)\n",
        "    feat_T = layers.Reshape((num_features, num_features))(x)\n",
        "    # Apply affine transformation to input features\n",
        "    return layers.Dot(axes=(2, 1))([inputs, feat_T])\n",
        "\n",
        "def build_pointnet_classifier_model(NUM_POINTS,NUM_CLASSES):\n",
        "    inputs = keras.Input(shape=(NUM_POINTS, 3))\n",
        "    x = tnet(inputs, 3)\n",
        "    x = conv_bn(x, 32)\n",
        "    x = conv_bn(x, 32)\n",
        "    x = tnet(x, 32)\n",
        "    x = conv_bn(x, 32)\n",
        "    x = conv_bn(x, 64)\n",
        "    x = conv_bn(x, 512)\n",
        "    x = layers.GlobalMaxPooling1D()(x)\n",
        "    x = dense_bn(x, 256)\n",
        "    x = layers.Dropout(0.3)(x)\n",
        "    x = dense_bn(x, 128)\n",
        "    x = layers.Dropout(0.3)(x)\n",
        "\n",
        "    # outputs = layers.Dense(NUM_CLASSES, activation=\"softmax\")(x)\n",
        "    outputs = layers.Dense(NUM_CLASSES, activation=\"sigmoid\")(x)\n",
        "\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs, name=\"pointnet\")\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "xveklusUk1T9"
      },
      "outputs": [],
      "source": [
        "NUM_POINTS = 1024\n",
        "#ebcause alpha_s can get 3 values\n",
        "NUM_CLASSES = 3\n",
        "\n",
        "pointnet=build_pointnet_classifier_model(NUM_POINTS,NUM_CLASSES)\n",
        "learning_rate=0.001\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "UgTjO5munWUX"
      },
      "outputs": [],
      "source": [
        "def compile_pointnet_classifier_model_with_hyperparam(model,learning_rate):\n",
        "  optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "  model.compile(\n",
        "      loss=\"sparse_categorical_crossentropy\",\n",
        "      # loss='categorical_crossentropy',\n",
        "      optimizer=optimizer,\n",
        "      metrics=[\"sparse_categorical_accuracy\"],\n",
        "      # metrics=[\"accuracy\"],\n",
        "  )\n",
        "  model.summary()\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6sK2bSktpOO",
        "outputId": "edb7fa42-bad8-467f-be80-0cbee4d27caa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"pointnet\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)        [(None, 1024, 3)]            0         []                            \n",
            "                                                                                                  \n",
            " conv1d (Conv1D)             (None, 1024, 32)             128       ['input_1[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization (Batch  (None, 1024, 32)             128       ['conv1d[0][0]']              \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation (Activation)     (None, 1024, 32)             0         ['batch_normalization[0][0]'] \n",
            "                                                                                                  \n",
            " conv1d_1 (Conv1D)           (None, 1024, 64)             2112      ['activation[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_1 (Bat  (None, 1024, 64)             256       ['conv1d_1[0][0]']            \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " activation_1 (Activation)   (None, 1024, 64)             0         ['batch_normalization_1[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " conv1d_2 (Conv1D)           (None, 1024, 512)            33280     ['activation_1[0][0]']        \n",
            "                                                                                                  \n",
            " batch_normalization_2 (Bat  (None, 1024, 512)            2048      ['conv1d_2[0][0]']            \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " activation_2 (Activation)   (None, 1024, 512)            0         ['batch_normalization_2[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " global_max_pooling1d (Glob  (None, 512)                  0         ['activation_2[0][0]']        \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " dense (Dense)               (None, 256)                  131328    ['global_max_pooling1d[0][0]']\n",
            "                                                                                                  \n",
            " batch_normalization_3 (Bat  (None, 256)                  1024      ['dense[0][0]']               \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " activation_3 (Activation)   (None, 256)                  0         ['batch_normalization_3[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " dense_1 (Dense)             (None, 128)                  32896     ['activation_3[0][0]']        \n",
            "                                                                                                  \n",
            " batch_normalization_4 (Bat  (None, 128)                  512       ['dense_1[0][0]']             \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " activation_4 (Activation)   (None, 128)                  0         ['batch_normalization_4[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " dense_2 (Dense)             (None, 9)                    1161      ['activation_4[0][0]']        \n",
            "                                                                                                  \n",
            " reshape (Reshape)           (None, 3, 3)                 0         ['dense_2[0][0]']             \n",
            "                                                                                                  \n",
            " dot (Dot)                   (None, 1024, 3)              0         ['input_1[0][0]',             \n",
            "                                                                     'reshape[0][0]']             \n",
            "                                                                                                  \n",
            " conv1d_3 (Conv1D)           (None, 1024, 32)             128       ['dot[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_5 (Bat  (None, 1024, 32)             128       ['conv1d_3[0][0]']            \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " activation_5 (Activation)   (None, 1024, 32)             0         ['batch_normalization_5[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " conv1d_4 (Conv1D)           (None, 1024, 32)             1056      ['activation_5[0][0]']        \n",
            "                                                                                                  \n",
            " batch_normalization_6 (Bat  (None, 1024, 32)             128       ['conv1d_4[0][0]']            \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " activation_6 (Activation)   (None, 1024, 32)             0         ['batch_normalization_6[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " conv1d_5 (Conv1D)           (None, 1024, 32)             1056      ['activation_6[0][0]']        \n",
            "                                                                                                  \n",
            " batch_normalization_7 (Bat  (None, 1024, 32)             128       ['conv1d_5[0][0]']            \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " activation_7 (Activation)   (None, 1024, 32)             0         ['batch_normalization_7[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " conv1d_6 (Conv1D)           (None, 1024, 64)             2112      ['activation_7[0][0]']        \n",
            "                                                                                                  \n",
            " batch_normalization_8 (Bat  (None, 1024, 64)             256       ['conv1d_6[0][0]']            \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " activation_8 (Activation)   (None, 1024, 64)             0         ['batch_normalization_8[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " conv1d_7 (Conv1D)           (None, 1024, 512)            33280     ['activation_8[0][0]']        \n",
            "                                                                                                  \n",
            " batch_normalization_9 (Bat  (None, 1024, 512)            2048      ['conv1d_7[0][0]']            \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " activation_9 (Activation)   (None, 1024, 512)            0         ['batch_normalization_9[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " global_max_pooling1d_1 (Gl  (None, 512)                  0         ['activation_9[0][0]']        \n",
            " obalMaxPooling1D)                                                                                \n",
            "                                                                                                  \n",
            " dense_3 (Dense)             (None, 256)                  131328    ['global_max_pooling1d_1[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " batch_normalization_10 (Ba  (None, 256)                  1024      ['dense_3[0][0]']             \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_10 (Activation)  (None, 256)                  0         ['batch_normalization_10[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " dense_4 (Dense)             (None, 128)                  32896     ['activation_10[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_11 (Ba  (None, 128)                  512       ['dense_4[0][0]']             \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_11 (Activation)  (None, 128)                  0         ['batch_normalization_11[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " dense_5 (Dense)             (None, 1024)                 132096    ['activation_11[0][0]']       \n",
            "                                                                                                  \n",
            " reshape_1 (Reshape)         (None, 32, 32)               0         ['dense_5[0][0]']             \n",
            "                                                                                                  \n",
            " dot_1 (Dot)                 (None, 1024, 32)             0         ['activation_6[0][0]',        \n",
            "                                                                     'reshape_1[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_8 (Conv1D)           (None, 1024, 32)             1056      ['dot_1[0][0]']               \n",
            "                                                                                                  \n",
            " batch_normalization_12 (Ba  (None, 1024, 32)             128       ['conv1d_8[0][0]']            \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_12 (Activation)  (None, 1024, 32)             0         ['batch_normalization_12[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv1d_9 (Conv1D)           (None, 1024, 64)             2112      ['activation_12[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_13 (Ba  (None, 1024, 64)             256       ['conv1d_9[0][0]']            \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_13 (Activation)  (None, 1024, 64)             0         ['batch_normalization_13[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv1d_10 (Conv1D)          (None, 1024, 512)            33280     ['activation_13[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_14 (Ba  (None, 1024, 512)            2048      ['conv1d_10[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_14 (Activation)  (None, 1024, 512)            0         ['batch_normalization_14[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " global_max_pooling1d_2 (Gl  (None, 512)                  0         ['activation_14[0][0]']       \n",
            " obalMaxPooling1D)                                                                                \n",
            "                                                                                                  \n",
            " dense_6 (Dense)             (None, 256)                  131328    ['global_max_pooling1d_2[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " batch_normalization_15 (Ba  (None, 256)                  1024      ['dense_6[0][0]']             \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_15 (Activation)  (None, 256)                  0         ['batch_normalization_15[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " dropout (Dropout)           (None, 256)                  0         ['activation_15[0][0]']       \n",
            "                                                                                                  \n",
            " dense_7 (Dense)             (None, 128)                  32896     ['dropout[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_16 (Ba  (None, 128)                  512       ['dense_7[0][0]']             \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_16 (Activation)  (None, 128)                  0         ['batch_normalization_16[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " dropout_1 (Dropout)         (None, 128)                  0         ['activation_16[0][0]']       \n",
            "                                                                                                  \n",
            " dense_8 (Dense)             (None, 3)                    387       ['dropout_1[0][0]']           \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 748076 (2.85 MB)\n",
            "Trainable params: 741996 (2.83 MB)\n",
            "Non-trainable params: 6080 (23.75 KB)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "pointnet=compile_pointnet_classifier_model_with_hyperparam(pointnet,learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "hXGQAvpJtN8t"
      },
      "outputs": [],
      "source": [
        "# Classifiers\n",
        "classifiers = {\n",
        "    'Pointnet': pointnet,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "nhYozTcXX1YW"
      },
      "outputs": [],
      "source": [
        "def get_coordinates(image_array):\n",
        "  import numpy as np\n",
        "  # Get the dimensions of the original array\n",
        "  height, width = image_array.shape\n",
        "  # Create an array of coordinates (x, y)\n",
        "  coordinates = np.column_stack((np.repeat(np.arange(height), width),\n",
        "                                np.tile(np.arange(width), height)))\n",
        "  return coordinates\n",
        "def get_point_clouds(image_array,coordinates):\n",
        "  # Assuming image_array is your 32x32 numpy array\n",
        "  # image_array = np.random.randint(0, 256, (32, 32), dtype=np.uint8)\n",
        "  # Create an nx3 array with x, y, and intensity values\n",
        "  result_array = np.column_stack((coordinates, image_array.flatten()))\n",
        "  return result_array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "vlFbDGl6XVQs"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def get_dataset_points(dataset_x):\n",
        "    \"\"\"\n",
        "    Get 3D points for each entry in the dataset.\n",
        "\n",
        "    Parameters:\n",
        "    - dataset: 3D array-like, the dataset containing non-zero values.\n",
        "\n",
        "    Returns:\n",
        "    - dataset_points: NumPy array, each entry corresponds to the 3D points of non-zero values for a particular entry in the dataset.\n",
        "\n",
        "    \"\"\"\n",
        "    dataset_points = []\n",
        "    coordinates=get_coordinates(dataset_x[0])\n",
        "\n",
        "    for data in dataset_x:\n",
        "\n",
        "        point_clouds=get_point_clouds(data,coordinates)\n",
        "        # Append coordinates to the list\n",
        "        dataset_points.append(point_clouds)\n",
        "\n",
        "    # Convert the list of coordinates to a NumPy array\n",
        "    dataset_points = np.array(dataset_points)\n",
        "    return dataset_points"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "6CBjsFewAJxe"
      },
      "outputs": [],
      "source": [
        "def split_dataset(dataset_x, dataset_x_points, dataset_y, test_size=0.2, random_state=None):\n",
        "    \"\"\"\n",
        "    Split the dataset into training and testing sets.\n",
        "\n",
        "    Parameters:\n",
        "    - dataset_x: The input data (3D array).\n",
        "    - dataset_x_points: The 3D coordinates corresponding to each entry.\n",
        "    - dataset_y: The target values (2D array).\n",
        "    - test_size: The proportion of the dataset to include in the test split.\n",
        "    - random_state: Seed for random number generation.\n",
        "\n",
        "    Returns:\n",
        "    - x_train, x_test: The split input data for training and testing.\n",
        "    - x_train_points, x_test_points: The split 3D coordinates for training and testing.\n",
        "    - y_train, y_test: The split target values for training and testing.\n",
        "    \"\"\"\n",
        "    # Flatten the input data to 2D\n",
        "    flattened_dataset_x = dataset_x.reshape(dataset_x.shape[0], -1)\n",
        "\n",
        "    # Split the dataset\n",
        "    x_train, x_test, x_train_points, x_test_points, y_train, y_test = \\\n",
        "        train_test_split(flattened_dataset_x, dataset_x_points, dataset_y, test_size=test_size, random_state=random_state)\n",
        "\n",
        "    # Reshape the input data back to 3D\n",
        "    x_train = x_train.reshape(x_train.shape[0], dataset_x.shape[1], dataset_x.shape[2])\n",
        "    x_test = x_test.reshape(x_test.shape[0], dataset_x.shape[1], dataset_x.shape[2])\n",
        "\n",
        "    return x_train, x_test, x_train_points, x_test_points, y_train, y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "5sWogII7Z06-"
      },
      "outputs": [],
      "source": [
        "def parse_dataset(x_train,x_test):\n",
        "    train_labels = []\n",
        "    test_labels = []\n",
        "    i=0\n",
        "    for f in x_train:\n",
        "\n",
        "        train_labels.append(i)\n",
        "        i=i+1\n",
        "    for f in x_test:\n",
        "        test_labels.append(i)\n",
        "        i=i+1\n",
        "    return (\n",
        "        np.array(train_labels),\n",
        "        np.array(test_labels),\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "FH9B4SCcaNev"
      },
      "outputs": [],
      "source": [
        "def augment(points, label):\n",
        "    # jitter points\n",
        "    points += tf.random.uniform(points.shape, -0.005, 0.005, dtype=tf.float64)\n",
        "    # shuffle points\n",
        "    points = tf.random.shuffle(points)\n",
        "    return points, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "8zniYk7xKyBy"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, x_test, y_test):\n",
        "    \"\"\"\n",
        "    Evaluate the performance of a trained model on test data.\n",
        "\n",
        "    Parameters:\n",
        "    - model: The trained deep neural model.\n",
        "    - x_test: Test data, it shall be in the cloud points format, each entry contains 1024x3 data.\n",
        "    - y_test: True labels.\n",
        "\n",
        "    Returns:\n",
        "    - accuracy: Accuracy of the model on the test data.\n",
        "    - confusion_matrix: Confusion matrix for the predictions.\n",
        "    \"\"\"\n",
        "    # Assuming model is your trained deep neural model\n",
        "    y_pred = model.predict(x_test)\n",
        "    y_pred_class = np.argmax(y_pred, axis=1)  # Extracting the class with the highest probability\n",
        "\n",
        "    # Assuming y_true is a Nx2 array where each row contains the true class probabilities\n",
        "    y_true_class = np.argmax(y_test, axis=1)  # Extracting the class with the highest true probability\n",
        "\n",
        "    accuracy = accuracy_score(y_true_class, y_pred_class)\n",
        "    print(f'Accuracy: {accuracy}')\n",
        "\n",
        "    cm = confusion_matrix(y_true_class, y_pred_class)\n",
        "    print(f'Confusion Matrix: {cm}')\n",
        "\n",
        "    return accuracy, cm\n",
        "\n",
        "# Example usage:\n",
        "# accuracy, confusion_matrix = evaluate_model(trained_model, test_data, true_labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "Jbi8LtS3bOtH"
      },
      "outputs": [],
      "source": [
        "def preprocess_dataset(dataset_x, dataset_y):\n",
        "  print(\"Pre-processing\")\n",
        "  # Example usage:\n",
        "  dataset_x_points = get_dataset_points(dataset_x)\n",
        "  print(\"dataset_x_points shape:\", dataset_x_points.shape)\n",
        "  x_train, x_test, x_train_points, x_test_points, y_train, y_test= \\\n",
        "    split_dataset(dataset_x, dataset_x_points, dataset_y, test_size=0.2, random_state=None)\n",
        "  print(\"deleting the original dataset after splitting ...\")\n",
        "  del dataset_x,dataset_x_points,dataset_y\n",
        "  print(\"train_x:\",type(x_train), x_train.size, x_train.shape)\n",
        "  print(\"train_points:\",type(x_train_points), x_train_points.size, x_train_points.shape)\n",
        "  print(\"train_y:\",type(y_train), y_train.size,y_train.shape)\n",
        "\n",
        "  print(\"x_test:\",type(x_test), x_test.size, x_test.shape)\n",
        "  print(\"x_test_points:\",type(x_test_points), x_test_points.size, x_test_points.shape)\n",
        "  print(\"y_test:\",type(y_test), y_test.size,y_test.shape)\n",
        "  train_labels, test_labels = parse_dataset(x_train,x_test)\n",
        "  print(train_labels.size,test_labels.size)\n",
        "\n",
        "\n",
        "  #begin:not using this part\n",
        "  train_dataset = tf.data.Dataset.from_tensor_slices((x_train_points, train_labels))\n",
        "  test_dataset = tf.data.Dataset.from_tensor_slices((x_test_points, test_labels))\n",
        "\n",
        "\n",
        "  BATCH_SIZE = 32\n",
        "\n",
        "  train_dataset = train_dataset.shuffle(len(x_train_points)).map(augment).batch(BATCH_SIZE)\n",
        "  test_dataset = test_dataset.shuffle(len(x_test_points)).batch(BATCH_SIZE)\n",
        "  #end: not using this part\n",
        "\n",
        "  # Preprocess y_train and y_test\n",
        "  # One-hot encode the categorical variable\n",
        "  # print(y_train)\n",
        "  y_train_categorical = np.array(y_train).reshape(-1, 1)\n",
        "  y_test_categorical = np.array(y_test).reshape(-1, 1)\n",
        "\n",
        "  encoder = OneHotEncoder(sparse_output=False)\n",
        "  y_train_categorical_encoded = encoder.fit_transform(y_train_categorical)\n",
        "  y_test_categorical_encoded = encoder.transform(y_test_categorical)\n",
        "  print(y_test_categorical[:10])\n",
        "  print(y_test_categorical_encoded[:10])\n",
        "  return (x_train_points,  y_train_categorical_encoded,x_test_points,  y_test_categorical_encoded)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "ILdXu--e2P_6"
      },
      "outputs": [],
      "source": [
        "def plot_training_history(history,simulation_path):\n",
        "    # Plot training & validation accuracy values\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['accuracy'])\n",
        "    plt.plot(history.history['val_accuracy'])\n",
        "    plt.title('Model Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "    # Set ticks on the epoch axis to display only integer values\n",
        "    plt.xticks(range(0, len(history.history['accuracy'])+1,5))\n",
        "\n",
        "    # Plot training & validation loss values\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.plot(history.history['val_loss'])\n",
        "    plt.title('Model Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "    # Set ticks on the epoch axis to display only integer values\n",
        "    plt.xticks(range(0, len(history.history['accuracy'])+1,5))\n",
        "\n",
        "    # Adjust layout and show the plot\n",
        "    plt.tight_layout()\n",
        "\n",
        "\n",
        "    # Save the plot with high resolution (300 dpi)\n",
        "    file_name='_accuracy_loss.png'\n",
        "    file_path=simulation_path+file_name\n",
        "    plt.savefig(file_path, dpi=300)\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "    return file_path\n",
        "\n",
        "# Plot the training history\n",
        "# plot_training_history_path=plot_training_history(history,simulation_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "pUyg0Teluaa0"
      },
      "outputs": [],
      "source": [
        "def save_training_history(history,simulation_path):\n",
        "  # Save the training history to a file (e.g., JSON format)\n",
        "\n",
        "  training_history_file_path =simulation_path+'_training_history'\n",
        "  # training_history_file_path  =simulation_directory_path+training_history_file_name\n",
        "\n",
        "  training_history_file_path_json=training_history_file_path+'.json'\n",
        "  with open(training_history_file_path_json, 'w') as f:\n",
        "      json.dump(history.history, f)\n",
        "  print(training_history_file_path_json)\n",
        "\n",
        "  training_history_file_path_csv=training_history_file_path+'.csv'\n",
        "  pd.DataFrame.from_dict(history.history).to_csv(training_history_file_path_csv,index=False)\n",
        "  print(training_history_file_path_csv)\n",
        "\n",
        "  training_history_file_path_npy=training_history_file_path+'.npy'\n",
        "  np.save(training_history_file_path_npy,history.history)\n",
        "  print(training_history_file_path_npy)\n",
        "  return training_history_file_path_json,training_history_file_path_csv,training_history_file_path_csv\n",
        "\n",
        "# training_history_file_path_json,training_history_file_path_csv,training_history_file_path_csv = \\\n",
        "#   save_training_history(history,simulation_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "4Txe-hJQNEZx"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Function to train and evaluate classifiers\n",
        "# This method shall get the cloud points as the trainset, to be trained by pointnet\n",
        "def train_and_evaluate_classifier_kfold(model, x_train,y_train , x_test, y_test, n_epochs, monitor, k_folds,simulation_path):\n",
        "\n",
        "\n",
        "    accuracies = []\n",
        "    cms = []\n",
        "    train_times = []\n",
        "    all_histories = []  # Store histories for each fold\n",
        "    plots=[]\n",
        "    models=[]\n",
        "\n",
        "\n",
        "    if k_folds==1:\n",
        "      print(\"Fold #1: Begin\")\n",
        "      fold=1\n",
        "      x_train_fold, x_val_fold,y_train_fold, y_val_fold = \\\n",
        "        train_test_split(x_train, y_train, test_size=0.1, random_state=None)\n",
        "      # Clear any previous TensorFlow session\n",
        "      tf.keras.backend.clear_session()\n",
        "\n",
        "      # Include both ModelCheckpoint and History callbacks in the callbacks list\n",
        "      # callbacks=[checkpoint_callback]\n",
        "      fold_path=f'{simulation_path}_fold_{fold}'\n",
        "      # Use ModelCheckpoint callback to save the best model\n",
        "      best_model_file_path = f'{fold_path}_best_model.keras'\n",
        "      models.append(best_model_file_path)\n",
        "      checkpoint_callback = ModelCheckpoint(\n",
        "          best_model_file_path,\n",
        "          monitor=monitor,\n",
        "          save_best_only=True,\n",
        "          mode=\"max\" if monitor == \"val_accuracy\" else \"min\",\n",
        "          verbose=1\n",
        "      )\n",
        "      earlystop_callback = EarlyStopping(monitor=monitor, patience=10, verbose=1)\n",
        "      callbacks = [checkpoint_callback, earlystop_callback]\n",
        "      # Use History callback to retrieve the training history\n",
        "      history_callback = History()\n",
        "\n",
        "      start = time()\n",
        "\n",
        "\n",
        "      # Create the model\n",
        "      # model_fold = compile_pointnet_binary_classifier_model_with_hyperparam()\n",
        "      history = model.fit(\n",
        "          x_train_fold,\n",
        "          y_train_fold,\n",
        "          epochs=n_epochs,\n",
        "          validation_data=(x_val_fold, y_val_fold),\n",
        "          callbacks=callbacks\n",
        "          #  [checkpoint_callback]\n",
        "      )\n",
        "      train_time = (time()-start)/60.0\n",
        "      train_times.append(train_time)\n",
        "      # Append history to the list\n",
        "      all_histories.append(history.history)\n",
        "\n",
        "      plot=plot_training_history(history,fold_path)\n",
        "      training_history_file_path_json,training_history_file_path_csv,training_history_file_path_csv = \\\n",
        "      save_training_history(history,fold_path)\n",
        "      plots.append(plot)\n",
        "\n",
        "      accuracy, confusion_matrix = evaluate_model(model, x_test, y_test)\n",
        "      accuracies.append(accuracy)\n",
        "      cms.append(confusion_matrix)\n",
        "      print(\"Fold #1: End\")\n",
        "    else:\n",
        "\n",
        "      # Use KFold for k-fold cross-validation\n",
        "      kfold = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
        "\n",
        "      for fold, (train_index, val_index) in enumerate(kfold.split(x_train)):\n",
        "      # train_index, val_index in kfold.split(x_train):\n",
        "          x_train_fold, x_val_fold = x_train[train_index], x_train[val_index]\n",
        "          y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n",
        "\n",
        "          # Clear any previous TensorFlow session\n",
        "          tf.keras.backend.clear_session()\n",
        "\n",
        "          # Include both ModelCheckpoint and History callbacks in the callbacks list\n",
        "          # callbacks=[checkpoint_callback]\n",
        "          fold_path=f'{simulation_path}_fold_{fold}'\n",
        "          # Use ModelCheckpoint callback to save the best model\n",
        "          best_model_file_path = f'{fold_path}_best_model.keras'\n",
        "          models.append(best_model_file_path)\n",
        "          checkpoint_callback = ModelCheckpoint(\n",
        "              best_model_file_path,\n",
        "              monitor=monitor,\n",
        "              save_best_only=True,\n",
        "              mode=\"max\" if monitor == \"val_accuracy\" else \"min\",\n",
        "              verbose=1\n",
        "          )\n",
        "          earlystop_callback = EarlyStopping(monitor=monitor, patience=10, verbose=1)\n",
        "          callbacks = [checkpoint_callback, earlystop_callback]\n",
        "          # Use History callback to retrieve the training history\n",
        "          history_callback = History()\n",
        "\n",
        "          start = time()\n",
        "\n",
        "\n",
        "          # Create the model\n",
        "          # model_fold = compile_pointnet_binary_classifier_model_with_hyperparam()\n",
        "          history = model.fit(\n",
        "              x_train_fold,\n",
        "              y_train_fold,\n",
        "              epochs=n_epochs,\n",
        "              validation_data=(x_val_fold, y_val_fold),\n",
        "              callbacks=callbacks\n",
        "              #  [checkpoint_callback]\n",
        "          )\n",
        "          train_time = (time()-start)/60.0\n",
        "          train_times.append(train_time)\n",
        "          # Append history to the list\n",
        "          all_histories.append(history.history)\n",
        "\n",
        "          plot=plot_training_history(history,fold_path)\n",
        "          training_history_file_path_json,training_history_file_path_csv,training_history_file_path_csv = \\\n",
        "          save_training_history(history,fold_path)\n",
        "          plots.append(plot)\n",
        "\n",
        "          accuracy, confusion_matrix = evaluate_model(model, x_test, y_test)\n",
        "          accuracies.append(accuracy)\n",
        "          cms.append(confusion_matrix)\n",
        "\n",
        "    return accuracies, cms, train_times, all_histories, plots,models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "PeY0tSPQ6lVJ"
      },
      "outputs": [],
      "source": [
        "# #example for testing classifier k fold training for a dataset size manually\n",
        "# monitor = 'val_accuracy'  # 'val_accuracy' or 'val_loss'\n",
        "# n_epochs = 50\n",
        "# k_folds = 10  # You can adjust the number of folds\n",
        "\n",
        "# print(simulation_directory_path)\n",
        "# current_simulation_name=f'jetml_pointnet_classification_eloss_{class_labels_str}_size_{1000}'\n",
        "# current_simulation_path=simulation_directory_path+current_simulation_name\n",
        "# print(current_simulation_path)\n",
        "\n",
        "# (dataset_x, dataset_y)= get_dataset(1000)\n",
        "# (x_train,  y_train,x_test,  y_test)=preprocess_dataset(dataset_x, dataset_y)\n",
        "# accuracies, cms, train_times, all_histories, plots,models = \\\n",
        "#     train_and_evaluate_classifier_kfold(pointnet, x_train,  y_train,x_test,  y_test, n_epochs, monitor, k_folds,current_simulation_path)\n",
        "\n",
        "# print(f'Accuracies: {accuracies}')\n",
        "# print(f'Confusion Matrices: {cms}')\n",
        "# print(f'Train Times: {train_times} minutes')\n",
        "# print(f'Plot Paths: {plots}')\n",
        "# print(f'Model Paths: {models}')\n",
        "# # Calculate average accuracy and confusion matrix\n",
        "# avg_accuracy = np.mean(accuracies)\n",
        "# avg_cm = np.mean(cms, axis=0)\n",
        "\n",
        "# print(f'Average Accuracy: {avg_accuracy}')\n",
        "# print(f'Average Confusion Matrix: {avg_cm}')\n",
        "# print(f'Average Train Time: {np.mean(train_times)} minutes')\n",
        "\n",
        "# print(f'Histories:{all_histories}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "lzP5rLoMLRqp"
      },
      "outputs": [],
      "source": [
        "# Load the DataFrame from the saved file\n",
        "def load_csv_into_dataframe(file_path):\n",
        "  df_results = pd.read_csv(file_path, sep='\\t')\n",
        "  return df_results\n",
        "#example usage\n",
        "# df_results=load_csv_into_dataframe(\"/content/drive/MyDrive/Colab Notebooks/binary_classification_results_kfold_errorbar.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "3lYKLbf1TxpA"
      },
      "outputs": [],
      "source": [
        "def plot_save_mean_error_bar(df_results,simulation_path):\n",
        "    # df_results= df_results_kfold_errorbar\n",
        "    print(df_results)\n",
        "    # Set a seaborn style (optional)\n",
        "    sns.set(style=\"whitegrid\")\n",
        "\n",
        "\n",
        "    # Define a dictionary to map classifiers to markers\n",
        "    marker_dict = {\n",
        "        'Pointnet': 'o',\n",
        "        # 'model1': 's',\n",
        "        # 'model2': '^',\n",
        "        # 'model3': 'v',\n",
        "        # 'model4': 'D'\n",
        "    }\n",
        "    plt.figure(figsize=(10, 6))\n",
        "\n",
        "    for clf_name, group in df_results.groupby('Classifier'):\n",
        "        plt.errorbar(\n",
        "            group['Dataset_Size'],\n",
        "            group['Mean_Accuracy'],\n",
        "            yerr=group['Std_Accuracy'],\n",
        "            label=clf_name,\n",
        "            marker=marker_dict.get(clf_name, 'o'),  # Use 'o' as default marker if not found in the dictionary\n",
        "            capsize=5\n",
        "        )\n",
        "\n",
        "    plt.xscale('log')  # Set x-axis to logarithmic scale\n",
        "    plt.xlabel('Dataset Size (log scale)')\n",
        "    plt.ylabel('Mean Accuracy')\n",
        "    plt.title('Classification Accuracy with Error Bars for Different Dataset Sizes')\n",
        "    plt.legend()\n",
        "    # plt.grid(True)\n",
        "    # Save the plot with high resolution (300 dpi)\n",
        "    accuracy_errorbar_plot_path=simulation_path+'_accuracy_errorbar_plot.png'\n",
        "    plt.savefig(accuracy_errorbar_plot_path, dpi=300)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "lFHDNMtlQ6Zh"
      },
      "outputs": [],
      "source": [
        "def train_and_evaluate_classifier_for_all_datasets(dataset_sizes,classifiers,simulation_path,n_epochs, monitor, k_folds):\n",
        "  print(simulation_path)\n",
        "\n",
        "  # Results storage\n",
        "  results_kfold = []\n",
        "  results_kfold_errorbar=[]\n",
        "  # Loop through different dataset sizes\n",
        "  for size in dataset_sizes:\n",
        "      current_simulation_name=f'_size_{size}'\n",
        "      current_simulation_path=simulation_path+current_simulation_name\n",
        "      print(current_simulation_path)\n",
        "      # Generate dataset\n",
        "      # x, y = get_dataset(size)\n",
        "      (dataset_x, dataset_y)= get_dataset(size)\n",
        "      (x_train,  y_train,x_test,  y_test)=preprocess_dataset(dataset_x, dataset_y)\n",
        "\n",
        "      # Loop through classifiers\n",
        "      for clf_name, clf in classifiers.items():\n",
        "\n",
        "          # Evaluate classifier using k-fold cross-validation\n",
        "          fold_accuracies, fold_conf_matrices, fold_train_times, all_histories, plots, models = \\\n",
        "          train_and_evaluate_classifier_kfold(pointnet, x_train,  y_train,x_test,  y_test, n_epochs, monitor, k_folds,current_simulation_path)\n",
        "\n",
        "          # Store results for each fold\n",
        "          for fold_num, (accuracy, cm,train_time,plot,model) in enumerate(zip(fold_accuracies, fold_conf_matrices,fold_train_times,plots,models), start=1):\n",
        "              results_kfold.append({\n",
        "                  'Dataset Size': size,\n",
        "                  'Classifier': clf_name,\n",
        "                  'Fold Number': fold_num,\n",
        "                  'Accuracy': accuracy,\n",
        "                  'Confusion Matrix': cm,\n",
        "                  'Train Time': train_time,\n",
        "                  'Loss/Accuracy Plot Path': plot,\n",
        "                  'Best Model Path': model\n",
        "              })\n",
        "          print(f'Average Train Time: {np.mean(fold_train_times)} minutes')\n",
        "          # Calculate mean and standard deviation of accuracy scores\n",
        "          mean_accuracy = np.mean(fold_accuracies)\n",
        "          std_accuracy = np.std(fold_accuracies)\n",
        "\n",
        "          # Store results\n",
        "          results_kfold_errorbar.append({\n",
        "              'Dataset_Size': size,\n",
        "              'Classifier': clf_name,\n",
        "              'Mean_Accuracy': mean_accuracy,\n",
        "              'Std_Accuracy': std_accuracy\n",
        "          })\n",
        "  # Create a DataFrame from k-fold results\n",
        "  df_results_kfold = pd.DataFrame(results_kfold)\n",
        "  # Save the DataFrame to a text file\n",
        "  results_kfold_path=simulation_path+'_results_kfold.txt'\n",
        "  df_results_kfold.to_csv(results_kfold_path, index=False, sep='\\t')\n",
        "  # Display results in a table\n",
        "  print(df_results_kfold)\n",
        "\n",
        "  # Create a DataFrame from k-fold results\n",
        "  df_results_kfold_errorbar = pd.DataFrame(results_kfold_errorbar)\n",
        "  # Save the DataFrame to a text file\n",
        "  results_kfold_errorbar_path=simulation_path+'_results_kfold_errorbar.txt'\n",
        "  df_results_kfold_errorbar.to_csv(results_kfold_errorbar_path, index=False, sep='\\t')\n",
        "  # Display results in a table\n",
        "  print(df_results_kfold_errorbar)\n",
        "  plot_save_mean_error_bar(df_results_kfold_errorbar,simulation_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XHqJCbwcA1oq",
        "outputId": "b913e6e3-201b-4497-a600-84d0f5892112"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "simulation_directory_path: /content/drive/MyDrive/Projects/110_JetscapeMl/hm.jetscapeml.data/simulation_results/\n",
            "simulation_path: /content/drive/MyDrive/Projects/110_JetscapeMl/hm.jetscapeml.data/simulation_results/jetml_pointnet_classification_alpha_s_0.2_0.3_0.4\n"
          ]
        }
      ],
      "source": [
        "monitor = 'val_accuracy'  # 'val_accuracy' or 'val_loss'\n",
        "n_epochs = 5\n",
        "k_folds = 1 # You can adjust the number of folds\n",
        "\n",
        "# defining dataset sizes and classifiers\n",
        "\n",
        "# Sizes of datasets\n",
        "dataset_sizes = [1000]\n",
        "# dataset_sizes = [100000]\n",
        "# dataset_sizes = [1000000]\n",
        "# dataset_sizes = [1000, 10000]\n",
        "#dataset_sizes = [1000, 10000,100000]\n",
        "# dataset_sizes = [1000, 10000, 100000, 1000000]\n",
        "\n",
        "print(\"simulation_directory_path:\",simulation_directory_path)\n",
        "simulation_path=f'{simulation_directory_path}jetml_pointnet_classification_alpha_s_{label_str_dict[\"alpha_s_items_str\"]}'\n",
        "print(\"simulation_path:\",simulation_path)\n",
        "# train_and_evaluate_classifier_for_all_datasets(dataset_sizes,classifiers,simulation_path,n_epochs, monitor, k_folds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "67SoiyITnFj1",
        "outputId": "ce12c484-a92c-4ccf-8ea6-378d66206116"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Projects/110_JetscapeMl/hm.jetscapeml.data/simulation_results/jetml_pointnet_classification_alpha_s_0.2_0.3_0.4\n",
            "/content/drive/MyDrive/Projects/110_JetscapeMl/hm.jetscapeml.data/simulation_results/jetml_pointnet_classification_alpha_s_0.2_0.3_0.4_size_1000\n",
            "dataset_file_name: /content/drive/MyDrive/Projects/110_JetscapeMl/hm.jetscapeml.data/jet_ml_benchmark_config_01_to_09_alpha_0.2_0.3_0.4_q0_1.5_2.0_2.5_MMAT_MLBT_size_1000_shuffled.pkl\n",
            "dataset.x: <class 'numpy.ndarray'> 1024000 (1000, 32, 32)\n",
            "dataset.y: <class 'numpy.ndarray'> 3000 (1000, 3)\n",
            "Extract the working column#0 for classification\n",
            "dataset.x: <class 'numpy.ndarray'> 1024000 (1000, 32, 32)\n",
            "dataset.y: <class 'numpy.ndarray'> 1000 (1000,)\n",
            "dataset.y(working_column) sample ['MLBT' 'MLBT' 'MLBT' 'MMAT' 'MMAT' 'MMAT' 'MMAT' 'MMAT' 'MLBT' 'MLBT']\n",
            "Pre-processing\n",
            "dataset_x_points shape: (1000, 1024, 3)\n",
            "deleting the original dataset after splitting ...\n",
            "train_x: <class 'numpy.ndarray'> 819200 (800, 32, 32)\n",
            "train_points: <class 'numpy.ndarray'> 2457600 (800, 1024, 3)\n",
            "train_y: <class 'numpy.ndarray'> 800 (800,)\n",
            "x_test: <class 'numpy.ndarray'> 204800 (200, 32, 32)\n",
            "x_test_points: <class 'numpy.ndarray'> 614400 (200, 1024, 3)\n",
            "y_test: <class 'numpy.ndarray'> 200 (200,)\n",
            "800 200\n",
            "[['MLBT']\n",
            " ['MMAT']\n",
            " ['MLBT']\n",
            " ['MMAT']\n",
            " ['MMAT']\n",
            " ['MLBT']\n",
            " ['MMAT']\n",
            " ['MLBT']\n",
            " ['MMAT']\n",
            " ['MMAT']]\n",
            "[[1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]]\n",
            "Fold #1: Begin\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1155, in train_step\n        return self.compute_metrics(x, y, y_pred, sample_weight)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1249, in compute_metrics\n        self.compiled_metrics.update_state(y, y_pred, sample_weight)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/compile_utils.py\", line 620, in update_state\n        metric_obj.update_state(y_t, y_p, sample_weight=mask)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/metrics_utils.py\", line 77, in decorated\n        result = update_state_fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/metrics/base_metric.py\", line 140, in update_state_fn\n        return ag_update_state(*args, **kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/metrics/base_metric.py\", line 723, in update_state  **\n        matches = ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/metrics/accuracy_metrics.py\", line 459, in sparse_categorical_accuracy\n        matches = metrics_utils.sparse_categorical_matches(y_true, y_pred)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/metrics_utils.py\", line 961, in sparse_categorical_matches\n        y_true = tf.squeeze(y_true, [-1])\n\n    ValueError: Can not squeeze dim[1], expected a dimension of 1, got 2 for '{{node Squeeze}} = Squeeze[T=DT_FLOAT, squeeze_dims=[-1]](IteratorGetNext:1)' with input shapes: [?,2].\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-510c477ba78d>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# start = time()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/GPU:0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtrain_and_evaluate_classifier_for_all_datasets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_sizes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclassifiers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msimulation_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonitor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_folds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m# gpu_train_time=time()-start\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# print(\"gpu_train_time\",gpu_train_time)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-f013b225c155>\u001b[0m in \u001b[0;36mtrain_and_evaluate_classifier_for_all_datasets\u001b[0;34m(dataset_sizes, classifiers, simulation_path, n_epochs, monitor, k_folds)\u001b[0m\n\u001b[1;32m     20\u001b[0m           \u001b[0;31m# Evaluate classifier using k-fold cross-validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m           \u001b[0mfold_accuracies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfold_conf_matrices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfold_train_times\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_histories\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplots\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m           \u001b[0mtrain_and_evaluate_classifier_kfold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpointnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonitor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_folds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcurrent_simulation_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m           \u001b[0;31m# Store results for each fold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-37-a8aa327f193d>\u001b[0m in \u001b[0;36mtrain_and_evaluate_classifier_kfold\u001b[0;34m(model, x_train, y_train, x_test, y_test, n_epochs, monitor, k_folds, simulation_path)\u001b[0m\n\u001b[1;32m     46\u001b[0m       \u001b[0;31m# Create the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m       \u001b[0;31m# model_fold = compile_pointnet_binary_classifier_model_with_hyperparam()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m       history = model.fit(\n\u001b[0m\u001b[1;32m     49\u001b[0m           \u001b[0mx_train_fold\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m           \u001b[0my_train_fold\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1155, in train_step\n        return self.compute_metrics(x, y, y_pred, sample_weight)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1249, in compute_metrics\n        self.compiled_metrics.update_state(y, y_pred, sample_weight)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/compile_utils.py\", line 620, in update_state\n        metric_obj.update_state(y_t, y_p, sample_weight=mask)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/metrics_utils.py\", line 77, in decorated\n        result = update_state_fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/metrics/base_metric.py\", line 140, in update_state_fn\n        return ag_update_state(*args, **kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/metrics/base_metric.py\", line 723, in update_state  **\n        matches = ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/metrics/accuracy_metrics.py\", line 459, in sparse_categorical_accuracy\n        matches = metrics_utils.sparse_categorical_matches(y_true, y_pred)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/metrics_utils.py\", line 961, in sparse_categorical_matches\n        y_true = tf.squeeze(y_true, [-1])\n\n    ValueError: Can not squeeze dim[1], expected a dimension of 1, got 2 for '{{node Squeeze}} = Squeeze[T=DT_FLOAT, squeeze_dims=[-1]](IteratorGetNext:1)' with input shapes: [?,2].\n"
          ]
        }
      ],
      "source": [
        "# %%timeit -n1 -r1\n",
        "\n",
        "# start = time()\n",
        "with tf.device('/GPU:0'):\n",
        "    train_and_evaluate_classifier_for_all_datasets(dataset_sizes,classifiers,simulation_path,n_epochs, monitor, k_folds)\n",
        "# gpu_train_time=time()-start\n",
        "# print(\"gpu_train_time\",gpu_train_time)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "19cSU69wnFj2"
      },
      "outputs": [],
      "source": [
        "# %%timeit -nl -rl\n",
        "# start = time()\n",
        "# with tf.device('/CPU:0'):\n",
        "#     train_and_evaluate_classifier_for_all_datasets(dataset_sizes,classifiers,simulation_path,n_epochs, monitor, k_folds)\n",
        "# gpu_train_time=time()-start\n",
        "# print(\"cpu_train_time\",gpu_train_time)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}