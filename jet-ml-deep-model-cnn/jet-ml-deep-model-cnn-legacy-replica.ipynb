{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Loading/Installing Package => Begin\\n\\n')\n",
    "# Commonly used modules\n",
    "import numpy as np\n",
    "import os\n",
    "from os import path, makedirs\n",
    "import time\n",
    "from time import time\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "\n",
    "def install(package):\n",
    "  print(\"Installing \"+package) \n",
    "  subprocess.check_call([sys.executable,\"-m\" ,\"pip\", \"install\", package])\n",
    "  print(\"Installed \"+package+\"\\n\") \n",
    "# TensorFlow and tf.keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# !pip3 install sklearn\n",
    "install(\"sklearn\")\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# !pip3 install seaborn\n",
    "install(\"seaborn\")\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Images, plots, display, and visualization\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "#reading/writing into files\n",
    "# !pip3 install pickle5\n",
    "install(\"pickle5\")\n",
    "import pickle5 as pickle\n",
    "\n",
    "\n",
    "#import cv2\n",
    "install(\"IPython\")\n",
    "import IPython\n",
    "from six.moves import urllib\n",
    "\n",
    "\n",
    "print('\\n########################################################################')\n",
    "print('Checking the running platforms\\n')\n",
    "import platform\n",
    "running_os=platform.system()\n",
    "print(\"OS: \"+running_os)\n",
    "print(\"OS version: \"+platform.release())\n",
    "\n",
    "try:\n",
    "  from google.colab import drive\n",
    "  COLAB = True\n",
    "except:\n",
    "  COLAB = False\n",
    "print(\"running on Colab: \"+str(COLAB))\n",
    "\n",
    "\n",
    "print(\"Python version: \"+platform.python_version())\n",
    "print(\"Tensorflow version: \"+tf.__version__)\n",
    "\n",
    "dataset_directory_path=''\n",
    "simulation_directory_path=''\n",
    "\n",
    "if COLAB == True:\n",
    "  drive.mount('/content/drive')\n",
    "  dataset_directory_path='/content/drive/MyDrive/Projects/110_JetscapeMl/hm.jetscapeml.data/simulation_results/'\n",
    "  simulation_directory_path=dataset_directory_path+'simulation-results-deep-model-cnn-01-1200K-config-07-epoch-50/'\n",
    "elif 'Linux' in running_os:\n",
    "  dataset_directory_path='/wsu/home/gy/gy40/gy4065/hm.jetscapeml.data/simulation_results/'\n",
    "  simulation_directory_path=dataset_directory_path+'simulation-results-deep-model-cnn-01-1200K-config-07-epoch-50/'\n",
    "else:\n",
    "  dataset_directory_path= 'G:\\\\My Drive\\\\Projects\\\\110_JetscapeMl\\\\hm.jetscapeml.data\\\\simulation_results\\\\'\n",
    "  simulation_directory_path=dataset_directory_path+'simulation-results-deep-model-cnn-01-1200K-config-07-epoch-50\\\\'\n",
    "print('Dataset Directory Path: '+dataset_directory_path)\n",
    "\n",
    "# dataset_file_name='jetscape-ml-benchmark-dataset-2k-randomized.pkl'\n",
    "# dataset_file_name='jetscape-ml-benchmark-dataset-matter-vs-lbt-2k-shuffled.pkl'\n",
    "# dataset_file_name='jetscape-ml-benchmark-dataset-matter-vs-lbt-200k-shuffled-01.pkl'\n",
    "# dataset_file_name='jetscape-ml-benchmark-dataset-matter-vs-lbt-1200k-momentum-shuffled.pkl'\n",
    "# dataset_file_name='config-01-matter-vs-lbt-simulationsize1200000-dataset-momentum-shuffled.pkl'\n",
    "# dataset_file_name='config-02-matter-vs-lbt-simulationsize1200000-dataset-momentum-shuffled.pkl'\n",
    "# dataset_file_name='config-03-matter-vs-lbt-simulationsize1200000-dataset-momentum-shuffled.pkl'\n",
    "# dataset_file_name='config-04-matter-vs-lbt-simulationsize1200000-dataset-momentum-shuffled.pkl'\n",
    "# dataset_file_name='config-05-matter-vs-lbt-simulationsize1200000-dataset-momentum-shuffled.pkl'\n",
    "# dataset_file_name='config-06-matter-vs-lbt-simulationsize1200000-dataset-momentum-shuffled.pkl'\n",
    "dataset_file_name='config-07-matter-vs-lbt-simulationsize1200000-dataset-momentum-shuffled.pkl'\n",
    "# dataset_file_name='config-08-matter-vs-lbt-simulationsize1200000-dataset-momentum-shuffled.pkl'\n",
    "# dataset_file_name='config-09-matter-vs-lbt-simulationsize1200000-dataset-momentum-shuffled.pkl'\n",
    "print(\"Dataset file name: \"+dataset_file_name)\n",
    "\n",
    "if not path.exists(simulation_directory_path):\n",
    "    makedirs(simulation_directory_path)\n",
    "print('Simulation Results Path: '+simulation_directory_path)\n",
    "print('########################################################################\\n')\n",
    "print('\\nLoading/Installing Package => End\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path, makedirs\n",
    "from glob import glob\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import json\n",
    "import shutil\n",
    "from time import time\n",
    "\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "seed = np.random.randint(1,100)\n",
    "print('Seed for random numbers: {}'.format(seed))\n",
    "print('Tensorflow version: {}'.format(tf.__version__))\n",
    "if tf.test.gpu_device_name():\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## event info\n",
    "collision = 'PbPb'\n",
    "energy = 5020\n",
    "centrality = '0_10'\n",
    "Modules = ['MATTER','LBT']\n",
    "JetptMinMax = '100_110'\n",
    "#observables = ['pt','charge','mass']\n",
    "observables = ['pt']\n",
    "kind = 'Hadron'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataset(file_name,dataset):\n",
    "    with open(file_name, 'wb') as dataset_file:\n",
    "        pickle.dump(dataset,dataset_file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "        \n",
    "def load_dataset(file_name):\n",
    "    with open(file_name, 'rb') as dataset_file:\n",
    "        (x_train, y_train), (x_test, y_test) = pickle.load(dataset_file, encoding='latin1')\n",
    "        dataset=((x_train, y_train), (x_test, y_test))\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JetscapeMlCnn:\n",
    "   # class attribute\n",
    "  \n",
    "    # Instance attribute\n",
    "    def __init__(self, x_train,y_train,x_test,y_test):\n",
    "        self.x_train=x_train\n",
    "        self.y_train=y_train\n",
    "        self.x_test=x_test\n",
    "        self.y_test=y_test\n",
    "\n",
    "\n",
    "#Loading Dataset Phase\n",
    "\n",
    "\n",
    "dataset_file_path=dataset_directory_path+dataset_file_name\n",
    "print(\"Dataset file path: \"+dataset_file_path)\n",
    "(x_train, y_train), (x_test, y_test) =load_dataset(dataset_file_path)\n",
    "\n",
    "oJetscapeMlCnn=JetscapeMlCnn(x_train, y_train, x_test, y_test)\n",
    "print(\"\\n#############################################################\")\n",
    "print(\"Post-Load: DataType Checkpoint: Begin\")\n",
    "print(type(oJetscapeMlCnn.x_train), oJetscapeMlCnn.x_train.size, oJetscapeMlCnn.x_train.shape)\n",
    "print(type(oJetscapeMlCnn.y_train), oJetscapeMlCnn.y_train.size, oJetscapeMlCnn.y_train.shape)\n",
    "print(type(oJetscapeMlCnn.x_test), oJetscapeMlCnn.x_test.size, oJetscapeMlCnn.x_test.shape)\n",
    "print(type(oJetscapeMlCnn.y_test), oJetscapeMlCnn.y_test.size, oJetscapeMlCnn.y_test.shape)\n",
    "print(oJetscapeMlCnn.y_train[1500], oJetscapeMlCnn.y_test[99])\n",
    "print(oJetscapeMlCnn.y_train[1:500])\n",
    "print(\"Post-Load: DataType Checkpoint: End\")\n",
    "print(\"#############################################################\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertDatasetYFromLiteralToNumeric(y_dataset):\n",
    "  y_train=y_dataset[0]\n",
    "  y_test=y_dataset[1]\n",
    "  y_train_unique_class_labels,y_train_positions = np.unique(y_train,return_inverse=True)\n",
    "  y_test_unique_class_labels,y_test_positions = np.unique(y_test,return_inverse=True)\n",
    "  \n",
    "  print(y_train_unique_class_labels)\n",
    "  print(y_test_unique_class_labels)\n",
    "  \n",
    "  y_train=y_train_positions\n",
    "  y_test=y_test_positions\n",
    "  \n",
    "  return ((y_train,y_test))\n",
    "\n",
    "\n",
    "print(\"\\n#############################################################\")\n",
    "print(\"Changing classification labels from Literal to Numeric:\")\n",
    "print(\"\\nBefore conversion:\")\n",
    "print(type(y_train), y_train.size, y_train.shape)\n",
    "print(type(y_test), y_test.size, y_test.shape)\n",
    "print(type(y_train[0]))\n",
    "print(type(y_test[0]))\n",
    "\n",
    "y_train,y_test =convertDatasetYFromLiteralToNumeric((y_train,y_test))\n",
    "\n",
    "print(\"\\nAfter conversion:\")\n",
    "print(type(y_train), y_train.size, y_train.shape)\n",
    "print(type(y_test), y_test.size, y_test.shape)\n",
    "\n",
    "print(type(y_train[0]))\n",
    "print(type(y_test[0]))\n",
    "print(\"#############################################################\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_dataset_x_amax_value(x_dataset):\n",
    "  x_train=x_dataset[0]\n",
    "  x_test=x_dataset[1]\n",
    "  max_x=np.amax([np.amax(x_train), np.amax(x_test)])\n",
    "  return max_x\n",
    "\n",
    "def calculate_dataset_x_max_value(x_dataset):\n",
    "  x_train=x_dataset[0]\n",
    "  x_test=x_dataset[1]\n",
    "  max_x=np.max([np.max(x_train), np.max(x_test)])\n",
    "  return max_x\n",
    "\n",
    "def calculate_dataset_x_norm_value(x_dataset):\n",
    "  x_train=x_dataset[0]\n",
    "  x_test=x_dataset[1]\n",
    "  x_train_norm = np.linalg.norm(x_train)       # To find the norm of the array\n",
    "  print(\"x_train_norm\",x_train_norm)\n",
    "  x_test_norm = np.linalg.norm(x_test)       # To find the norm of the array\n",
    "  print(\"x_test_norm\",x_test_norm)\n",
    "  max_x=np.max([np.max(x_train_norm), np.max(x_test_norm)])\n",
    "  return max_x\n",
    "  \n",
    "  # max_x=np.max([np.max(x_train), np.max(x_test)])\n",
    "  # return max_x\n",
    "\n",
    "def normalize_dataset_x_value_range_between_0_and_1(x_dataset,max_x):\n",
    "  x_train=x_dataset[0]\n",
    "  x_test=x_dataset[1]\n",
    "\n",
    "  # Normalize the data to a 0.0 to 1.0 scale for faster processing\n",
    "  x_train, x_test = x_train / max_x, x_test / max_x\n",
    "  return (x_train, x_test)\n",
    "\n",
    "\n",
    "#Normalizing Phase\n",
    "x_dataset=(x_train,x_test)\n",
    "# norm_max_x=calculate_dataset_x_norm_value(x_dataset)\n",
    "# amax_x=calculate_dataset_x_amax_value(x_dataset)\n",
    "# max_x=calculate_dataset_x_max_value(x_dataset)\n",
    "# x_train,x_test=normalize_dataset_x_value_range_between_0_and_1(x_dataset,norm_max_x)\n",
    "\n",
    "# image_frame_size=32\n",
    "\n",
    "print(\"\\n#############################################################\")\n",
    "\n",
    "print(\"Normalizing Dataset X: maximum sum of transfer momentum in the dataset: \")\n",
    "# print(\"norm_max_x\",norm_max_x)\n",
    "# print(\"amax\",amax_x)\n",
    "# print(\"max_x\",max_x)\n",
    "\n",
    "print(\"#############################################################\\n\")\n",
    "\n",
    "# exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reserve 20% samples for validation dataset\n",
    "def calculate_validation_dataset_size(dataset_train_size,dataset_test_size):\n",
    "  dataset_size= dataset_train_size+dataset_test_size\n",
    "  dataset_validation_size=dataset_size*.2\n",
    "  return int(dataset_validation_size)\n",
    "\n",
    "def set_validation_dataset(x_train,y_train,validation_dataset_size):\n",
    "  \n",
    "  x_val = x_train[-validation_dataset_size:]\n",
    "  y_val = y_train[-validation_dataset_size:]\n",
    "  x_train = x_train[:-validation_dataset_size]\n",
    "  y_train = y_train[:-validation_dataset_size]\n",
    "  \n",
    "  \n",
    "  return (x_train, y_train), (x_val, y_val)\n",
    "\n",
    "validation_dataset_size= calculate_validation_dataset_size(y_train.size,y_test.size)\n",
    "(x_train, y_train), (x_val, y_val)=set_validation_dataset(x_train,y_train,validation_dataset_size)\n",
    "print(\"\\n#############################################################\")\n",
    "print(\"Defining Validation Dataset from Train Dataset:\")\n",
    "\n",
    "print(\"\\nTrain data info:\")\n",
    "print(type(y_train), y_train.size, y_train.shape)\n",
    "print(type(x_train), x_train.size, x_train.shape)\n",
    "\n",
    "print(\"\\nValidation data info:\")\n",
    "print(type(y_val), y_val.size, y_val.shape)\n",
    "print(type(x_val), x_val.size, x_val.shape)\n",
    "\n",
    "print(\"\\nTest data info:\")\n",
    "print(type(y_test), y_test.size, y_test.shape)\n",
    "print(type(x_test), x_test.size, x_test.shape)\n",
    "print(\"#############################################################\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n#############################################################\")\n",
    "print(\"Reshaping dataset X-side dimension to be fit in the defined convolutional :\")\n",
    "\n",
    "print(\"\\nX train:\")\n",
    "print(x_train.shape)\n",
    "print (x_train.shape[0],x_train.shape[1],x_train.shape[2])\n",
    "x_train_reshaped=x_train.reshape(x_train.shape[0],x_train.shape[1],x_train.shape[2],1)\n",
    "print(x_train_reshaped.shape)\n",
    "\n",
    "print(\"\\nX val:\")\n",
    "print(x_val.shape)\n",
    "print (x_val.shape[0],x_val.shape[1],x_val.shape[2])\n",
    "x_val_reshaped=x_val.reshape(x_val.shape[0],x_val.shape[1],x_val.shape[2],1)\n",
    "print(x_train_reshaped.shape)\n",
    "\n",
    "print(\"\\nX test:\")\n",
    "print(x_test.shape)\n",
    "print (x_test.shape[0],x_test.shape[1],x_test.shape[2])\n",
    "x_test_reshaped=x_test.reshape(x_test.shape[0],x_test.shape[1],x_test.shape[2],1)\n",
    "print(x_test_reshaped.shape)\n",
    "print(\"#############################################################\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create a directory to save the best model\n",
    "\n",
    "save_dir = (simulation_directory_path+'Models_{}_vs_{}_{}_ch{}').format(Modules[0], Modules[1], kind, len(observables))\n",
    "if not path.exists(save_dir):\n",
    "    makedirs(save_dir)\n",
    "print('Directory to save models: {}'.format(save_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, LeakyReLU\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D, BatchNormalization, Flatten\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam, Adagrad\n",
    "\n",
    "\n",
    "def get_callbacks(monitor, save_dir):\n",
    "    mode = None\n",
    "    if 'loss' in monitor:\n",
    "        mode = 'min'\n",
    "    elif 'accuracy' in monitor:\n",
    "        mode = 'max'\n",
    "    assert mode != None, 'Check the monitor parameter!'\n",
    "\n",
    "    es = EarlyStopping(monitor=monitor, mode=mode, patience=10,\n",
    "                      min_delta=0., verbose=1)\n",
    "    rlp = ReduceLROnPlateau(monitor=monitor, mode=mode, factor=0.2, patience=5,\n",
    "                            min_lr=0.001, verbose=1)\n",
    "    mcp = ModelCheckpoint(path.join(save_dir, 'hm_jetscape_ml_model_best.h5'), monitor=monitor, \n",
    "                          save_best_only=True, mode=mode, verbose=1)\n",
    "    \n",
    "    return [es, rlp, mcp]\n",
    "\n",
    "def conv2d_layer_block(prev_layer, filters, dropout_rate, input_shape=None):\n",
    "    if input_shape != None:\n",
    "        prev_layer.add(Conv2D(filters=filters, kernel_size=5,\n",
    "                              kernel_initializer='he_uniform',\n",
    "                              padding='same',\n",
    "                              activation='relu',\n",
    "                              kernel_regularizer=l2(l=0.02),\n",
    "                              input_shape=input_shape\n",
    "                             )\n",
    "                      )\n",
    "    else:\n",
    "        prev_layer.add(Conv2D(filters=filters, kernel_size=5,\n",
    "                              kernel_initializer='he_uniform',\n",
    "                              padding='same',\n",
    "                              activation='relu',\n",
    "                              kernel_regularizer=l2(l=0.02)\n",
    "                             )\n",
    "                      )\n",
    "    prev_layer.add(Conv2D(filters=filters, kernel_size=5,\n",
    "                              kernel_initializer='he_uniform',\n",
    "                              padding='same',\n",
    "                              activation='relu',\n",
    "                              kernel_regularizer=l2(l=0.02)\n",
    "                             )\n",
    "                      )    \n",
    "    prev_layer.add(MaxPool2D(pool_size=(2, 2)))\n",
    "    prev_layer.add(Dropout(dropout_rate))\n",
    "    \n",
    "    return prev_layer\n",
    "\n",
    "def fc_layer_block(prev_layer, units, dropout_rate, last_layer=False):\n",
    "    if last_layer == False:\n",
    "        prev_layer.add(Dense(units, activation='relu',\n",
    "                             kernel_initializer='he_uniform',\n",
    "                             kernel_regularizer=l2(l=0.02)\n",
    "                            )\n",
    "                      )\n",
    "        prev_layer.add(Dropout(dropout_rate))\n",
    "    else:\n",
    "        prev_layer.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    return prev_layer\n",
    "\n",
    "def CNN_model(input_shape, lr, dropout1, dropout2):\n",
    "    model = Sequential()\n",
    "    # model.add(Rescaling(1./max_x))\n",
    "    model = conv2d_layer_block(model, 256, dropout1, input_shape)\n",
    "    model = conv2d_layer_block(model, 256, dropout1)\n",
    "    model = conv2d_layer_block(model, 256, dropout1)\n",
    "    model = conv2d_layer_block(model, 256, dropout1)\n",
    "    #model = conv2d_layer_block(model, 128, dropout1)\n",
    "    model.add(Flatten())\n",
    "    model = fc_layer_block(model, 1024, dropout2)\n",
    "    model = fc_layer_block(model, 1024, dropout2)\n",
    "    model = fc_layer_block(model, 1024, dropout2)\n",
    "    model = fc_layer_block(model, 1024, dropout2)\n",
    "    model = fc_layer_block(model, 1, None, last_layer=True)\n",
    "    \n",
    "    optimizer = Adam(learning_rate=lr)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer,\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## parameers for training\n",
    "n_epochs = 50\n",
    "batch_size = 256\n",
    "input_shape = x_train_reshaped.shape[1:]\n",
    "monitor='val_accuracy' #'val_accuracy' or 'val_loss'\n",
    "lr = 5e-6\n",
    "dropout1, dropout2 = 0.2, 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(train_set, val_set, n_epochs, lr, batch_size, monitor):\n",
    "    tf.keras.backend.clear_session()\n",
    "    X_train = train_set[0]\n",
    "    Y_train = train_set[1]\n",
    "    model = CNN_model(input_shape, lr, dropout1, dropout2)\n",
    "    callbacks = get_callbacks(monitor, save_dir)\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    start = time()\n",
    "    history = model.fit(X_train, Y_train, epochs=n_epochs, verbose=1, batch_size=batch_size, \n",
    "                        validation_data=val_set, shuffle=True, callbacks=callbacks)\n",
    "\n",
    "    train_time = (time()-start)/60.\n",
    "    return history, train_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training and validation sets\n",
    "train_set, val_set = (x_train_reshaped, y_train), (x_val_reshaped, y_val)\n",
    "\n",
    "# train the network\n",
    "history, train_time = train_network(train_set, val_set, n_epochs, lr, batch_size, monitor)\n",
    "file_name='hm_jetscape_ml_model_history.csv'\n",
    "file_path=simulation_directory_path+file_name\n",
    "pd.DataFrame.from_dict(history.history).to_csv(file_path,index=False)\n",
    "\n",
    "\n",
    "file_name='hm_jetscape_ml_model_history.npy'\n",
    "file_path=simulation_directory_path+file_name\n",
    "np.save(file_path,history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This section shall be just used after training or for stand alone evaluations\n",
    "# Building a dictionary which is accessable by dot\n",
    "class dotdict(dict):\n",
    "    \"\"\"dot.notation access to dictionary attributes\"\"\"\n",
    "    __getattr__ = dict.get\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__\n",
    "\n",
    "#Loading learning history after training \n",
    "file_name='hm_jetscape_ml_model_history.npy'\n",
    "file_path=simulation_directory_path+file_name\n",
    "\n",
    "\n",
    "history=dict({'history':np.load(file_path,allow_pickle='TRUE').item()})\n",
    "history=dotdict(history)\n",
    "print(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "def plot_train_history(history):\n",
    "\n",
    "    color_list = ['red','blue','black','green']\n",
    "\n",
    "    plt.figure(figsize=(8, 2.5), dpi=200)\n",
    "\n",
    "    plt.subplot(121)\n",
    "    plt.plot(history.history['loss'], label='Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.yscale('log')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.title('Loss History')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(122)\n",
    "    plt.plot(history.history['accuracy'], label='Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.title('Accuracy History')\n",
    "    plt.legend()\n",
    "    file_name='hm_jetscape_ml_plot_train_history.png'\n",
    "    file_path=simulation_directory_path+file_name\n",
    "    plt.savefig(file_path)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "# plot the training history for each fold\n",
    "plot_train_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "## load the best model\n",
    "best_model = load_model(path.join(save_dir,'hm_jetscape_ml_model_best.h5'))\n",
    "\n",
    "outputStr='Train   | Validation | Test sets\\n'\n",
    "\n",
    "## evaluate the model on train/val/test sets and append the results to lists\n",
    "_, train_acc = best_model.evaluate(x_train_reshaped, y_train, verbose=0)\n",
    "_, val_acc = best_model.evaluate(x_val_reshaped, y_val, verbose=0)\n",
    "_, test_acc = best_model.evaluate(x_test_reshaped, y_test, verbose=0)\n",
    "    \n",
    "## print out the accuracy\n",
    "outputStr+='{:.4f}%  {:.4f}%     {:.4f}%\\n'.format(train_acc * 100, val_acc * 100, test_acc * 100)\n",
    "print(outputStr)\n",
    "\n",
    "file_name=\"hm_jetscape_ml_model_evaluation.txt\"\n",
    "file_path=simulation_directory_path+file_name\n",
    "evaluation_file = open(file_path, \"w\")\n",
    "evaluation_file.write(outputStr)\n",
    "evaluation_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "## plot confution matrix\n",
    "y_pred = best_model.predict_classes(x_test_reshaped)\n",
    "\n",
    "conf_mat = confusion_matrix(y_pred, y_test)\n",
    "sns.heatmap(conf_mat, annot=True, cmap='Blues', \n",
    "            xticklabels=Modules, yticklabels=Modules, fmt='g')\n",
    "plt.xlabel('True Label', fontsize=15)\n",
    "plt.ylabel('Prediction', fontsize=15)\n",
    "file_name='hm_jetscape_ml_model_confision_matrix.png'\n",
    "file_path=simulation_directory_path+file_name\n",
    "plt.savefig(file_path)\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "classification_report_str= classification_report(y_test,y_pred)\n",
    "\n",
    "print (classification_report_str)\n",
    "file_name=\"hm_jetscape_ml_model_evaluation.txt\"\n",
    "file_path=simulation_directory_path+file_name\n",
    "evaluation_file = open(file_path, \"a\")\n",
    "evaluation_file.write(classification_report_str)\n",
    "evaluation_file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ada828d16365d2b22d3899327f52f8feba3feb56b4fde7279c1cd0b9201605e0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
