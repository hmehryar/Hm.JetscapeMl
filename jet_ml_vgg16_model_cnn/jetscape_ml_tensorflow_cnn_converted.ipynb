{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4fb1dfc",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Using TensorFlow with Jetscape Benchmarck Dataset\n",
    "**About the JETSCAPE Eloss Classification dataset**.   \n",
    "Jetscape Eloss Classification is the equivalent *Hello World* of jet image analysis.\n",
    "It consists of 4 categories, MATTER-VACCUUM/MATTER-MEDIUM/MATTER+LBT/MATTER+MARTINI, in 32x32 pixel squares.  \n",
    "Each gray-scale pixel contains an integer 0-255 to indicate darkness, with 0 white and 255 black.  \n",
    "There are about 180,000 training records, and about 20,000 test records.  \n",
    "In other words, the images of numbers have already been transformed into arrays of ints to make them easier to use for ML projects. You can find more info on the jetscape [here](https://jetscape.org/). You can also download it from [here](#).\n",
    "\n",
    "\n",
    "## Part 0: Prerequisites:\n",
    "\n",
    "We recommend that you run this this notebook in the cloud on Google Colab (see link with icon at the top) if you're not already doing so. It's the simplest way to get started. You can also [install TensorFlow locally](https://www.tensorflow.org/install/).\n",
    "\n",
    "Note that there's [tf.keras](https://www.tensorflow.org/guide/keras) (comes with TensorFlow) and there's [Keras](https://keras.io/) (standalone). You should be using [tf.keras](https://www.tensorflow.org/guide/keras) because (1) it comes with TensorFlow so you don't need to install anything extra and (2) it comes with powerful TensorFlow-specific features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f7300b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('Loading/Installing Package => Begin\\n\\n')\n",
    "# Commonly used modules\n",
    "import numpy as np\n",
    "import os\n",
    "from os import path, makedirs\n",
    "import time\n",
    "from time import time\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "\n",
    "def install(package):\n",
    "  print(\"Installing \"+package) \n",
    "  subprocess.check_call([sys.executable,\"-m\" ,\"pip\", \"install\", package])\n",
    "  print(\"Installed \"+package+\"\\n\") \n",
    "# TensorFlow and tf.keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# !pip3 install sklearn\n",
    "install(\"sklearn\")\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# !pip3 install seaborn\n",
    "install(\"seaborn\")\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Images, plots, display, and visualization\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "#reading/writing into files\n",
    "# !pip3 install pickle5\n",
    "install(\"pickle5\")\n",
    "import pickle5 as pickle\n",
    "\n",
    "\n",
    "#import cv2\n",
    "install(\"IPython\")\n",
    "import IPython\n",
    "from six.moves import urllib\n",
    "\n",
    "\n",
    "print('\\n########################################################################')\n",
    "print('Checking the running platforms\\n')\n",
    "import platform\n",
    "running_os=platform.system()\n",
    "print(\"OS: \"+running_os)\n",
    "print(\"OS version: \"+platform.release())\n",
    "\n",
    "try:\n",
    "  from google.colab import drive\n",
    "  COLAB = True\n",
    "except:\n",
    "  COLAB = False\n",
    "print(\"running on Colab: \"+str(COLAB))\n",
    "\n",
    "# if 'google.colab' in str(get_ipython()):\n",
    "#   print('Running on CoLab')\n",
    "#   install(\"google.colab\")\n",
    "#   from google.colab import drive\n",
    "#   drive.mount('/content/drive')\n",
    "# else:\n",
    "#   print('Not running on CoLab')\n",
    "\n",
    "\n",
    "print(\"Python version: \"+platform.python_version())\n",
    "print(\"Tensorflow version: \"+tf.__version__)\n",
    "\n",
    "dataset_directory_path=''\n",
    "simulation_directory_path=''\n",
    "\n",
    "if COLAB == True:\n",
    "  drive.mount('/content/drive')\n",
    "  dataset_directory_path='/content/drive/MyDrive/Projects/110_JetscapeMl/hm.jetscapeml.data/simulation_results/'\n",
    "  simulation_directory_path=dataset_directory_path+'simulation-results-cnn-01-200K-config-05-01/'\n",
    "elif 'Linux' in running_os:\n",
    "  dataset_directory_path='/wsu/home/gy/gy40/gy4065/hm.jetscapeml.data//simulation_results/'\n",
    "  simulation_directory_path=dataset_directory_path+'simulation-results-cnn-01-200K-config-05-01/'\n",
    "else:\n",
    "  dataset_directory_path= 'G:\\\\My Drive\\\\Projects\\\\110_JetscapeMl\\\\hm.jetscapeml.data\\\\simulation_results\\\\'\n",
    "  simulation_directory_path=dataset_directory_path+'simulation-results-cnn-01-200K-config-05-01\\\\'\n",
    "print('Dataset Directory Path: '+dataset_directory_path)\n",
    "\n",
    "#dataset_file_name='jetscape-ml-benchmark-dataset-2k-randomized.pkl'\n",
    "# dataset_file_name='jetscape-ml-benchmark-dataset-matter-vs-lbt-2000.pkl'\n",
    "# dataset_file_name='jetscape-ml-benchmark-dataset-matter-vs-lbt-200k-shuffled-01.pkl'\n",
    "dataset_file_name='jetscape-ml-benchmark-dataset-matter-vs-lbt-200k-shuffled-02.pkl'\n",
    "# dataset_file_name='jetscape-ml-benchmark-dataset-matter-vs-lbt-200k-shuffled-03.pkl'\n",
    "print(\"Dataset file name: \"+dataset_file_name)\n",
    "\n",
    "if not path.exists(simulation_directory_path):\n",
    "    makedirs(simulation_directory_path)\n",
    "print('Simulation Results Path: '+simulation_directory_path)\n",
    "print('########################################################################\\n')\n",
    "print('\\nLoading/Installing Package => End\\n\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8119c873",
   "metadata": {},
   "source": [
    "## 1. Load Data into a Numpy Array  \n",
    "I downloaded the data file onto my desktop and loaded it locally.  \n",
    "You can also load it directly from the cloud as follows:  \n",
    "```mnist = tf.keras.datasets.mnist  \n",
    "(x_train, y_train), (x_test, y_test) = jetscapeMl.load_data()  \n",
    "```  \n",
    "**After the load:**   \n",
    "x_train contains 180k arrays of 32x32.  \n",
    "The y_train vector contains the corresponding labels for these.  \n",
    "x_test contains 20k arrays of 32x32.  \n",
    "The y_test vector contains the corresponding labels for these.\n",
    "\n",
    "**Building Randomized Dataset**\n",
    "Before having the simulation data, researcher tried to implmenet a psudo random data to create the architecture for the project. I thought it could be useful for further usages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f8a010",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_y_builder(y_size,y_class_label_items):\n",
    "    class_size=int(y_size/len(y_class_label_items))\n",
    "    y=[]\n",
    "    for class_label_item in y_class_label_items:\n",
    "        y = np.append (y, [class_label_item]*class_size)\n",
    "    return y\n",
    "\n",
    "def dataset_x_builder_randomized(x_size,dataset_frame_size):\n",
    "    x=np.arange(x_size*dataset_frame_size*dataset_frame_size)    .reshape((x_size,dataset_frame_size,dataset_frame_size))\n",
    "    return x\n",
    "\n",
    "def build_randomized_dataset():\n",
    "  dataset_frame_size=32\n",
    "  #train_size=600\n",
    "  #test_size=100\n",
    "  train_size=1600\n",
    "  test_size=400\n",
    "\n",
    "  y_class_label_items=['MVAC','MMED','MLBT','MMAR']\n",
    "  y_train=dataset_y_builder(train_size,y_class_label_items)\n",
    "  y_test=dataset_y_builder(test_size,y_class_label_items)\n",
    "\n",
    "\n",
    "  x_train=dataset_x_builder_randomized(train_size,dataset_frame_size)\n",
    "  x_test=dataset_x_builder_randomized(test_size,dataset_frame_size)\n",
    "\n",
    "  print(type(x_train), x_train.size, x_train.shape)\n",
    "  print(type(y_train), y_train.size, y_train.shape)\n",
    "  print(type(x_test), x_test.size, x_test.shape)\n",
    "  print(type(y_test), y_test.size, y_test.shape)\n",
    "  dataset=((x_train, y_train), (x_test, y_test))\n",
    "  return dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5d2035",
   "metadata": {},
   "source": [
    "##Saving and Loading Dataset Methods Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4ebba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataset(file_name,dataset):\n",
    "    with open(file_name, 'wb') as dataset_file:\n",
    "        pickle.dump(dataset,dataset_file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "        \n",
    "def load_dataset(file_name):\n",
    "    with open(file_name, 'rb') as dataset_file:\n",
    "        (x_train, y_train), (x_test, y_test) = pickle.load(dataset_file, encoding='latin1')\n",
    "        dataset=((x_train, y_train), (x_test, y_test))\n",
    "        return dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15b42ea",
   "metadata": {},
   "source": [
    "\n",
    "##Suffling the dataset and Saving the shuffling result\n",
    "Shuffle function doesn't work in this case, because we need to change both x, and y array together. Therefore, take function is used to make a random permutation. For shuffling the x data, the axis shall be mentioned, so it will shuffle over the first dimension.\n",
    "\n",
    "\n",
    "---\n",
    "1. Shuffling Train Dataset\n",
    "2. Shuffling Test Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8298d774",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_training_dataset(x_train, y_train):\n",
    "  \n",
    "  print(\"Train Dataset Permutation Array:\")\n",
    "  train_permutation_array_indices=np.random.permutation(y_train.size)\n",
    "  #print(train_permutation_array_indices[1:100])\n",
    "\n",
    "  print(\"y_train:\")\n",
    "  print(y_train, type(y_train),y_train.size, y_train.shape)\n",
    "  #print(y_train[1:100])\n",
    "\n",
    "  print(\"y_train_shuffled:\")\n",
    "  y_train_shuffled=np.take(y_train, train_permutation_array_indices)\n",
    "  print(y_train_shuffled, type(y_train_shuffled),y_train_shuffled.size, y_train_shuffled.shape)\n",
    "  #print(y_train_shuffled[1:100])\n",
    "\n",
    "  print(\"x_train:\")\n",
    "  print(x_train, type(x_train),x_train.size, x_train.shape)\n",
    "  #print(x_train[1:100])\n",
    "\n",
    "  print(\"x_train_shuffled:\")\n",
    "  x_train_shuffled=np.take(x_train, train_permutation_array_indices,axis=0)\n",
    "  print(x_train_shuffled, type(x_train_shuffled),x_train_shuffled.size, x_train_shuffled.shape)\n",
    "  #print(x_train_shuffled[1:100])\n",
    "\n",
    "  dataset_train_shuffled=(x_train_shuffled, y_train_shuffled)\n",
    "  return dataset_train_shuffled\n",
    "\n",
    "\n",
    "#main method\n",
    "\n",
    "def shuffle_training_dataset_runner():\n",
    "\n",
    "  start_time = time.time()\n",
    "\n",
    "  #file_name='jetscape-ml-benchmark-dataset-matter-vs-lbt-200k.pkl'\n",
    "  file_name='jetscape-ml-benchmark-dataset-matter-vs-lbt-2000.pkl'\n",
    "  # file_name='jetscape-ml-benchmark-dataset-2k-randomized.pkl'\n",
    "  dataset_path=dataset_directory_path+file_name\n",
    "\n",
    "  (x_train, y_train), (x_test, y_test) =load_dataset(dataset_path)\n",
    "\n",
    "  (x_train_shuffled, y_train_shuffled)=shuffle_training_dataset(x_train, y_train)\n",
    "\n",
    "  #file_name='jetscape-ml-benchmark-dataset-matter-vs-lbt-200k-shuffled-03.pkl'\n",
    "  file_name='jetscape-ml-benchmark-dataset-matter-vs-lbt-2000-shuffled.pkl'\n",
    "  # file_name='jetscape-ml-benchmark-dataset-2k-randomized-shuffled.pkl'\n",
    "  shuffled_dataset_path=dataset_directory_path+file_name\n",
    "  dataset=((x_train_shuffled,y_train_shuffled),(x_test,y_test))\n",
    "  save_dataset(shuffled_dataset_path,dataset)\n",
    "  end_time=time.time()\n",
    "\n",
    "  elapsed_time=end_time-start_time\n",
    "  print('Elapsed Time: ')\n",
    "  print(elapsed_time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2e4d57",
   "metadata": {},
   "source": [
    "**Saving Dataset Benchmark as a file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8dd1a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataset_runner():\n",
    "  file_name='jetscape-ml-benchmark-dataset-2k-randomized.pkl'\n",
    "  dataset=((x_train,y_train),(x_test,y_test))\n",
    "  save_dataset(dataset_directory_path+file_name,dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17d7d9f",
   "metadata": {},
   "source": [
    "**Creatinng a random event and demostrate it in a 2-D histogram**\n",
    "This module implemented for developemental purpose, just as an example of how the events can be shown in 2-D images with their hit frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b25b0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_and_plot_random_event():\n",
    "  pi=3.14\n",
    "  current_event_hits=np.random.uniform(-pi, pi, size=(2, 10000))\n",
    "  counts, xedges, yedges = np.histogram2d(current_event_hits[0], current_event_hits[1], bins=32)\n",
    "  print(counts)\n",
    "  #plt.imshow(counts.reshape(32, 32), cmap=cm.Greys)\n",
    "  plt.imshow(counts, interpolation='nearest', origin='lower',\n",
    "        extent=[xedges[0], xedges[-1], yedges[0], yedges[-1]])\n",
    "  cb = plt.colorbar()\n",
    "  cb.set_label(\"Hit Frequency\")\n",
    "  \n",
    "  file_name='sample_random_event_histogram_32x32.png'\n",
    "  file_path=simulation_directory_path+file_name\n",
    "  plt.savefig(file_path)\n",
    "\n",
    "  return counts\n",
    "\n",
    "# # # Funtionality Testing\n",
    "# counts=create_and_plot_random_event()\n",
    "# sample_random_event_file_name='sample_random_event_histogram_32x32.pkl'\n",
    "# sample_random_event_file_path=simulation_directory_path+sample_random_event_file_name\n",
    "# save_dataset(sample_random_event_file_path,counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc38c04",
   "metadata": {},
   "source": [
    "## 2. Use Matplotlib to visualize one record.  \n",
    "I set the colormap to Grey and ColorMap. There are a bunch of other colormap choices if you like bright visualizations. Try magma or any of the other  choice in the [docs](https://matplotlib.org/tutorials/colors/colormaps.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b6b997",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_event(image_frame_size,event_matrix,file_name):\n",
    "  plt.imshow(event_matrix.reshape(image_frame_size, image_frame_size), cmap=cm.Greys)\n",
    "  cb = plt.colorbar()\n",
    "  cb.set_label(\"Hit Frequency\")\n",
    "  \n",
    "  file_path=simulation_directory_path+file_name\n",
    "  plt.savefig(file_path)\n",
    "# # Funtionality Testing\n",
    "# # Plotting sample Random Event Histogram on gray scale\n",
    "# image_frame_size=32\n",
    "# plot_event(image_frame_size,counts,file_name='sample_random_event_histogram_32x32_grayscale.png')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d1b0d2",
   "metadata": {},
   "source": [
    "#Loading Dataset\n",
    "**First learning step**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3bf557d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JetscapeMlCnn:\n",
    "   # class attribute\n",
    "  \n",
    "    # Instance attribute\n",
    "    def __init__(self, x_train,y_train,x_test,y_test):\n",
    "        self.x_train=x_train\n",
    "        self.y_train=y_train\n",
    "        self.x_test=x_test\n",
    "        self.y_test=y_test\n",
    "\n",
    "\n",
    "#Loading Dataset Phase\n",
    "\n",
    "\n",
    "dataset_file_path=dataset_directory_path+dataset_file_name\n",
    "print(\"Dataset file path: \"+dataset_file_path)\n",
    "(x_train, y_train), (x_test, y_test) =load_dataset(dataset_file_path)\n",
    "\n",
    "oJetscapeMlCnn=JetscapeMlCnn(x_train, y_train, x_test, y_test)\n",
    "print(\"\\n#############################################################\")\n",
    "print(\"Post-Load: DataType Checkpoint: Begin\")\n",
    "print(type(oJetscapeMlCnn.x_train), oJetscapeMlCnn.x_train.size, oJetscapeMlCnn.x_train.shape)\n",
    "print(type(oJetscapeMlCnn.y_train), oJetscapeMlCnn.y_train.size, oJetscapeMlCnn.y_train.shape)\n",
    "print(type(oJetscapeMlCnn.x_test), oJetscapeMlCnn.x_test.size, oJetscapeMlCnn.x_test.shape)\n",
    "print(type(oJetscapeMlCnn.y_test), oJetscapeMlCnn.y_test.size, oJetscapeMlCnn.y_test.shape)\n",
    "print(oJetscapeMlCnn.y_train[1500], oJetscapeMlCnn.y_test[99])\n",
    "print(oJetscapeMlCnn.y_train[1:500])\n",
    "print(\"Post-Load: DataType Checkpoint: End\")\n",
    "print(\"#############################################################\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a51dcc",
   "metadata": {},
   "source": [
    "## 3. Plot a bunch of records to see sample data  \n",
    "Basically, use the same Matplotlib commands above in a for loop to show 20 records from the train set in a subplot figure. We also make the figsize a bit bigger and remove the tick marks for readability.\n",
    "** TODO: try to make the subplot like the below from the first project meeting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a676f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_20_sample_events(events_matrix_items):\n",
    "  # images = x_train[0:18]\n",
    "  # fig, axes = plt.subplots(3, 6, figsize=[9,5])\n",
    "  images = events_matrix_items\n",
    "  fig, axes = plt.subplots(2, 10, figsize=[15,5])\n",
    "\n",
    "  for i, ax in enumerate(axes.flat):\n",
    "      current_plot= ax.imshow(x_train[i].reshape(32, 32), cmap=cm.Greys)\n",
    "      ax.set_xticks([])\n",
    "      ax.set_yticks([])     \n",
    "  \n",
    "\n",
    "  file_name='hm_jetscape_ml_plot_20_sample_events.png'\n",
    "  file_path=simulation_directory_path+file_name\n",
    "  plt.savefig(file_path)\n",
    "\n",
    "  plt.show()\n",
    "  plt.close()\n",
    "#Plotting 20 Sample Events Phase\n",
    "events_matrix_items=[x_train[0:10],x_train[1500:10]]\n",
    "plot_20_sample_events(events_matrix_items)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5222e53e",
   "metadata": {},
   "source": [
    "## 4. Show distribution of training data labels   \n",
    "The training data is about evenly distributed across all nine digits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f1b244",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_y_train_dataset_distribution(y_train):\n",
    "  unique_class_labels,positions = np.unique(y_train,return_inverse=True)\n",
    "\n",
    "\n",
    "  counts = np.bincount(positions)\n",
    "  plt.bar(unique_class_labels, counts)\n",
    "  plt.title(\"Dataset classification vector's distribution\")\n",
    "  \n",
    "  \n",
    "  file_name='hm_jetscape_ml_plot_y_train_dataset_distribution.png'\n",
    "  file_path=simulation_directory_path+file_name\n",
    "  plt.savefig(file_path)\n",
    "\n",
    "  plt.show()\n",
    "  plt.close()\n",
    "  print(\"\\n#############################################################\")\n",
    "  print(\"Classification vector statistics:\")\n",
    "  print(unique_class_labels)\n",
    "  unique_class_labels,positions = np.unique(y_train,return_inverse=True)\n",
    "  print(counts)\n",
    "  print(unique_class_labels)\n",
    "  print(\"Sample 20 head labels:\")\n",
    "  print(positions[:20])\n",
    "  print(\"#############################################################\\n\")\n",
    "\n",
    "#Checking Train Dataset Y Distribution\n",
    "plot_y_train_dataset_distribution(y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803412e6",
   "metadata": {},
   "source": [
    "# Changing classification labels from Literal to Numeric\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb88e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertDatasetYFromLiteralToNumeric(y_dataset):\n",
    "  y_train=y_dataset[0]\n",
    "  y_test=y_dataset[1]\n",
    "  y_train_unique_class_labels,y_train_positions = np.unique(y_train,return_inverse=True)\n",
    "  y_test_unique_class_labels,y_test_positions = np.unique(y_test,return_inverse=True)\n",
    "  \n",
    "  print(y_train_unique_class_labels)\n",
    "  print(y_test_unique_class_labels)\n",
    "  \n",
    "  y_train=y_train_positions\n",
    "  y_test=y_test_positions\n",
    "  \n",
    "  return ((y_train,y_test))\n",
    "\n",
    "\n",
    "print(\"\\n#############################################################\")\n",
    "print(\"Changing classification labels from Literal to Numeric:\")\n",
    "print(\"\\nBefore conversion:\")\n",
    "print(type(y_train), y_train.size, y_train.shape)\n",
    "print(type(y_test), y_test.size, y_test.shape)\n",
    "print(type(y_train[0]))\n",
    "print(type(y_test[0]))\n",
    "\n",
    "y_train,y_test =convertDatasetYFromLiteralToNumeric((y_train,y_test))\n",
    "\n",
    "print(\"\\nAfter conversion:\")\n",
    "print(type(y_train), y_train.size, y_train.shape)\n",
    "print(type(y_test), y_test.size, y_test.shape)\n",
    "\n",
    "print(type(y_train[0]))\n",
    "print(type(y_test[0]))\n",
    "print(\"#############################################################\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ffa6a6",
   "metadata": {},
   "source": [
    "## Normalizing the Dataset X\n",
    "For training the model, the dataset needs to be normalized, meaning all of the dataset values should me between zero and one. This can be done by finding the maximum values over the dataset X side values and devide all the element by it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12b5803",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_dataset_x_max_value(x_dataset):\n",
    "  x_train=x_dataset[0]\n",
    "  x_test=x_dataset[1]\n",
    "  max_x=np.amax([np.amax(x_train), np.amax(x_test)])\n",
    "  return max_x\n",
    "\n",
    "def normalize_dataset_x_value_range_between_0_and_1(x_dataset,max_x):\n",
    "  x_train=x_dataset[0]\n",
    "  x_test=x_dataset[1]\n",
    "\n",
    "  # Normalize the data to a 0.0 to 1.0 scale for faster processing\n",
    "  x_train, x_test = x_train / max_x, x_test / max_x\n",
    "  return (x_train, x_test)\n",
    "\n",
    "\n",
    "#Normalizing Phase\n",
    "x_dataset=(x_train,x_test)\n",
    "max_x=calculate_dataset_x_max_value(x_dataset)\n",
    "x_train,x_test=normalize_dataset_x_value_range_between_0_and_1(x_dataset,max_x)\n",
    "\n",
    "image_frame_size=32\n",
    "\n",
    "print(\"\\n#############################################################\")\n",
    "\n",
    "print(\"Normalizing Dataset X: maximum hit frequency in the dataset: \")\n",
    "print(max_x)\n",
    "\n",
    "print(\"#############################################################\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15abf8a4",
   "metadata": {},
   "source": [
    "##Defining Validation Dataset from Train Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f804ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reserve 20% samples for validation dataset\n",
    "def calculate_validation_dataset_size(dataset_train_size,dataset_test_size):\n",
    "  dataset_size= dataset_train_size+dataset_test_size\n",
    "  dataset_validation_size=dataset_size*.2\n",
    "  return int(dataset_validation_size)\n",
    "\n",
    "def set_validation_dataset(x_train,y_train,validation_dataset_size):\n",
    "  \n",
    "  x_val = x_train[-validation_dataset_size:]\n",
    "  y_val = y_train[-validation_dataset_size:]\n",
    "  x_train = x_train[:-validation_dataset_size]\n",
    "  y_train = y_train[:-validation_dataset_size]\n",
    "  \n",
    "  \n",
    "  return (x_train, y_train), (x_val, y_val)\n",
    "\n",
    "validation_dataset_size= calculate_validation_dataset_size(y_train.size,y_test.size)\n",
    "(x_train, y_train), (x_val, y_val)=set_validation_dataset(x_train,y_train,validation_dataset_size)\n",
    "print(\"\\n#############################################################\")\n",
    "print(\"Defining Validation Dataset from Train Dataset:\")\n",
    "\n",
    "print(\"\\nTrain data info:\")\n",
    "print(type(y_train), y_train.size, y_train.shape)\n",
    "print(type(x_train), x_train.size, x_train.shape)\n",
    "\n",
    "print(\"\\nValidation data info:\")\n",
    "print(type(y_val), y_val.size, y_val.shape)\n",
    "print(type(x_val), x_val.size, x_val.shape)\n",
    "\n",
    "print(\"\\nTest data info:\")\n",
    "print(type(y_test), y_test.size, y_test.shape)\n",
    "print(type(x_test), x_test.size, x_test.shape)\n",
    "print(\"#############################################################\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77b0449",
   "metadata": {},
   "source": [
    "## 5.1 Apply Keras/TensorFlow with simple CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef24ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import tensorflow as tf\n",
    "\n",
    "\n",
    "# from tensorflow import keras\n",
    "# from keras_preprocessing import image\n",
    "# from keras_preprocessing.image import ImageDataGenerator\n",
    "def build_and_train_model():\n",
    "  model = tf.keras.models.Sequential([\n",
    "      tf.keras.layers.Conv2D(32,(3,3), activation='relu', input_shape=(image_frame_size,image_frame_size,1)),\n",
    "      tf.keras.layers.MaxPool2D((2,2)),\n",
    "      tf.keras.layers.Conv2D(64,(3,3),activation='relu'),\n",
    "      tf.keras.layers.MaxPool2D(2,2),\n",
    "      tf.keras.layers.Conv2D(128,(3,3), activation='relu'),\n",
    "      tf.keras.layers.MaxPool2D(2,2),\n",
    "      # tf.keras.layers.Conv2D(128,(3,3), activation='relu'),\n",
    "      # tf.keras.layers.MaxPool2D(2,2),\n",
    "      tf.keras.layers.Flatten(),\n",
    "      tf.keras.layers.Dropout(0.5),\n",
    "      tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "      tf.keras.layers.Dense(1, activation=tf.nn.sigmoid)\n",
    "      ])\n",
    "\n",
    "  model.summary()\n",
    "\n",
    "  model.compile(optimizer=tf.optimizers.Adam(), \n",
    "                loss='binary_crossentropy',\n",
    "                metrics=['acc'])\n",
    "\n",
    "\n",
    "\n",
    "  print(\"Fit model on training data\")\n",
    "  # fit(object, x = NULL, y = NULL, batch_size = NULL, epochs = 10,\n",
    "  #   verbose = getOption(\"keras.fit_verbose\", default = 1),\n",
    "  #   callbacks = NULL, view_metrics = getOption(\"keras.view_metrics\",\n",
    "  #   default = \"auto\"), validation_split = 0, validation_data = NULL,\n",
    "  #   shuffle = TRUE, class_weight = NULL, sample_weight = NULL,\n",
    "  #   initial_epoch = 0, steps_per_epoch = NULL, validation_steps = NULL,\n",
    "  #   ...)\n",
    "  # -> object : the model to train.      \n",
    "  # -> X : our training data. Can be Vector, array or matrix      \n",
    "  # -> Y : our training labels. Can be Vector, array or matrix       \n",
    "  # -> Batch_size : it can take any integer value or NULL and by default, it will\n",
    "  # be set to 32. It specifies no. of samples per gradient.      \n",
    "  # -> Epochs : an integer and number of epochs we want to train our model for.      \n",
    "  # -> Verbose : specifies verbosity mode(0 = silent, 1= progress bar, 2 = one\n",
    "  # line per epoch).      \n",
    "  # -> Shuffle : whether we want to shuffle our training data before each epoch.      \n",
    "  # -> steps_per_epoch : it specifies the total number of steps taken before\n",
    "  # one epoch has finished and started the next epoch. By default it values is set to NULL.\n",
    "\n",
    "  start_time =time.perf_counter()\n",
    "\n",
    "  no_epoch=10\n",
    "  history = model.fit(\n",
    "      x_train,\n",
    "      y_train,\n",
    "      batch_size=64,\n",
    "      epochs=no_epoch,\n",
    "      # We pass some validation for\n",
    "      # monitoring validation loss and metrics\n",
    "      # at the end of each epoch\n",
    "      validation_data=(x_val, y_val))\n",
    "\n",
    "  model.save('matter_vs_matterlbt_2k.h5')\n",
    "  acc_train = history.history['acc']\n",
    "  acc_val = history.history['val_acc']\n",
    "  print(acc_train)\n",
    "  print(acc_train)\n",
    "  epochs = range(1,no_epoch+1)\n",
    "  plt.plot(epochs,acc_train, 'g', label='training accuracy')\n",
    "  plt.plot(epochs, acc_val, 'b', label= 'validation accuracy')\n",
    "  plt.title('Training and Validation accuracy')\n",
    "  plt.xlabel('Epochs')\n",
    "  plt.ylabel('Accuracy')\n",
    "  plt.legend()\n",
    "  plt.show()\n",
    "  plt.close()\n",
    "  elapsed_time=time.perf_counter() - start_time\n",
    "  print('Elapsed %.3f seconds.' % elapsed_time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a8f604",
   "metadata": {},
   "source": [
    "\n",
    "## 5.2 Apply Keras/TensorFlow with complicated CNN\n",
    "Previous research replication\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b59fc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## event info\n",
    "collision = 'PbPb'\n",
    "energy = 5020\n",
    "centrality = '0_10'\n",
    "Modules = ['Matter','LBT']\n",
    "JetptMinMax = '100_110'\n",
    "#observables = ['pt','charge','mass']\n",
    "observables = ['pt']\n",
    "kind = 'Hadron'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45ef630",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create a directory to save the best model\n",
    "\n",
    "save_dir = (simulation_directory_path+'Models_{}_vs_{}_{}_ch{}').format(Modules[0], Modules[1], kind, len(observables))\n",
    "if not path.exists(save_dir):\n",
    "    makedirs(save_dir)\n",
    "print('Directory to save models: {}'.format(save_dir))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ded33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, LeakyReLU\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D, BatchNormalization, Flatten\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam, Adagrad\n",
    "\n",
    "\n",
    "def get_callbacks(monitor, save_dir):\n",
    "    mode = None\n",
    "    if 'loss' in monitor:\n",
    "        mode = 'min'\n",
    "    elif 'accuracy' in monitor:\n",
    "        mode = 'max'\n",
    "    assert mode != None, 'Check the monitor parameter!'\n",
    "\n",
    "    es = EarlyStopping(monitor=monitor, mode=mode, patience=10,\n",
    "                      min_delta=0., verbose=1)\n",
    "    rlp = ReduceLROnPlateau(monitor=monitor, mode=mode, factor=0.2, patience=5,\n",
    "                            min_lr=0.001, verbose=1)\n",
    "    mcp = ModelCheckpoint(path.join(save_dir, 'hm_jetscape_ml_model_best.h5'), monitor=monitor, \n",
    "                          save_best_only=True, mode=mode, verbose=1)\n",
    "    \n",
    "    return [es, rlp, mcp]\n",
    "\n",
    "def conv2d_layer_block(prev_layer, filters, dropout_rate, input_shape=None):\n",
    "    if input_shape != None:\n",
    "        prev_layer.add(Conv2D(filters=filters, kernel_size=5,\n",
    "                              kernel_initializer='he_uniform',\n",
    "                              padding='same',\n",
    "                              activation='relu',\n",
    "                              kernel_regularizer=l2(l=0.02),\n",
    "                              input_shape=input_shape\n",
    "                             )\n",
    "                      )\n",
    "    else:\n",
    "        prev_layer.add(Conv2D(filters=filters, kernel_size=5,\n",
    "                              kernel_initializer='he_uniform',\n",
    "                              padding='same',\n",
    "                              activation='relu',\n",
    "                              kernel_regularizer=l2(l=0.02),\n",
    "                             )\n",
    "                      )\n",
    "    prev_layer.add(Conv2D(filters=filters, kernel_size=5,\n",
    "                              kernel_initializer='he_uniform',\n",
    "                              padding='same',\n",
    "                              activation='relu',\n",
    "                              kernel_regularizer=l2(l=0.02)\n",
    "                             )\n",
    "                      )    \n",
    "    prev_layer.add(MaxPool2D(pool_size=(2, 2)))\n",
    "    prev_layer.add(Dropout(dropout_rate))\n",
    "    \n",
    "    return prev_layer\n",
    "\n",
    "def fc_layer_block(prev_layer, units, dropout_rate, last_layer=False):\n",
    "    if last_layer == False:\n",
    "        prev_layer.add(Dense(units, activation='relu',\n",
    "                             kernel_initializer='he_uniform',\n",
    "                             kernel_regularizer=l2(l=0.02)\n",
    "                            )\n",
    "                      )\n",
    "        prev_layer.add(Dropout(dropout_rate))\n",
    "    else:\n",
    "        prev_layer.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    return prev_layer\n",
    "\n",
    "def CNN_model(input_shape, lr, dropout1, dropout2):\n",
    "    model = Sequential()\n",
    "    model = conv2d_layer_block(model, 256, dropout1, input_shape)\n",
    "    model = conv2d_layer_block(model, 256, dropout1)\n",
    "    model = conv2d_layer_block(model, 256, dropout1)\n",
    "    model = conv2d_layer_block(model, 256, dropout1)\n",
    "    #model = conv2d_layer_block(model, 128, dropout1)\n",
    "    model.add(Flatten())\n",
    "    model = fc_layer_block(model, 1024, dropout2)\n",
    "    model = fc_layer_block(model, 1024, dropout2)\n",
    "    model = fc_layer_block(model, 1024, dropout2)\n",
    "    model = fc_layer_block(model, 1024, dropout2)\n",
    "    model = fc_layer_block(model, 1, None, last_layer=True)\n",
    "    \n",
    "    optimizer = Adam(learning_rate=lr)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer,\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2300aa58",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n#############################################################\")\n",
    "print(\"Reshaping dataset X-side dimention to be fit in the defined convolutional :\")\n",
    "\n",
    "print(\"\\nX train:\")\n",
    "print(x_train.shape)\n",
    "print (x_train.shape[0],x_train.shape[1],x_train.shape[2])\n",
    "x_train_reshaped=x_train.reshape(x_train.shape[0],x_train.shape[1],x_train.shape[2],1)\n",
    "print(x_train_reshaped.shape)\n",
    "\n",
    "print(\"\\nX val:\")\n",
    "print(x_val.shape)\n",
    "print (x_val.shape[0],x_val.shape[1],x_val.shape[2])\n",
    "x_val_reshaped=x_val.reshape(x_val.shape[0],x_val.shape[1],x_val.shape[2],1)\n",
    "print(x_train_reshaped.shape)\n",
    "\n",
    "print(\"\\nX test:\")\n",
    "print(x_test.shape)\n",
    "print (x_test.shape[0],x_test.shape[1],x_test.shape[2])\n",
    "x_test_reshaped=x_test.reshape(x_test.shape[0],x_test.shape[1],x_test.shape[2],1)\n",
    "print(x_test_reshaped.shape)\n",
    "print(\"#############################################################\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c71f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "## parameers for training\n",
    "n_epochs = 30\n",
    "# n_epochs=2\n",
    "batch_size = 256\n",
    "input_shape = x_train_reshaped.shape[1:]\n",
    "monitor='val_accuracy' #'val_accuracy' or 'val_loss'\n",
    "lr = 5e-6\n",
    "dropout1, dropout2 = 0.2, 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6db7567",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from time import time\n",
    "def train_network(train_set, val_set, n_epochs, lr, batch_size, monitor):\n",
    "    tf.keras.backend.clear_session()\n",
    "    X_train = train_set[0]\n",
    "    Y_train = train_set[1]\n",
    "    model = CNN_model(input_shape, lr, dropout1, dropout2)\n",
    "    callbacks = get_callbacks(monitor, save_dir)\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    start = time()\n",
    "    history = model.fit(X_train, Y_train, epochs=n_epochs, verbose=1, batch_size=batch_size, \n",
    "                        validation_data=val_set, shuffle=True, callbacks=callbacks)\n",
    "    train_time = (time()-start)/60.\n",
    "    return history, train_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3791146c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training and validation sets\n",
    "train_set, val_set = (x_train_reshaped, y_train), (x_val_reshaped, y_val)\n",
    "\n",
    "# train the network\n",
    "history, train_time = train_network(train_set, val_set, n_epochs, lr, batch_size, monitor)\n",
    "print(\"Elapsed Training Time:\")\n",
    "print(train_time)\n",
    "file_name='hm_jetscape_ml_model_history.csv'\n",
    "file_path=simulation_directory_path+file_name\n",
    "pd.DataFrame.from_dict(history.history).to_csv(file_path,index=False)\n",
    "\n",
    "\n",
    "file_name='hm_jetscape_ml_model_history.npy'\n",
    "file_path=simulation_directory_path+file_name\n",
    "np.save(file_path,history.history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a637fbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This section shall be just used after training or for stand alone evaluations\n",
    "# Building a dictionary which is accessable by dot\n",
    "class dotdict(dict):\n",
    "    \"\"\"dot.notation access to dictionary attributes\"\"\"\n",
    "    __getattr__ = dict.get\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__\n",
    "\n",
    "#Loading learning history after training \n",
    "file_name='hm_jetscape_ml_model_history.npy'\n",
    "file_path=simulation_directory_path+file_name\n",
    "\n",
    "\n",
    "history=dict({'history':np.load(file_path,allow_pickle='TRUE').item()})\n",
    "history=dotdict(history)\n",
    "print(history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba9ea7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "def plot_train_history(history):\n",
    "\n",
    "    color_list = ['red','blue','black','green']\n",
    "\n",
    "    plt.figure(figsize=(8, 2.5), dpi=100)\n",
    "\n",
    "    plt.subplot(121)\n",
    "    plt.plot(history.history['loss'], label='loss')\n",
    "    plt.plot(history.history['val_loss'], label='val_loss')\n",
    "    plt.yscale('log')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.title('Loss history')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(122)\n",
    "    plt.plot(history.history['accuracy'], label='accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='val_accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.title('Accuracy history')\n",
    "    plt.legend()\n",
    "    file_name='hm_jetscape_ml_plot_train_history.png'\n",
    "    file_path=simulation_directory_path+file_name\n",
    "    plt.savefig(file_path)\n",
    "    plt.show()\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f9e961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the training history for each fold\n",
    "plot_train_history(history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3871c9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "## load the best model\n",
    "best_model = load_model(path.join(save_dir,'hm_jetscape_ml_model_best.h5'))\n",
    "\n",
    "outputStr='Train   | Validation | Test sets\\n'\n",
    "\n",
    "## evaluate the model on train/val/test sets and append the results to lists\n",
    "_, train_acc = best_model.evaluate(x_train_reshaped, y_train, verbose=0)\n",
    "_, val_acc = best_model.evaluate(x_val_reshaped, y_val, verbose=0)\n",
    "_, test_acc = best_model.evaluate(x_test_reshaped, y_test, verbose=0)\n",
    "    \n",
    "## print out the accuracy\n",
    "outputStr+='{:.4f}%  {:.4f}%     {:.4f}%\\n'.format(train_acc * 100, val_acc * 100, test_acc * 100)\n",
    "print(outputStr)\n",
    "\n",
    "file_name=\"hm_jetscape_ml_model_evaluation.txt\"\n",
    "file_path=simulation_directory_path+file_name\n",
    "evaluation_file = open(file_path, \"w\")\n",
    "evaluation_file.write(outputStr)\n",
    "evaluation_file.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d7744c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "## plot confution matrix\n",
    "y_pred = best_model.predict_classes(x_test_reshaped)\n",
    "\n",
    "conf_mat = confusion_matrix(y_pred, y_test)\n",
    "sns.heatmap(conf_mat, annot=True, cmap='Blues', \n",
    "            xticklabels=Modules, yticklabels=Modules, fmt='g')\n",
    "plt.xlabel('Truth', fontsize=15)\n",
    "plt.ylabel('Prediction', fontsize=15)\n",
    "\n",
    "file_name='hm_jetscape_ml_model_confision_matrix.png'\n",
    "file_path=simulation_directory_path+file_name\n",
    "plt.savefig(file_path)\n",
    "\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "classification_report_str= classification_report(y_test,y_pred)\n",
    "\n",
    "print (classification_report_str)\n",
    "file_name=\"hm_jetscape_ml_model_evaluation.txt\"\n",
    "file_path=simulation_directory_path+file_name\n",
    "evaluation_file = open(file_path, \"a\")\n",
    "evaluation_file.write(classification_report_str)\n",
    "evaluation_file.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c851c207",
   "metadata": {},
   "source": [
    "#Validation Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92f3ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history():\n",
    "    hist = pd.DataFrame(history.history)\n",
    "\n",
    "    hist['epoch'] = history.epoch\n",
    "    # hist['rmse']=np.sqrt( hist.loss)\n",
    "    hist['accuracy']=np.sqrt( hist.accuracy)\n",
    "\n",
    "    print(hist)\n",
    "    print(\"Mean Training Accuracy\", np.mean(hist.accuracy))\n",
    "    # rmse_final = np.sqrt(float(hist['loss']))\n",
    "\n",
    "    plt.figure()\n",
    "    plt.xlabel('Epoch')\n",
    "    # plt.ylabel('Mean Square Error')\n",
    "    plt.ylabel('Loss(Blue)/Accuracy(Orange)')\n",
    "    plt.plot(hist['epoch'], hist['loss'], label='loss')\n",
    "    plt.plot(hist['epoch'], hist['accuracy'], label='accuracy')\n",
    "    # plt.plot(hist['epoch'], hist['rmse'], label = 'RMSE')\n",
    "    plt.legend()\n",
    "    plt.ylim([0,1])\n",
    "    plt.close()\n",
    "# plot_history()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#Binary Classification Results\n",
    "\n",
    "\n",
    "Classification                      | #epoch | Hadron \\begin{pmatrix} p_{T} \\end{pmatrix}\n",
    "-------------------                 |--------|------------------\n",
    "MATTER (in-medium) v.s. MATTER+LBT  |500| 95.88%\n",
    " \n",
    "\n",
    "#Precision - Recall - F1Score\n",
    "\n",
    "$precision= \\frac{true \\: positive}{true \\: positive \\: + \\:false \\: positive}$\n",
    "\n",
    "$recal= \\frac{true \\: positive}{true \\: positive \\: + \\: false \\: negative}$\n",
    "\n",
    "$f1-score= \\frac{2}{precision^{-1} \\: + \\: recall^{-1}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0563e3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source code credit for this function: https://gist.github.com/shaypal5/94c53d765083101efc0240d776a23823\n",
    "def print_confusion_matrix(confusion_matrix, class_names, figsize = (10,7), fontsize=14):\n",
    "    \"\"\"Prints a confusion matrix, as returned by sklearn.metrics.confusion_matrix, as a heatmap.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    confusion_matrix: numpy.ndarray\n",
    "        The numpy.ndarray object returned from a call to sklearn.metrics.confusion_matrix. \n",
    "        Similarly constructed ndarrays can also be used.\n",
    "    class_names: list\n",
    "        An ordered list of class names, in the order they index the given confusion matrix.\n",
    "    figsize: tuple\n",
    "        A 2-long tuple, the first value determining the horizontal size of the ouputted figure,\n",
    "        the second determining the vertical size. Defaults to (10,7).\n",
    "    fontsize: int\n",
    "        Font size for axes labels. Defaults to 14.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    matplotlib.figure.Figure\n",
    "        The resulting confusion matrix figure\n",
    "    \"\"\"\n",
    "    df_cm = pd.DataFrame(\n",
    "        confusion_matrix, index=class_names, columns=class_names, \n",
    "    )\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    try:\n",
    "        heatmap = sns.heatmap(df_cm, annot=True, fmt=\"d\")\n",
    "    except ValueError:\n",
    "        raise ValueError(\"Confusion matrix values must be integers.\")\n",
    "    heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=fontsize)\n",
    "    heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize=fontsize)\n",
    "    plt.ylabel('Truth')\n",
    "    plt.xlabel('Prediction')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d655b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcualte_precision_recall_f1score_confision_matrix():\n",
    "  prediction=model.predict(x_test)\n",
    "  prediction=[ 1 if current_y_test>0.5 else 0 for current_y_test in prediction ]\n",
    "  #checking if prediction vector is binary\n",
    "  truth=y_test\n",
    "  cm = confusion_matrix(truth,prediction)\n",
    "  print_confusion_matrix(cm,[\"MATTER\",\"MATTER-LBT\"])\n",
    "  print(classification_report(truth, prediction))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45a5aad",
   "metadata": {},
   "source": [
    "\n",
    "### Mean Square Error\n",
    "\n",
    "The mean square error is the sum of the squared differences between the prediction ($\\hat{y}$) and the expected ($y$).  MSE values are not of a particular unit.  If an MSE value has decreased for a model, that is good.  However, beyond this, there is not much more you can determine.  Low MSE values are desired.\n",
    "\n",
    "$ \\mbox{MSE} = \\frac{1}{n} \\sum_{i=1}^n \\left(\\hat{y}_i - y_i\\right)^2 $\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4716a8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn import metrics\n",
    "def calcualte_mse(x_test):\n",
    "\n",
    "  # Predict\n",
    "  pred = model.predict(x_test)\n",
    "  # Measure MSE error.  \n",
    "  mse = metrics.mean_squared_error(pred,y_test)\n",
    "  print(\"Final score (MSE): {}\".format(mse))\n",
    "  rmse = np.sqrt(mse)\n",
    "  print('Root Mean Square Error on test set: {}'.format(round(rmse, 3)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb8bc31",
   "metadata": {},
   "source": [
    "\n",
    "### Evaluate accuracy\n",
    "\n",
    "To predict new values, the Neural Network uses classifier.predict. I'm going to pass it the test values for x_test (which the Neural Network hasn't previously seen) and it will give me back a set of predictions. These predicitons will be probabilities, so I will clean them up by saying that if thye are greater than .5, I'll make them 1, else I'll make them 0.\n",
    "Next, compare how the model performs on the test dataset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11b8f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_test_loss_and_accuracy(x_test,):\n",
    "  y_test_prediction=model.predict(x_test)\n",
    "\n",
    "  y_test_prediction = [ 1 if y_test>0.5 else 0 for y_test in y_test_prediction ]\n",
    "  print(y_test_prediction)\n",
    "  total = 0\n",
    "  correct = 0\n",
    "  wrong = 0\n",
    "  for i in y_test_prediction:\n",
    "    total=total+1\n",
    "    if(y_test[i] == y_test_prediction[i]):\n",
    "      correct=correct+1\n",
    "    else:\n",
    "      wrong=wrong+1\n",
    "\n",
    "  print(\"Total \" + str(total))\n",
    "  print(\"Correct \" + str(correct))\n",
    "  print(\"Wrong \" + str(wrong))\n",
    "  print(correct/total)\n",
    "  print(wrong/total)\n",
    "\n",
    "  test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "  return (test_loss,test_acc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ad0806",
   "metadata": {},
   "source": [
    "\n",
    "Often times, the accuracy on the test dataset is a little less than the accuracy on the training dataset. This gap between training accuracy and test accuracy is an example of *overfitting*. In our case, the accuracy is better at 60.5. This is, in part, due to successful regularization accomplished with the Dropout layers.\n",
    "\n",
    "### Make predictions\n",
    "\n",
    "With the model trained, we can use it to make predictions about some events. Let's step outside the dataset for that and go with the beautiful high-resolution images generated by a real world expiremental dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ced13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# mnist_dream_path = 'images/mnist_dream.mp4'\n",
    "# mnist_prediction_path = 'images/mnist_dream_predicted.mp4'\n",
    "\n",
    "# # download the video if running in Colab\n",
    "# if not os.path.isfile(mnist_dream_path): \n",
    "#     print('downloading the sample video...')\n",
    "#     vid_url = this_tutorial_url + '/' + mnist_dream_path\n",
    "    \n",
    "#     mnist_dream_path = urllib.request.urlretrieve(vid_url)[0]\n",
    "                                                                                                  \n",
    "# def cv2_imshow(img):\n",
    "#     ret = cv2.imencode('.png', img)[1].tobytes() \n",
    "#     img_ip = IPython.display.Image(data=ret)\n",
    "#     IPython.display.display(img_ip)\n",
    "# def cap_read(frame):\n",
    "#     img= x_test[frame-1]\n",
    "#     if img is not None:\n",
    "#       ret=True\n",
    "#     else:\n",
    "#       ret=False\n",
    "#     return ret, img \n",
    "# # cap = cv2.VideoCapture(mnist_dream_path) \n",
    "# cap = x_test[100:300]\n",
    "# vw = None\n",
    "# frame = -1 # counter for debugging (mostly), 0-indexed\n",
    "\n",
    "# # go through all the frames and run our classifier on the high res MNIST images as they morph from number to number\n",
    "# while True: # should 481 frames\n",
    "#     frame += 1\n",
    "#     # ret, img = cap.read()\n",
    "#     ret, img = cap.read(frame)\n",
    "#     if not ret: break\n",
    "               \n",
    "#     assert img.shape[0] == img.shape[1] # should be a square\n",
    "#     if img.shape[0] != 32:\n",
    "#         img = cv2.resize(img, (32, 32))\n",
    "       \n",
    "#     #preprocess the image for prediction\n",
    "#     img_proc = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "#     img_proc = cv2.resize(img_proc, (32, 32))\n",
    "#     img_proc = preprocess_images(img_proc)\n",
    "#     img_proc = 1 - img_proc # inverse since training dataset is white text with black background\n",
    "\n",
    "#     net_in = np.expand_dims(img_proc, axis=0) # expand dimension to specify batch size of 1\n",
    "#     net_in = np.expand_dims(net_in, axis=3) # expand dimension to specify number of channels\n",
    "    \n",
    "#     preds = model.predict(net_in)[0]\n",
    "#     guess = np.argmax(preds)\n",
    "#     perc = np.rint(preds * 100).astype(int)\n",
    "    \n",
    "#     img = 255 - img\n",
    "#     pad_color = 0\n",
    "#     img = np.pad(img, ((0,0), (0,1280-720), (0,0)), mode='constant', constant_values=(pad_color))  \n",
    "    \n",
    "#     line_type = cv2.LINE_AA\n",
    "#     font_face = cv2.FONT_HERSHEY_SIMPLEX\n",
    "#     font_scale = 1.3        \n",
    "#     thickness = 2\n",
    "#     x, y = 740, 60\n",
    "#     color = (255, 255, 255)\n",
    "    \n",
    "#     text = \"Neural Network Output:\"\n",
    "#     cv2.putText(img, text=text, org=(x, y), fontScale=font_scale, fontFace=font_face, thickness=thickness,\n",
    "#                     color=color, lineType=line_type)\n",
    "    \n",
    "#     text = \"Input:\"\n",
    "#     cv2.putText(img, text=text, org=(30, y), fontScale=font_scale, fontFace=font_face, thickness=thickness,\n",
    "#                     color=color, lineType=line_type)   \n",
    "        \n",
    "#     y = 130\n",
    "#     for i, p in enumerate(perc):\n",
    "#         if i == guess: color = (255, 218, 158)\n",
    "#         else: color = (100, 100, 100)\n",
    "            \n",
    "#         rect_width = 0\n",
    "#         if p > 0: rect_width = int(p * 3.3)\n",
    "        \n",
    "#         rect_start = 180\n",
    "#         cv2.rectangle(img, (x+rect_start, y-5), (x+rect_start+rect_width, y-20), color, -1)\n",
    "\n",
    "#         text = '{}: {:>3}%'.format(i, int(p))\n",
    "#         cv2.putText(img, text=text, org=(x, y), fontScale=font_scale, fontFace=font_face, thickness=thickness,\n",
    "#                     color=color, lineType=line_type)\n",
    "#         y += 60\n",
    "    \n",
    "#     # if you don't want to save the output as a video, set this to False\n",
    "#     save_video = True\n",
    "    \n",
    "#     if save_video:\n",
    "#         if vw is None:\n",
    "#             codec = cv2.VideoWriter_fourcc(*'DIVX')\n",
    "#             vid_width_height = img.shape[1], img.shape[0]\n",
    "#             vw = cv2.VideoWriter(mnist_prediction_path, codec, 30, vid_width_height)\n",
    "#         # 15 fps above doesn't work robustly so we right frame twice at 30 fps\n",
    "#         vw.write(img)\n",
    "#         vw.write(img)\n",
    "    \n",
    "#     # scale down image for display\n",
    "#     img_disp = cv2.resize(img, (0,0), fx=0.5, fy=0.5)\n",
    "#     cv2_imshow(img_disp)\n",
    "#     IPython.display.clear_output(wait=True)\n",
    "        \n",
    "# cap.release()\n",
    "# if vw is not None:\n",
    "#     vw.release()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "jetscape-ml-tensorflow-cnn.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "ada828d16365d2b22d3899327f52f8feba3feb56b4fde7279c1cd0b9201605e0"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6 (default, Jan  8 2020, 19:59:22) \n[GCC 7.3.0]"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
