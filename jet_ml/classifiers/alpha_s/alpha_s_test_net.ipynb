{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_namespace():\n",
    "    import sys\n",
    "    sys.path.insert(1,'/wsu/home/gy/gy40/gy4065/hm.jetscapeml_source')\n",
    "    sys.path.insert(1,'/content/drive/My Drive/Projects/110_JetscapeMl/hm_jetscapeml_source')\n",
    "    sys.path.insert(1,'/content/drive/MyDrive/Projects/110_JetscapeMl/hm_jetscapeml_source')\n",
    "    sys.path.insert(1,'/g/My Drive/Projects/110_JetscapeMl/hm_jetscapeml_source')\n",
    "    sys.path.insert(1,'G:\\\\My Drive\\\\Projects\\\\110_JetscapeMl\\\\hm_jetscapeml_source')\n",
    "load_namespace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Preprocessor\n",
      "All Path are set\n",
      "Aggregatring all parameters values\n",
      "label_items:\n",
      " {'eloss_items': ['MMAT', 'MLBT'], 'alpha_s_items': [0.2, 0.3, 0.4], 'q0_items': [1.5, 2.0, 2.5]}\n",
      "Building required params for the loading the dataset file\n",
      "labels_str:\n",
      " {'eloss_items_str': 'MMAT_MLBT', 'alpha_s_items_str': '0.2_0.3_0.4', 'q0_items_str': '1.5_2.0_2.5'}\n",
      "Loading the whole dataset\n",
      "Extract the working column#1 for classification\n",
      "dataset.x: <class 'numpy.ndarray'> 1024000 (1000, 32, 32)\n",
      "dataset.y: <class 'numpy.ndarray'> 1000 (1000,)\n",
      "x shape: (1000, 32, 32, 1)\n",
      "x samples:(32, 32, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006908</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006803</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000693</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000215</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004978</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002313</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013841</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003726</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000541</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015641</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000277</td>\n",
       "      <td>0.006826</td>\n",
       "      <td>0.000625</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.012780</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013518</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002623</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004466</td>\n",
       "      <td>0.018978</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013089</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.025609</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003306</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0061</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012129</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001285</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001274</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005599</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007679</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000495</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008035</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006176</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0       1         2         3         4         5         6   \\\n",
       "0   0.000000  0.0000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1   0.000000  0.0000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2   0.000000  0.0000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "3   0.000215  0.0000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4   0.000000  0.0000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "5   0.000000  0.0000  0.000000  0.000000  0.000000  0.000000  0.004978   \n",
       "6   0.000000  0.0000  0.000000  0.000000  0.000000  0.013841  0.000000   \n",
       "7   0.000000  0.0000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "8   0.000000  0.0000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "9   0.000000  0.0000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "10  0.000000  0.0000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "11  0.000000  0.0000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "12  0.000000  0.0000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "13  0.000000  0.0000  0.000000  0.000000  0.000000  0.015641  0.000000   \n",
       "14  0.012780  0.0000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "15  0.000000  0.0000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "16  0.000000  0.0000  0.000000  0.000000  0.000000  0.000000  0.002623   \n",
       "17  0.000000  0.0000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "18  0.000000  0.0000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "19  0.000000  0.0000  0.025609  0.000000  0.000000  0.000000  0.000000   \n",
       "20  0.000000  0.0000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "21  0.000000  0.0000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "22  0.000000  0.0061  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "23  0.000000  0.0000  0.000000  0.000000  0.001285  0.000000  0.000000   \n",
       "24  0.000000  0.0000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "25  0.000000  0.0000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "26  0.000000  0.0000  0.000000  0.000000  0.000000  0.005599  0.000000   \n",
       "27  0.000000  0.0000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "28  0.000000  0.0000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "29  0.000000  0.0000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "30  0.000000  0.0000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "31  0.000000  0.0000  0.000000  0.008035  0.000000  0.000000  0.000000   \n",
       "\n",
       "          7         8         9   ...        22        23        24        25  \\\n",
       "0   0.000000  0.000000  0.000000  ...  0.006908  0.000000  0.000000  0.000000   \n",
       "1   0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "2   0.000693  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "3   0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "4   0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "5   0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.002313  0.000000   \n",
       "6   0.000000  0.000000  0.000000  ...  0.000000  0.003726  0.000000  0.000000   \n",
       "7   0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "8   0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "9   0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "10  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "11  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "12  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "13  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000277  0.006826   \n",
       "14  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "15  0.000000  0.000000  0.000000  ...  0.013518  0.000000  0.000000  0.000000   \n",
       "16  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.004466  0.018978   \n",
       "17  0.000000  0.000000  0.000000  ...  0.000000  0.013089  0.000000  0.000000   \n",
       "18  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "19  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "20  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "21  0.000000  0.000000  0.000000  ...  0.000000  0.003306  0.000000  0.000000   \n",
       "22  0.012129  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "23  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "24  0.000000  0.001274  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "25  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "26  0.007679  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "27  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "28  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "29  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "30  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "31  0.000000  0.000000  0.006176  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "          26   27   28        29   30       31  \n",
       "0   0.000000  0.0  0.0  0.000000  0.0  0.00000  \n",
       "1   0.006803  0.0  0.0  0.000000  0.0  0.00000  \n",
       "2   0.000000  0.0  0.0  0.000000  0.0  0.00000  \n",
       "3   0.000000  0.0  0.0  0.000000  0.0  0.00547  \n",
       "4   0.000000  0.0  0.0  0.000000  0.0  0.00000  \n",
       "5   0.000000  0.0  0.0  0.000000  0.0  0.00000  \n",
       "6   0.000000  0.0  0.0  0.000000  0.0  0.00000  \n",
       "7   0.000000  0.0  0.0  0.000000  0.0  0.00000  \n",
       "8   0.000000  0.0  0.0  0.000000  0.0  0.00000  \n",
       "9   0.000000  0.0  0.0  0.000000  0.0  0.00000  \n",
       "10  0.000000  0.0  0.0  0.000000  0.0  0.00000  \n",
       "11  0.000000  0.0  0.0  0.000000  0.0  0.00000  \n",
       "12  0.000000  0.0  0.0  0.000541  0.0  0.00000  \n",
       "13  0.000625  0.0  0.0  0.000000  0.0  0.00000  \n",
       "14  0.000000  0.0  0.0  0.000000  0.0  0.00000  \n",
       "15  0.000000  0.0  0.0  0.000000  0.0  0.00000  \n",
       "16  0.000000  0.0  0.0  0.000000  0.0  0.00000  \n",
       "17  0.000000  0.0  0.0  0.000000  0.0  0.00000  \n",
       "18  0.000000  0.0  0.0  0.000000  0.0  0.00000  \n",
       "19  0.000000  0.0  0.0  0.000000  0.0  0.00000  \n",
       "20  0.000000  0.0  0.0  0.000000  0.0  0.00000  \n",
       "21  0.000000  0.0  0.0  0.000000  0.0  0.00000  \n",
       "22  0.000000  0.0  0.0  0.000000  0.0  0.00000  \n",
       "23  0.000000  0.0  0.0  0.000000  0.0  0.00000  \n",
       "24  0.000000  0.0  0.0  0.000000  0.0  0.00000  \n",
       "25  0.000000  0.0  0.0  0.000000  0.0  0.00000  \n",
       "26  0.000000  0.0  0.0  0.000000  0.0  0.00000  \n",
       "27  0.000495  0.0  0.0  0.000000  0.0  0.00000  \n",
       "28  0.000000  0.0  0.0  0.000000  0.0  0.00000  \n",
       "29  0.000000  0.0  0.0  0.000000  0.0  0.00000  \n",
       "30  0.000000  0.0  0.0  0.000000  0.0  0.00000  \n",
       "31  0.000000  0.0  0.0  0.000000  0.0  0.00000  \n",
       "\n",
       "[32 rows x 32 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1  2\n",
       "0  0  0  1\n",
       "1  0  0  1\n",
       "2  1  0  0\n",
       "3  0  0  1\n",
       "4  0  0  1\n",
       "5  0  1  0\n",
       "6  0  0  1\n",
       "7  0  1  0\n",
       "8  0  0  1\n",
       "9  0  0  1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import preprocess_dataset as pred\n",
    "(x,y,num_classes)=pred.preprocess_dataset_for_alpha_s(1000)\n",
    "pred.get_preprocess_dataset_info(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(    \n",
    "    x, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"testnet\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"testnet\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12544</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,605,760</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">387</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m320\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12544\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │     \u001b[38;5;34m1,605,760\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │           \u001b[38;5;34m387\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,624,963</span> (6.20 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,624,963\u001b[0m (6.20 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,624,963</span> (6.20 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,624,963\u001b[0m (6.20 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import jet_ml.models.test_net as test_net\n",
    "input_shape=x[0].shape\n",
    "activation='softmax'\n",
    "model=test_net.build_model(input_shape,num_classes,activation)\n",
    "model=test_net.compile_model(model)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\HP\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\backend\\common\\global_state.py:82: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "Epoch 1/1000\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 1.09753, saving model to G:\\My Drive\\Projects\\110_JetscapeMl\\hm_jetscapeml_source\\data\\testnet\\model.keras\n",
      "6/6 - 4s - 632ms/step - accuracy: 0.3213 - loss: 1.0997 - val_accuracy: 0.3760 - val_loss: 1.0975\n",
      "Epoch 2/1000\n",
      "\n",
      "Epoch 2: val_loss improved from 1.09753 to 1.09445, saving model to G:\\My Drive\\Projects\\110_JetscapeMl\\hm_jetscapeml_source\\data\\testnet\\model.keras\n",
      "6/6 - 2s - 304ms/step - accuracy: 0.3813 - loss: 1.0941 - val_accuracy: 0.3880 - val_loss: 1.0945\n",
      "Epoch 3/1000\n",
      "\n",
      "Epoch 3: val_loss improved from 1.09445 to 1.08872, saving model to G:\\My Drive\\Projects\\110_JetscapeMl\\hm_jetscapeml_source\\data\\testnet\\model.keras\n",
      "6/6 - 2s - 276ms/step - accuracy: 0.4107 - loss: 1.0865 - val_accuracy: 0.3760 - val_loss: 1.0887\n",
      "Epoch 4/1000\n",
      "\n",
      "Epoch 4: val_loss improved from 1.08872 to 1.08126, saving model to G:\\My Drive\\Projects\\110_JetscapeMl\\hm_jetscapeml_source\\data\\testnet\\model.keras\n",
      "6/6 - 2s - 285ms/step - accuracy: 0.4173 - loss: 1.0691 - val_accuracy: 0.4080 - val_loss: 1.0813\n",
      "Epoch 5/1000\n",
      "\n",
      "Epoch 5: val_loss improved from 1.08126 to 1.08047, saving model to G:\\My Drive\\Projects\\110_JetscapeMl\\hm_jetscapeml_source\\data\\testnet\\model.keras\n",
      "6/6 - 2s - 266ms/step - accuracy: 0.4280 - loss: 1.0504 - val_accuracy: 0.3800 - val_loss: 1.0805\n",
      "Epoch 6/1000\n",
      "\n",
      "Epoch 6: val_loss did not improve from 1.08047\n",
      "6/6 - 1s - 235ms/step - accuracy: 0.4573 - loss: 1.0385 - val_accuracy: 0.3680 - val_loss: 1.0850\n",
      "Epoch 7/1000\n",
      "\n",
      "Epoch 7: val_loss did not improve from 1.08047\n",
      "6/6 - 1s - 240ms/step - accuracy: 0.4840 - loss: 1.0147 - val_accuracy: 0.3800 - val_loss: 1.1020\n",
      "Epoch 8/1000\n",
      "\n",
      "Epoch 8: val_loss did not improve from 1.08047\n",
      "6/6 - 1s - 227ms/step - accuracy: 0.4760 - loss: 0.9988 - val_accuracy: 0.3560 - val_loss: 1.1075\n",
      "Epoch 9/1000\n",
      "\n",
      "Epoch 9: val_loss did not improve from 1.08047\n",
      "6/6 - 1s - 214ms/step - accuracy: 0.4840 - loss: 0.9886 - val_accuracy: 0.3720 - val_loss: 1.1172\n",
      "Epoch 10/1000\n",
      "\n",
      "Epoch 10: val_loss did not improve from 1.08047\n",
      "6/6 - 1s - 218ms/step - accuracy: 0.5107 - loss: 0.9677 - val_accuracy: 0.3760 - val_loss: 1.1415\n",
      "Epoch 11/1000\n",
      "\n",
      "Epoch 11: val_loss did not improve from 1.08047\n",
      "6/6 - 1s - 209ms/step - accuracy: 0.5040 - loss: 0.9632 - val_accuracy: 0.3800 - val_loss: 1.1459\n",
      "Epoch 12/1000\n",
      "\n",
      "Epoch 12: val_loss did not improve from 1.08047\n",
      "6/6 - 1s - 209ms/step - accuracy: 0.5227 - loss: 0.9370 - val_accuracy: 0.3560 - val_loss: 1.1533\n",
      "Epoch 13/1000\n",
      "\n",
      "Epoch 13: val_loss did not improve from 1.08047\n",
      "6/6 - 1s - 240ms/step - accuracy: 0.5360 - loss: 0.9165 - val_accuracy: 0.3440 - val_loss: 1.1912\n",
      "Epoch 14/1000\n",
      "\n",
      "Epoch 14: val_loss did not improve from 1.08047\n",
      "6/6 - 2s - 292ms/step - accuracy: 0.5547 - loss: 0.9153 - val_accuracy: 0.3720 - val_loss: 1.1995\n",
      "Epoch 15/1000\n",
      "\n",
      "Epoch 15: val_loss did not improve from 1.08047\n",
      "6/6 - 1s - 195ms/step - accuracy: 0.5467 - loss: 0.9165 - val_accuracy: 0.3640 - val_loss: 1.2193\n",
      "Epoch 16/1000\n",
      "\n",
      "Epoch 16: val_loss did not improve from 1.08047\n",
      "6/6 - 1s - 220ms/step - accuracy: 0.5440 - loss: 0.8919 - val_accuracy: 0.3880 - val_loss: 1.2241\n",
      "Epoch 17/1000\n",
      "\n",
      "Epoch 17: val_loss did not improve from 1.08047\n",
      "6/6 - 1s - 236ms/step - accuracy: 0.5907 - loss: 0.8629 - val_accuracy: 0.4040 - val_loss: 1.2566\n",
      "Epoch 18/1000\n",
      "\n",
      "Epoch 18: val_loss did not improve from 1.08047\n",
      "6/6 - 1s - 215ms/step - accuracy: 0.5707 - loss: 0.8664 - val_accuracy: 0.3200 - val_loss: 1.2891\n",
      "Epoch 19/1000\n",
      "\n",
      "Epoch 19: val_loss did not improve from 1.08047\n",
      "6/6 - 1s - 214ms/step - accuracy: 0.5947 - loss: 0.8499 - val_accuracy: 0.4120 - val_loss: 1.2807\n",
      "Epoch 20/1000\n",
      "\n",
      "Epoch 20: val_loss did not improve from 1.08047\n",
      "6/6 - 1s - 212ms/step - accuracy: 0.5907 - loss: 0.8201 - val_accuracy: 0.3720 - val_loss: 1.3222\n",
      "Epoch 21/1000\n",
      "\n",
      "Epoch 21: val_loss did not improve from 1.08047\n",
      "6/6 - 1s - 196ms/step - accuracy: 0.6027 - loss: 0.8037 - val_accuracy: 0.3920 - val_loss: 1.3561\n",
      "Epoch 22/1000\n",
      "\n",
      "Epoch 22: val_loss did not improve from 1.08047\n",
      "6/6 - 1s - 205ms/step - accuracy: 0.6240 - loss: 0.7737 - val_accuracy: 0.3840 - val_loss: 1.4078\n",
      "Epoch 23/1000\n",
      "\n",
      "Epoch 23: val_loss did not improve from 1.08047\n",
      "6/6 - 1s - 239ms/step - accuracy: 0.6240 - loss: 0.7764 - val_accuracy: 0.3520 - val_loss: 1.3922\n",
      "Epoch 24/1000\n",
      "\n",
      "Epoch 24: val_loss did not improve from 1.08047\n",
      "6/6 - 1s - 228ms/step - accuracy: 0.6373 - loss: 0.7732 - val_accuracy: 0.3800 - val_loss: 1.4006\n",
      "Epoch 25/1000\n",
      "\n",
      "Epoch 25: val_loss did not improve from 1.08047\n",
      "6/6 - 1s - 203ms/step - accuracy: 0.6400 - loss: 0.7572 - val_accuracy: 0.3760 - val_loss: 1.4220\n",
      "Epoch 26/1000\n",
      "\n",
      "Epoch 26: val_loss did not improve from 1.08047\n",
      "6/6 - 1s - 180ms/step - accuracy: 0.6653 - loss: 0.7345 - val_accuracy: 0.3880 - val_loss: 1.4800\n",
      "Epoch 27/1000\n",
      "\n",
      "Epoch 27: val_loss did not improve from 1.08047\n",
      "6/6 - 2s - 278ms/step - accuracy: 0.6813 - loss: 0.7004 - val_accuracy: 0.4000 - val_loss: 1.4691\n",
      "Epoch 28/1000\n",
      "\n",
      "Epoch 28: val_loss did not improve from 1.08047\n",
      "6/6 - 1s - 240ms/step - accuracy: 0.6720 - loss: 0.6930 - val_accuracy: 0.3960 - val_loss: 1.4953\n",
      "Epoch 29/1000\n",
      "\n",
      "Epoch 29: val_loss did not improve from 1.08047\n",
      "6/6 - 1s - 164ms/step - accuracy: 0.6773 - loss: 0.6850 - val_accuracy: 0.3840 - val_loss: 1.5471\n",
      "Epoch 30/1000\n",
      "\n",
      "Epoch 30: val_loss did not improve from 1.08047\n",
      "6/6 - 1s - 161ms/step - accuracy: 0.6973 - loss: 0.6730 - val_accuracy: 0.3560 - val_loss: 1.5493\n",
      "Epoch 31/1000\n",
      "\n",
      "Epoch 31: val_loss did not improve from 1.08047\n",
      "6/6 - 1s - 161ms/step - accuracy: 0.7187 - loss: 0.6419 - val_accuracy: 0.3680 - val_loss: 1.5858\n",
      "Epoch 32/1000\n",
      "\n",
      "Epoch 32: val_loss did not improve from 1.08047\n",
      "6/6 - 1s - 167ms/step - accuracy: 0.7173 - loss: 0.6156 - val_accuracy: 0.3840 - val_loss: 1.6139\n",
      "Epoch 33/1000\n",
      "\n",
      "Epoch 33: val_loss did not improve from 1.08047\n",
      "6/6 - 1s - 173ms/step - accuracy: 0.7147 - loss: 0.6457 - val_accuracy: 0.3760 - val_loss: 1.6204\n",
      "Epoch 34/1000\n",
      "\n",
      "Epoch 34: val_loss did not improve from 1.08047\n",
      "6/6 - 1s - 153ms/step - accuracy: 0.7280 - loss: 0.6117 - val_accuracy: 0.3640 - val_loss: 1.6421\n",
      "Epoch 35/1000\n",
      "\n",
      "Epoch 35: val_loss did not improve from 1.08047\n",
      "6/6 - 1s - 175ms/step - accuracy: 0.7373 - loss: 0.5953 - val_accuracy: 0.3760 - val_loss: 1.6910\n",
      "Epoch 36/1000\n",
      "\n",
      "Epoch 36: val_loss did not improve from 1.08047\n",
      "6/6 - 2s - 276ms/step - accuracy: 0.7493 - loss: 0.5892 - val_accuracy: 0.3880 - val_loss: 1.7011\n",
      "Epoch 37/1000\n",
      "\n",
      "Epoch 37: val_loss did not improve from 1.08047\n",
      "6/6 - 1s - 168ms/step - accuracy: 0.7707 - loss: 0.5336 - val_accuracy: 0.3920 - val_loss: 1.7493\n",
      "Epoch 38/1000\n",
      "\n",
      "Epoch 38: val_loss did not improve from 1.08047\n",
      "6/6 - 1s - 161ms/step - accuracy: 0.7680 - loss: 0.5365 - val_accuracy: 0.3640 - val_loss: 1.8144\n",
      "Epoch 39/1000\n",
      "\n",
      "Epoch 39: val_loss did not improve from 1.08047\n",
      "6/6 - 1s - 211ms/step - accuracy: 0.8000 - loss: 0.5187 - val_accuracy: 0.3800 - val_loss: 1.8235\n",
      "Epoch 40/1000\n",
      "\n",
      "Epoch 40: val_loss did not improve from 1.08047\n",
      "6/6 - 1s - 215ms/step - accuracy: 0.7840 - loss: 0.5200 - val_accuracy: 0.3840 - val_loss: 1.8338\n",
      "Epoch 41/1000\n",
      "\n",
      "Epoch 41: val_loss did not improve from 1.08047\n",
      "6/6 - 1s - 222ms/step - accuracy: 0.8107 - loss: 0.4940 - val_accuracy: 0.4000 - val_loss: 1.8596\n",
      "Epoch 42/1000\n",
      "\n",
      "Epoch 42: val_loss did not improve from 1.08047\n",
      "6/6 - 1s - 184ms/step - accuracy: 0.8053 - loss: 0.4726 - val_accuracy: 0.3720 - val_loss: 1.9109\n",
      "Epoch 43/1000\n",
      "\n",
      "Epoch 43: val_loss did not improve from 1.08047\n",
      "6/6 - 1s - 201ms/step - accuracy: 0.8213 - loss: 0.4716 - val_accuracy: 0.4040 - val_loss: 1.9368\n",
      "Epoch 44/1000\n",
      "\n",
      "Epoch 44: val_loss did not improve from 1.08047\n",
      "6/6 - 1s - 241ms/step - accuracy: 0.8213 - loss: 0.4611 - val_accuracy: 0.3920 - val_loss: 2.0060\n",
      "Epoch 45/1000\n",
      "\n",
      "Epoch 45: val_loss did not improve from 1.08047\n",
      "6/6 - 1s - 226ms/step - accuracy: 0.8280 - loss: 0.4334 - val_accuracy: 0.3800 - val_loss: 2.0081\n",
      "Epoch 46/1000\n",
      "\n",
      "Epoch 46: val_loss did not improve from 1.08047\n",
      "6/6 - 1s - 194ms/step - accuracy: 0.8387 - loss: 0.4321 - val_accuracy: 0.3800 - val_loss: 2.0190\n",
      "Epoch 47/1000\n",
      "\n",
      "Epoch 47: val_loss did not improve from 1.08047\n",
      "6/6 - 1s - 202ms/step - accuracy: 0.8333 - loss: 0.4162 - val_accuracy: 0.3840 - val_loss: 2.0782\n",
      "Epoch 48/1000\n",
      "\n",
      "Epoch 48: val_loss did not improve from 1.08047\n",
      "6/6 - 1s - 202ms/step - accuracy: 0.8360 - loss: 0.4121 - val_accuracy: 0.4040 - val_loss: 2.1865\n",
      "Epoch 49/1000\n",
      "\n",
      "Epoch 49: val_loss did not improve from 1.08047\n",
      "6/6 - 1s - 198ms/step - accuracy: 0.8480 - loss: 0.3877 - val_accuracy: 0.3840 - val_loss: 2.1630\n",
      "Epoch 50/1000\n",
      "\n",
      "Epoch 50: val_loss did not improve from 1.08047\n",
      "6/6 - 1s - 215ms/step - accuracy: 0.8680 - loss: 0.3698 - val_accuracy: 0.3720 - val_loss: 2.2826\n",
      "Epoch 51/1000\n",
      "\n",
      "Epoch 51: val_loss did not improve from 1.08047\n",
      "6/6 - 1s - 221ms/step - accuracy: 0.8627 - loss: 0.3675 - val_accuracy: 0.3920 - val_loss: 2.2805\n",
      "Epoch 52/1000\n",
      "\n",
      "Epoch 52: val_loss did not improve from 1.08047\n",
      "6/6 - 1s - 227ms/step - accuracy: 0.8547 - loss: 0.3574 - val_accuracy: 0.3800 - val_loss: 2.3128\n",
      "Epoch 53/1000\n",
      "\n",
      "Epoch 53: val_loss did not improve from 1.08047\n",
      "6/6 - 1s - 218ms/step - accuracy: 0.8680 - loss: 0.3353 - val_accuracy: 0.3800 - val_loss: 2.3817\n",
      "Epoch 54/1000\n",
      "\n",
      "Epoch 54: val_loss did not improve from 1.08047\n",
      "6/6 - 1s - 214ms/step - accuracy: 0.8973 - loss: 0.3090 - val_accuracy: 0.3960 - val_loss: 2.5455\n",
      "Epoch 55/1000\n",
      "\n",
      "Epoch 55: val_loss did not improve from 1.08047\n",
      "6/6 - 1s - 187ms/step - accuracy: 0.8880 - loss: 0.3099 - val_accuracy: 0.3840 - val_loss: 2.5891\n",
      "Epoch 56/1000\n",
      "\n",
      "Epoch 56: val_loss did not improve from 1.08047\n",
      "6/6 - 1s - 193ms/step - accuracy: 0.8947 - loss: 0.3128 - val_accuracy: 0.3800 - val_loss: 2.6414\n",
      "Epoch 57/1000\n",
      "\n",
      "Epoch 57: val_loss did not improve from 1.08047\n",
      "6/6 - 1s - 199ms/step - accuracy: 0.9173 - loss: 0.2692 - val_accuracy: 0.3800 - val_loss: 2.6285\n",
      "Epoch 58/1000\n",
      "\n",
      "Epoch 58: val_loss did not improve from 1.08047\n",
      "6/6 - 1s - 202ms/step - accuracy: 0.9080 - loss: 0.2743 - val_accuracy: 0.3680 - val_loss: 2.6560\n",
      "Epoch 59/1000\n",
      "\n",
      "Epoch 59: val_loss did not improve from 1.08047\n",
      "6/6 - 1s - 212ms/step - accuracy: 0.9053 - loss: 0.2725 - val_accuracy: 0.3840 - val_loss: 2.7799\n",
      "Epoch 60/1000\n",
      "\n",
      "Epoch 60: val_loss did not improve from 1.08047\n",
      "6/6 - 1s - 224ms/step - accuracy: 0.9107 - loss: 0.2618 - val_accuracy: 0.3960 - val_loss: 2.8506\n",
      "Epoch 61/1000\n",
      "\n",
      "Epoch 61: val_loss did not improve from 1.08047\n",
      "6/6 - 1s - 217ms/step - accuracy: 0.9253 - loss: 0.2382 - val_accuracy: 0.3680 - val_loss: 2.8823\n",
      "Epoch 62/1000\n",
      "\n",
      "Epoch 62: val_loss did not improve from 1.08047\n",
      "6/6 - 1s - 211ms/step - accuracy: 0.9240 - loss: 0.2334 - val_accuracy: 0.3720 - val_loss: 2.8690\n",
      "Epoch 63/1000\n",
      "\n",
      "Epoch 63: val_loss did not improve from 1.08047\n",
      "6/6 - 1s - 196ms/step - accuracy: 0.9253 - loss: 0.2399 - val_accuracy: 0.3640 - val_loss: 2.9818\n",
      "Epoch 64/1000\n",
      "\n",
      "Epoch 64: val_loss did not improve from 1.08047\n",
      "6/6 - 1s - 206ms/step - accuracy: 0.9307 - loss: 0.2095 - val_accuracy: 0.3640 - val_loss: 3.0363\n",
      "Epoch 65/1000\n",
      "\n",
      "Epoch 65: val_loss did not improve from 1.08047\n",
      "6/6 - 1s - 199ms/step - accuracy: 0.9213 - loss: 0.2332 - val_accuracy: 0.3840 - val_loss: 3.0769\n",
      "Epoch 66/1000\n",
      "\n",
      "Epoch 66: val_loss did not improve from 1.08047\n",
      "6/6 - 1s - 206ms/step - accuracy: 0.9253 - loss: 0.2373 - val_accuracy: 0.3760 - val_loss: 3.0104\n",
      "Epoch 67/1000\n",
      "\n",
      "Epoch 67: val_loss did not improve from 1.08047\n",
      "6/6 - 1s - 195ms/step - accuracy: 0.9387 - loss: 0.2037 - val_accuracy: 0.3560 - val_loss: 3.0502\n",
      "Epoch 68/1000\n",
      "\n",
      "Epoch 68: val_loss did not improve from 1.08047\n",
      "6/6 - 1s - 216ms/step - accuracy: 0.9480 - loss: 0.1990 - val_accuracy: 0.3680 - val_loss: 3.1715\n",
      "Epoch 69/1000\n",
      "\n",
      "Epoch 69: val_loss did not improve from 1.08047\n",
      "6/6 - 1s - 223ms/step - accuracy: 0.9280 - loss: 0.2147 - val_accuracy: 0.3560 - val_loss: 3.1790\n",
      "Epoch 70/1000\n",
      "\n",
      "Epoch 70: val_loss did not improve from 1.08047\n",
      "6/6 - 1s - 217ms/step - accuracy: 0.9427 - loss: 0.1963 - val_accuracy: 0.3520 - val_loss: 3.2826\n",
      "Epoch 71/1000\n",
      "\n",
      "Epoch 71: val_loss did not improve from 1.08047\n",
      "6/6 - 2s - 307ms/step - accuracy: 0.9520 - loss: 0.1562 - val_accuracy: 0.3520 - val_loss: 3.2906\n",
      "Epoch 72/1000\n",
      "\n",
      "Epoch 72: val_loss did not improve from 1.08047\n",
      "6/6 - 1s - 217ms/step - accuracy: 0.9520 - loss: 0.1576 - val_accuracy: 0.3880 - val_loss: 3.4432\n",
      "Epoch 73/1000\n",
      "\n",
      "Epoch 73: val_loss did not improve from 1.08047\n",
      "6/6 - 1s - 203ms/step - accuracy: 0.9360 - loss: 0.1873 - val_accuracy: 0.3440 - val_loss: 3.4861\n",
      "Epoch 74/1000\n",
      "\n",
      "Epoch 74: val_loss did not improve from 1.08047\n",
      "6/6 - 1s - 194ms/step - accuracy: 0.9453 - loss: 0.1689 - val_accuracy: 0.3720 - val_loss: 3.4621\n",
      "Epoch 75/1000\n",
      "\n",
      "Epoch 75: val_loss did not improve from 1.08047\n",
      "6/6 - 1s - 192ms/step - accuracy: 0.9653 - loss: 0.1538 - val_accuracy: 0.3520 - val_loss: 3.4956\n",
      "Epoch 76/1000\n",
      "\n",
      "Epoch 76: val_loss did not improve from 1.08047\n",
      "6/6 - 1s - 197ms/step - accuracy: 0.9573 - loss: 0.1499 - val_accuracy: 0.3560 - val_loss: 3.4943\n",
      "Epoch 77/1000\n",
      "\n",
      "Epoch 77: val_loss did not improve from 1.08047\n",
      "6/6 - 1s - 193ms/step - accuracy: 0.9573 - loss: 0.1487 - val_accuracy: 0.3760 - val_loss: 3.6925\n",
      "Epoch 78/1000\n",
      "\n",
      "Epoch 78: val_loss did not improve from 1.08047\n",
      "6/6 - 1s - 196ms/step - accuracy: 0.9573 - loss: 0.1458 - val_accuracy: 0.3640 - val_loss: 3.6449\n",
      "Epoch 79/1000\n",
      "\n",
      "Epoch 79: val_loss did not improve from 1.08047\n",
      "6/6 - 1s - 196ms/step - accuracy: 0.9680 - loss: 0.1279 - val_accuracy: 0.3640 - val_loss: 3.6972\n",
      "Epoch 80/1000\n",
      "\n",
      "Epoch 80: val_loss did not improve from 1.08047\n",
      "6/6 - 1s - 187ms/step - accuracy: 0.9653 - loss: 0.1301 - val_accuracy: 0.3840 - val_loss: 3.8064\n",
      "Epoch 81/1000\n",
      "\n",
      "Epoch 81: val_loss did not improve from 1.08047\n",
      "6/6 - 1s - 212ms/step - accuracy: 0.9587 - loss: 0.1265 - val_accuracy: 0.3760 - val_loss: 3.8478\n",
      "Epoch 82/1000\n",
      "\n",
      "Epoch 82: val_loss did not improve from 1.08047\n",
      "6/6 - 1s - 199ms/step - accuracy: 0.9707 - loss: 0.1165 - val_accuracy: 0.3600 - val_loss: 3.9868\n",
      "Epoch 83/1000\n",
      "\n",
      "Epoch 83: val_loss did not improve from 1.08047\n",
      "6/6 - 1s - 207ms/step - accuracy: 0.9547 - loss: 0.1229 - val_accuracy: 0.3640 - val_loss: 3.9612\n",
      "Epoch 84/1000\n",
      "\n",
      "Epoch 84: val_loss did not improve from 1.08047\n",
      "6/6 - 1s - 191ms/step - accuracy: 0.9627 - loss: 0.1289 - val_accuracy: 0.3760 - val_loss: 3.9171\n",
      "Epoch 85/1000\n",
      "\n",
      "Epoch 85: val_loss did not improve from 1.08047\n",
      "6/6 - 1s - 193ms/step - accuracy: 0.9640 - loss: 0.1309 - val_accuracy: 0.3600 - val_loss: 3.9631\n",
      "Epoch 86/1000\n",
      "\n",
      "Epoch 86: val_loss did not improve from 1.08047\n",
      "6/6 - 1s - 197ms/step - accuracy: 0.9600 - loss: 0.1346 - val_accuracy: 0.3680 - val_loss: 4.1335\n",
      "Epoch 87/1000\n",
      "\n",
      "Epoch 87: val_loss did not improve from 1.08047\n",
      "6/6 - 1s - 187ms/step - accuracy: 0.9693 - loss: 0.1052 - val_accuracy: 0.3480 - val_loss: 4.1259\n",
      "Epoch 88/1000\n",
      "\n",
      "Epoch 88: val_loss did not improve from 1.08047\n",
      "6/6 - 1s - 201ms/step - accuracy: 0.9680 - loss: 0.1037 - val_accuracy: 0.3680 - val_loss: 4.1876\n",
      "Epoch 89/1000\n",
      "\n",
      "Epoch 89: val_loss did not improve from 1.08047\n",
      "6/6 - 1s - 199ms/step - accuracy: 0.9667 - loss: 0.1025 - val_accuracy: 0.3640 - val_loss: 4.1431\n",
      "Epoch 90/1000\n",
      "\n",
      "Epoch 90: val_loss did not improve from 1.08047\n",
      "6/6 - 1s - 188ms/step - accuracy: 0.9733 - loss: 0.1059 - val_accuracy: 0.3680 - val_loss: 4.2637\n",
      "Epoch 91/1000\n",
      "\n",
      "Epoch 91: val_loss did not improve from 1.08047\n",
      "6/6 - 1s - 232ms/step - accuracy: 0.9707 - loss: 0.1064 - val_accuracy: 0.3560 - val_loss: 4.2396\n",
      "Epoch 92/1000\n",
      "\n",
      "Epoch 92: val_loss did not improve from 1.08047\n",
      "6/6 - 1s - 225ms/step - accuracy: 0.9680 - loss: 0.1080 - val_accuracy: 0.3640 - val_loss: 4.1683\n",
      "Epoch 93/1000\n",
      "\n",
      "Epoch 93: val_loss did not improve from 1.08047\n",
      "6/6 - 1s - 237ms/step - accuracy: 0.9827 - loss: 0.0866 - val_accuracy: 0.3720 - val_loss: 4.2861\n",
      "Epoch 94/1000\n",
      "\n",
      "Epoch 94: val_loss did not improve from 1.08047\n",
      "6/6 - 1s - 226ms/step - accuracy: 0.9707 - loss: 0.0930 - val_accuracy: 0.3880 - val_loss: 4.2598\n",
      "Epoch 95/1000\n",
      "\n",
      "Epoch 95: val_loss did not improve from 1.08047\n",
      "6/6 - 1s - 215ms/step - accuracy: 0.9813 - loss: 0.0886 - val_accuracy: 0.3400 - val_loss: 4.2337\n",
      "Epoch 96/1000\n",
      "\n",
      "Epoch 96: val_loss did not improve from 1.08047\n",
      "6/6 - 1s - 220ms/step - accuracy: 0.9720 - loss: 0.0898 - val_accuracy: 0.3600 - val_loss: 4.4307\n",
      "Epoch 97/1000\n",
      "\n",
      "Epoch 97: val_loss did not improve from 1.08047\n",
      "6/6 - 1s - 211ms/step - accuracy: 0.9920 - loss: 0.0640 - val_accuracy: 0.3760 - val_loss: 4.4753\n",
      "Epoch 98/1000\n",
      "\n",
      "Epoch 98: val_loss did not improve from 1.08047\n",
      "6/6 - 1s - 219ms/step - accuracy: 0.9800 - loss: 0.0799 - val_accuracy: 0.3720 - val_loss: 4.4998\n",
      "Epoch 99/1000\n",
      "\n",
      "Epoch 99: val_loss did not improve from 1.08047\n",
      "6/6 - 1s - 215ms/step - accuracy: 0.9853 - loss: 0.0681 - val_accuracy: 0.3640 - val_loss: 4.5716\n",
      "Epoch 100/1000\n",
      "\n",
      "Epoch 100: val_loss did not improve from 1.08047\n",
      "6/6 - 1s - 216ms/step - accuracy: 0.9867 - loss: 0.0598 - val_accuracy: 0.3640 - val_loss: 4.5561\n",
      "Epoch 101/1000\n",
      "\n",
      "Epoch 101: val_loss did not improve from 1.08047\n",
      "6/6 - 1s - 227ms/step - accuracy: 0.9773 - loss: 0.0729 - val_accuracy: 0.3600 - val_loss: 4.6503\n",
      "Epoch 102/1000\n",
      "\n",
      "Epoch 102: val_loss did not improve from 1.08047\n",
      "6/6 - 1s - 213ms/step - accuracy: 0.9787 - loss: 0.0817 - val_accuracy: 0.3640 - val_loss: 4.6810\n",
      "Epoch 103/1000\n",
      "\n",
      "Epoch 103: val_loss did not improve from 1.08047\n",
      "6/6 - 1s - 215ms/step - accuracy: 0.9827 - loss: 0.0622 - val_accuracy: 0.3600 - val_loss: 4.6685\n",
      "Epoch 104/1000\n",
      "\n",
      "Epoch 104: val_loss did not improve from 1.08047\n",
      "6/6 - 1s - 214ms/step - accuracy: 0.9840 - loss: 0.0611 - val_accuracy: 0.3480 - val_loss: 4.6488\n",
      "Epoch 104: early stopping\n",
      "Restoring model weights from the end of the best epoch: 4.\n",
      "Elpased time: 0:02:16\n"
     ]
    }
   ],
   "source": [
    "batch_size=128\n",
    "epochs=1000\n",
    "monitor='val_loss' #'val_accuracy' or 'val_loss'\n",
    "model, history=test_net.train_model(model,\n",
    "                                    x_train,y_train, x_test,y_test, \n",
    "                                    epochs, batch_size, monitor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss 1.0812569856643677\n",
      "Test accuracy: 0.40799999237060547\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "Test accuracy: 0.40404040404040403\n"
     ]
    }
   ],
   "source": [
    "import jet_ml.evaluation as eval\n",
    "eval.get_accuracy_cpu(model=model,x_test=x_test,y_test=y_test)\n",
    "eval.get_accuracy_gpu(model=model,x_test=x_test,y_test=y_test,batch_size=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
