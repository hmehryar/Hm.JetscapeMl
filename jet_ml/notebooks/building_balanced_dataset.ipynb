{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook load both 10.8m and try to build datasets in 1k,10,100k,1m size with abalance amount in each feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading 10.8M events dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_namespace():\n",
    "    import sys\n",
    "    sys.path.insert(1,'/wsu/home/gy/gy40/gy4065/hm_jetscapeml_source')#WSU Grid\n",
    "    sys.path.insert(1,'/content/drive/My Drive/Projects/110_JetscapeMl/hm_jetscapeml_source')#Colab GDrive v1\n",
    "    sys.path.insert(1,'/content/drive/MyDrive/Projects/110_JetscapeMl/hm_jetscapeml_source')#Colab GDrive v2\n",
    "    sys.path.insert(1,f'/mnt/g/My Drive/Projects/110_JetscapeMl/hm_jetscapeml_source')#wsl gdrive\n",
    "    sys.path.insert(1,'G:\\\\My Drive\\\\Projects\\\\110_JetscapeMl\\\\hm_jetscapeml_source') #Windows GDrive\n",
    "    sys.path.insert(1,'/home/arsalan/Projects/110_JetscapeML/hm_jetscapeml_source/') #office tower\n",
    "    \n",
    "load_namespace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-14 13:54:44.722312: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-14 13:54:44.893826: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-14 13:54:44.924519: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-14 13:54:45.170415: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-14 13:54:47.399972: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory /mnt/g/My Drive/Projects/110_JetscapeMl/hm_jetscapeml_source/models/dataset_balanced_1k_dataset_size_HmSrv1 created.\n",
      "Directory /mnt/g/My Drive/Projects/110_JetscapeMl/hm_jetscapeml_source/reports/dataset_balanced_1k_dataset_size_HmSrv1 created.\n",
      "Directory /mnt/g/My Drive/Projects/110_JetscapeMl/hm_jetscapeml_source/reports/figures/dataset_balanced_1k_dataset_size_HmSrv1 created.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1728928490.614875    5839 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1728928490.944949    5839 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1728928490.945268    5839 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Project Root: /mnt/g/My Drive/Projects/110_JetscapeMl/hm_jetscapeml_source\\nData Directory: /mnt/g/My Drive/Projects/110_JetscapeMl/hm_jetscapeml_source/data\\nModels Directory: /mnt/g/My Drive/Projects/110_JetscapeMl/hm_jetscapeml_source/models\\nReports Directory: /mnt/g/My Drive/Projects/110_JetscapeMl/hm_jetscapeml_source/reports\\nFigures Directory: /mnt/g/My Drive/Projects/110_JetscapeMl/hm_jetscapeml_source/reports/figures\\nSimulation Models Directory: /mnt/g/My Drive/Projects/110_JetscapeMl/hm_jetscapeml_source/models/dataset_balanced_1k_dataset_size_HmSrv1\\nSimulation Reports Directory: /mnt/g/My Drive/Projects/110_JetscapeMl/hm_jetscapeml_source/reports/dataset_balanced_1k_dataset_size_HmSrv1\\nSimulation Figures Directory: /mnt/g/My Drive/Projects/110_JetscapeMl/hm_jetscapeml_source/reports/figures/dataset_balanced_1k_dataset_size_HmSrv1\\nEnvironment Details:\\n  TensorFlow Version: 2.17.0\\n  Keras Version: 3.5.0\\n  Python Version: 3.12.5 | packaged by conda-forge | (main, Aug  8 2024, 18:36:51) [GCC 12.4.0]\\n  Pandas Version: 2.2.2\\n  Scikit-Learn Version: 1.5.1\\n  GPU is available'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading/Preparing Environment for simulation\n",
    "from jet_ml.config import Config\n",
    "\n",
    "dataset_size=1000 #10800000 #1000000\n",
    "simulation_name=f\"dataset_balanced_{int(dataset_size/1000)}k_dataset_size_HmSrv1\"\n",
    "\n",
    "config=Config(simulation_name=simulation_name)\n",
    "config.__str__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "original_dataset_file_name = \"jet_ml_benchmark_config_01_to_09_alpha_0.2_0.3_0.4_q0_1.5_2.0_2.5_MMAT_MLBT_size_10000_shuffled.pkl\"\n",
    "original_dataset_file_path= f\"{config.DATA_DIR}/{original_dataset_file_name}\"\n",
    "\n",
    "display(original_dataset_file_path)\n",
    "dataset = pd.read_pickle(original_dataset_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the whole dataset\n",
      "dataset.x: <class 'numpy.ndarray'> 10240000 (10000, 32, 32)\n",
      "dataset.y: <class 'numpy.ndarray'> 30000 (10000, 3)\n"
     ]
    }
   ],
   "source": [
    "import jet_ml.dataset as ds\n",
    "original_dataset_size=10000 \n",
    "(x, y)=ds.load_dataset(size=original_dataset_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['MLBT', '0.2', '1.5'],\n",
       "       ['MLBT', '0.2', '2.0'],\n",
       "       ['MLBT', '0.2', '2.5'],\n",
       "       ['MLBT', '0.3', '1.5'],\n",
       "       ['MLBT', '0.3', '2.0'],\n",
       "       ['MLBT', '0.3', '2.5'],\n",
       "       ['MLBT', '0.4', '1.5'],\n",
       "       ['MLBT', '0.4', '2.0'],\n",
       "       ['MLBT', '0.4', '2.5'],\n",
       "       ['MMAT', '0.2', '1'],\n",
       "       ['MMAT', '0.3', '1'],\n",
       "       ['MMAT', '0.4', '1']], dtype='<U32')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([ 8,  8,  2, ...,  4,  9, 10])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Step 1: Find unique vectors in y and their indices\n",
    "unique_vectors, inverse_indices = np.unique(y, axis=0, return_inverse=True)\n",
    "display(unique_vectors)\n",
    "display(inverse_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of unique classes\n",
    "num_unique_vectors = len(unique_vectors)\n",
    "\n",
    "# Step 2: Calculate number of samples per class\n",
    "# We want to evenly distribute the samples, rounding down first\n",
    "samples_per_class = 1000 // num_unique_vectors\n",
    "remainder_samples = 1000 % num_unique_vectors  # Remaining samples to distribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Collect balanced indices\n",
    "balanced_indices = []\n",
    "\n",
    "for vector_id in range(num_unique_vectors):\n",
    "    # Get all indices for the current unique vector class\n",
    "    class_indices = np.where(inverse_indices == vector_id)[0]\n",
    "    \n",
    "    # Determine how many samples to take from this class\n",
    "    n_samples = samples_per_class\n",
    "    if remainder_samples > 0:\n",
    "        n_samples += 1\n",
    "        remainder_samples -= 1\n",
    "    \n",
    "    # Randomly sample `n_samples` from this class\n",
    "    if len(class_indices) >= n_samples:\n",
    "        selected_indices = np.random.choice(class_indices, n_samples, replace=False)\n",
    "    else:\n",
    "        # If there are fewer samples available than needed, take all\n",
    "        selected_indices = class_indices\n",
    "    \n",
    "    balanced_indices.extend(selected_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced dataset shape: (1000, 32, 32) (1000, 3)\n",
      "Balanced dataset size: 1024000 3000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 4: Create the balanced 1000-size dataset\n",
    "balanced_x = x[balanced_indices]\n",
    "balanced_y = y[balanced_indices]\n",
    "\n",
    "# Output: balanced_x and balanced_y will contain the balanced 1000-sample dataset\n",
    "print(\"Balanced dataset shape:\", balanced_x.shape, balanced_y.shape)\n",
    "print(\"Balanced dataset size:\", balanced_x.size, balanced_y.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced dataset shape: (996, 32, 32) (996, 3)\n",
      "Balanced dataset size: 1019904 2988\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Create the balanced dataset\n",
    "balanced_x = x[balanced_indices]\n",
    "balanced_y = y[balanced_indices]\n",
    "\n",
    "# Output: balanced_x and balanced_y will contain the balanced 1000-sample dataset\n",
    "print(\"Balanced dataset shape:\", balanced_x.shape, balanced_y.shape)\n",
    "print(\"Balanced dataset size:\", balanced_x.size, balanced_y.size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the balanced dataset\n",
    "import pickle\n",
    "def save_dataset(file_name,dataset):\n",
    "    with open(file_name, 'wb') as dataset_file:\n",
    "        pickle.dump(dataset,dataset_file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_dataset= (balanced_x, balanced_y)\n",
    "save_dataset(f\"{config.DATA_DIR}/jet_ml_benchmark_config_01_to_09_alpha_0.2_0.3_0.4_q0_1.5_2.0_2.5_MMAT_MLBT_size_1000_shuffled_balanced.pkl\",balanced_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Investivating its structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After loading the dataset\n",
      "dataset.x_train: <class 'numpy.ndarray'> 9953270784 (9719991, 32, 32)\n",
      "dataset.y_train: <class 'numpy.ndarray'> 29159973 (9719991, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([['MLBT', '0.4', '2.5'],\n",
       "       ['MLBT', '0.4', '2.5'],\n",
       "       ['MLBT', '0.2', '2.5'],\n",
       "       ['MMAT', '0.4', '1'],\n",
       "       ['MMAT', '0.4', '1']], dtype='<U32')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset.x_test: <class 'numpy.ndarray'> 1105920000 (1080000, 32, 32)\n",
      "dataset.y_test: <class 'numpy.ndarray'> 3240000 (1080000, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([['MMAT', '0.2', '1'],\n",
       "       ['MMAT', '0.2', '1'],\n",
       "       ['MMAT', '0.2', '1'],\n",
       "       ['MMAT', '0.2', '1'],\n",
       "       ['MMAT', '0.2', '1']], dtype='<U32')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"After loading the 10.8m dataset\")\n",
    "print(\"dataset.x_train:\",type(dataset[0][0]), dataset[0][0].size, dataset[0][0].shape)\n",
    "print(\"dataset.y_train:\",type(dataset[0][1]), dataset[0][1].size,dataset[0][1].shape)\n",
    "display( dataset[0][1][:5])\n",
    "\n",
    "print(\"dataset.x_test:\",type(dataset[1][0]), dataset[1][0].size, dataset[1][0].shape)\n",
    "print(\"dataset.y_test:\",type(dataset[1][1]), dataset[1][1].size, dataset[1][1].shape)\n",
    "display( dataset[1][1][:5])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12.5 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
