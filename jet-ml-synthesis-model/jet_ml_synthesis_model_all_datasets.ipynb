{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VT8MlMaW_zlo"
      },
      "source": [
        "[link text](https://)# Synthesis mode classification for Heavy Ion Collisions for diffrent dataset size and K-Folding\n",
        "\n",
        "**Authors:** [Haydar Mehryar](https://github.com/hmehryar) <br>\n",
        "**Date created:** 2023/12/01<br>\n",
        "**Last modified:** 2020/12/20<br>\n",
        "**Description:** Implementation of Simple Deep NEtwork for heavy ion colllisions classifiction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T4UgitN8Jch3",
        "outputId": "76d2758a-6e7c-4fcb-835c-0b31acdd1240"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\ProgramData\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
            "\n",
            "2.15.0\n",
            "False\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "# device_name = tf.test.gpu_device_name()\n",
        "# if device_name != '/device:GPU:0':\n",
        "#   raise SystemError('GPU device not found')\n",
        "# print('Found GPU at: {}'.format(device_name))\n",
        "print (tf.__version__)\n",
        "print(len(tf.config.experimental.list_physical_devices('GPU'))>0)\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hk_dCbkKm-4k",
        "outputId": "bd31efd3-5997-4152-c77b-3d0ca663f2c9"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "import sys\n",
        "sys.path.insert(1,'/wsu/home/gy/gy40/gy4065/hm.jetscapeml.source')\n",
        "sys.path.insert(1,'/content/drive/My Drive/Projects/110_JetscapeMl/hm.jetscapeml.source')\n",
        "sys.path.insert(1,'/g/My Drive/Projects/110_JetscapeMl/hm.jetscapeml.source')\n",
        "sys.path.insert(1,'G:\\\\My Drive\\\\Projects\\\\110_JetscapeMl\\\\hm.jetscapeml.source')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V5FNXhmJqtdZ",
        "outputId": "40c6ee24-a95c-4201-a892-8f9b8f5e28b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading/Installing Package => Begin\n",
            "\n",
            "\n",
            "\n",
            "Loading/Installing Package => End\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# loading libraries\n",
        "print('Loading/Installing Package => Begin\\n\\n')\n",
        "import jet_ml_dataset_builder.jet_ml_dataset_builder_utilities as util\n",
        "from jet_ml_dataset_builder.jet_ml_dataset_builder_utilities import set_directory_paths\n",
        "from jet_ml_dataset_builder.jet_ml_dataset_builder_utilities import parse_parameters\n",
        "from jet_ml_dataset_builder.jet_ml_dataset_builder_utilities import load_dataset\n",
        "from jet_ml_dataset_builder.jet_ml_dataset_builder_utilities import install\n",
        "\n",
        "import os\n",
        "from time import time\n",
        "import glob\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, History\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "import json\n",
        "\n",
        "\n",
        "tf.random.set_seed(1234)\n",
        "print('\\nLoading/Installing Package => End\\n\\n')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IrDIuS_1sqb6",
        "outputId": "810c844f-93e2-4558-e2fa-4602e223457f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "########################################################################\n",
            "Checking the running platforms\n",
            "\n",
            "Python version: 3.11.5\n",
            "OS: Windows\n",
            "OS version: 10\n",
            "running on Colab: False\n",
            "Dataset Directory Path: G:\\My Drive\\Projects\\110_JetscapeMl\\hm.jetscapeml.data\\\n",
            "Simulation Results Path: G:\\My Drive\\Projects\\110_JetscapeMl\\hm.jetscapeml.data\\simulation_results\\\n",
            "Dataset Directory Path: G:\\My Drive\\Projects\\110_JetscapeMl\\hm.jetscapeml.data\\\n",
            "Simulation Directory Path: G:\\My Drive\\Projects\\110_JetscapeMl\\hm.jetscapeml.data\\simulation_results\\\n",
            "########################################################################\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print('\\n########################################################################')\n",
        "print('Checking the running platforms\\n')\n",
        "\n",
        "# Call the function and retrieve the dataset_directory_path and simulation_directory_path\n",
        "dataset_directory_path, simulation_directory_path = set_directory_paths()\n",
        "\n",
        "# Access the dataset_directory_path and simulation_directory_path\n",
        "print(\"Dataset Directory Path:\", dataset_directory_path)\n",
        "print(\"Simulation Directory Path:\", simulation_directory_path)\n",
        "print('########################################################################\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jX15cxNSs6o3",
        "outputId": "1d02a92e-54ea-4f9d-b397-8a4e71281827"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "option --f not recognized\n",
            "Tokenized Arguments:\n",
            "\n",
            "Tokenized Values:\n",
            "y_class_label_items: ['MMAT', 'MLBT']\n",
            "alpha_s_items: [0.2, 0.3, 0.4]\n",
            "q0_items: [1.5, 2.0, 2.5]\n"
          ]
        }
      ],
      "source": [
        "# Call the function and retrieve the tokenized parameters\n",
        "tokenized_arguments, tokenized_values = parse_parameters()\n",
        "\n",
        "# Access the tokenized arguments and values\n",
        "print(\"Tokenized Arguments:\")\n",
        "for argument in tokenized_arguments:\n",
        "    print(argument)\n",
        "\n",
        "print(\"\\nTokenized Values:\")\n",
        "for argument, value in tokenized_values.items():\n",
        "    print(f\"{argument}: {value}\")\n",
        "\n",
        "y_class_label_items=['MMAT','MLBT']\n",
        "alpha_s_items=[0.2 ,0.3 ,0.4]\n",
        "q0_items=[1.5 ,2.0 ,2.5]\n",
        "\n",
        "print(\"y_class_label_items:\",y_class_label_items)\n",
        "print(\"alpha_s_items:\",alpha_s_items)\n",
        "print(\"q0_items:\",q0_items)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aTojIHZMtGcg",
        "outputId": "07669c33-4ff8-4243-f70b-7ee1601aa8ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Building required params for the loading the dataset file\n"
          ]
        }
      ],
      "source": [
        "print(\"Building required params for the loading the dataset file\")\n",
        "\n",
        "class_labels_str = '_'.join(y_class_label_items)\n",
        "alpha_s_items_str='_'.join(map(str, alpha_s_items))\n",
        "q0_items_str='_'.join(map(str, q0_items))\n",
        "total_size=9*1200000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "VUyAMovFsgDO"
      },
      "outputs": [],
      "source": [
        "# loading dataset by size and getting just the first column\n",
        "# Function to load datasets of different sizes\n",
        "def get_dataset(size):\n",
        "    dataset_file_name = f\"jet_ml_benchmark_config_01_to_09_alpha_{alpha_s_items_str}_q0_{q0_items_str}_{class_labels_str}_size_{size}_shuffled.pkl\"\n",
        "\n",
        "    dataset_file_name=simulation_directory_path+dataset_file_name\n",
        "    print(\"dataset_file_name:\",dataset_file_name)\n",
        "\n",
        "    dataset=load_dataset(dataset_file_name,has_test=False)\n",
        "    (dataset_x, dataset_y) = dataset\n",
        "    print('Extract the first column for binary classification')\n",
        "    # dataset_y = dataset_y[:, 0]\n",
        "    print(\"dataset.x:\",type(dataset_x), dataset_x.size, dataset_x.shape)\n",
        "    print(\"dataset.y:\",type(dataset_y), dataset_y.size,dataset_y.shape)\n",
        "    return dataset_x, dataset_y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HcmpjB3C-HH2"
      },
      "source": [
        "Building and Compiling the Classifier model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "def build_eloss_classifier_model(input_shape,num_of_classes):\n",
        "    # Define the model for eloss prediction\n",
        "    eloss_model = models.Sequential([\n",
        "        layers.Flatten(input_shape=input_shape),\n",
        "        layers.Dense(64, activation='relu'),\n",
        "        layers.Dense(32, activation='relu'),\n",
        "        layers.Dense(2, activation='sigmoid')  # Assuming two classes: 'MMAT' and 'MLBT'\n",
        "    ],name=\"eloss_net\")    \n",
        "    return eloss_model\n",
        "\n",
        "def build_alpha_s_classifier_model(input_shape,num_of_classes):\n",
        "    # Repeat the same process for alpha_s prediction\n",
        "    alpha_s_model = models.Sequential([\n",
        "        layers.Flatten(input_shape=input_shape),\n",
        "        layers.Dense(64, activation='relu'),\n",
        "        layers.Dense(32, activation='relu'),\n",
        "        layers.Dense(3, activation='softmax')  # Assuming three classes: 0.2, 0.3, 0.4\n",
        "    ],name=\"alpha_s_net\")\n",
        "    return alpha_s_model\n",
        "\n",
        "def build_q_0_classifier_model(input_shape, num_of_classes):\n",
        "    # Repeat for the third column\n",
        "    q_0_model = models.Sequential([\n",
        "        layers.Flatten(input_shape=input_shape),\n",
        "        layers.Dense(64, activation='relu'),\n",
        "        layers.Dense(32, activation='relu'),\n",
        "        layers.Dense(4, activation='softmax')  # Assuming four classes for the third column\n",
        "    ],name=\"q_0_net\")\n",
        "    return q_0_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "xveklusUk1T9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\ProgramData\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Assuming 'input_shape' is the shape of each input in dataset.x (e.g., (32, 32))\n",
        "input_shape=(32,32)\n",
        "eloss_net=build_eloss_classifier_model(input_shape,num_of_classes=2)\n",
        "alpha_s_net=build_alpha_s_classifier_model(input_shape,num_of_classes=3)\n",
        "q_0_net=build_q_0_classifier_model(input_shape,num_of_classes=4)\n",
        "learning_rate=0.001"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "#ToDo a better implementation which include learning rate\n",
        "# optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "# model.compile(\n",
        "#     # loss=\"sparse_categorical_crossentropy\",\n",
        "#     loss=\"binary_crossentropy\",\n",
        "#     optimizer=optimizer,\n",
        "#     # metrics=[\"sparse_categorical_accuracy\"],\n",
        "#     metrics=[\"accuracy\"],\n",
        "# )\n",
        "  \n",
        "def compile_eloss_classifier_model_with_hyperparam(model,learning_rate):\n",
        "  model.compile(optimizer='adam',\n",
        "                      loss='binary_crossentropy',\n",
        "                      metrics=['accuracy'])\n",
        "  model.summary()\n",
        "  return model\n",
        "def compile_alpha_s_classifier_model_with_hyperparam(model,learning_rate):\n",
        "  model.compile(optimizer='adam',\n",
        "                        loss='sparse_categorical_crossentropy',\n",
        "                        metrics=['accuracy'])\n",
        "  model.summary()\n",
        "  return model\n",
        "def compile_q_0_classifier_model_with_hyperparam(model,learning_rate):\n",
        "  model.compile(optimizer='adam',\n",
        "                          loss='sparse_categorical_crossentropy',\n",
        "                          metrics=['accuracy'])\n",
        "  model.summary()\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6sK2bSktpOO",
        "outputId": "21d03f00-1c64-449e-ef92-6f8a8258baf3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\ProgramData\\anaconda3\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "Model: \"eloss_net\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten (Flatten)           (None, 1024)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 64)                65600     \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 32)                2080      \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 2)                 66        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 67746 (264.63 KB)\n",
            "Trainable params: 67746 (264.63 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Model: \"alpha_s_net\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_1 (Flatten)         (None, 1024)              0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 64)                65600     \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 32)                2080      \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 3)                 99        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 67779 (264.76 KB)\n",
            "Trainable params: 67779 (264.76 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Model: \"q_0_net\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_2 (Flatten)         (None, 1024)              0         \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 64)                65600     \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 32)                2080      \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 4)                 132       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 67812 (264.89 KB)\n",
            "Trainable params: 67812 (264.89 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "eloss_net=compile_eloss_classifier_model_with_hyperparam(eloss_net,learning_rate)\n",
        "alpha_s_net=compile_alpha_s_classifier_model_with_hyperparam(alpha_s_net,learning_rate)\n",
        "q_0_net=compile_q_0_classifier_model_with_hyperparam(q_0_net,learning_rate)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n",
            "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n",
            "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
          ]
        }
      ],
      "source": [
        "tf.keras.utils.plot_model(eloss_net, show_shapes=True)\n",
        "tf.keras.utils.plot_model(alpha_s_net, show_shapes=True)\n",
        "tf.keras.utils.plot_model(q_0_net, show_shapes=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "6CBjsFewAJxe"
      },
      "outputs": [],
      "source": [
        "def split_dataset(dataset_x, dataset_y, test_size=0.2, random_state=None):\n",
        "    \"\"\"\n",
        "    Split the dataset into training and testing sets.\n",
        "\n",
        "    Parameters:\n",
        "    - dataset_x: The input data (3D array).\n",
        "    - dataset_y: The target values (2D array).\n",
        "    - test_size: The proportion of the dataset to include in the test split.\n",
        "    - random_state: Seed for random number generation.\n",
        "\n",
        "    Returns:\n",
        "    - x_train, x_test: The split input data for training and testing.\n",
        "    - y_train, y_test: The split target values for training and testing.\n",
        "    \"\"\"\n",
        "    # Flatten the input data to 2D\n",
        "    flattened_dataset_x = dataset_x.reshape(dataset_x.shape[0], -1)\n",
        "\n",
        "    # Split the dataset\n",
        "    x_train, x_test,  y_train, y_test = \\\n",
        "        train_test_split(flattened_dataset_x, dataset_y, test_size=test_size, random_state=random_state)\n",
        "\n",
        "    # Reshape the input data back to 3D\n",
        "    x_train = x_train.reshape(x_train.shape[0], dataset_x.shape[1], dataset_x.shape[2])\n",
        "    x_test = x_test.reshape(x_test.shape[0], dataset_x.shape[1], dataset_x.shape[2])\n",
        "\n",
        "    return x_train, x_test, y_train, y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "5sWogII7Z06-"
      },
      "outputs": [],
      "source": [
        "def parse_dataset(x_train,x_test):\n",
        "    train_labels = []\n",
        "    test_labels = []\n",
        "    i=0\n",
        "    for f in x_train:\n",
        "\n",
        "        train_labels.append(i)\n",
        "        i=i+1\n",
        "    for f in x_test:\n",
        "        test_labels.append(i)\n",
        "        i=i+1\n",
        "    return (\n",
        "        np.array(train_labels),\n",
        "        np.array(test_labels),\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "def augment(points, label):\n",
        "    # jitter points\n",
        "    points += tf.random.uniform(points.shape, -0.005, 0.005, dtype=tf.float64)\n",
        "    # shuffle points\n",
        "    points = tf.random.shuffle(points)\n",
        "    return points, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "8zniYk7xKyBy"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, x_test, y_test):\n",
        "    \"\"\"\n",
        "    Evaluate the performance of a trained model on test data.\n",
        "\n",
        "    Parameters:\n",
        "    - model: The trained deep neural model.\n",
        "    - x_test: Test data, it shall be in the flatten format, each entry contains 1024x1 data.\n",
        "    - y_test: True labels.\n",
        "\n",
        "    Returns:\n",
        "    - accuracy: Accuracy of the model on the test data.\n",
        "    - confusion_matrix: Confusion matrix for the predictions.\n",
        "    \"\"\"\n",
        "    # Assuming model is your trained deep neural model\n",
        "    y_pred = model.predict(x_test)\n",
        "    print(\"y_pred\",y_pred)\n",
        "    y_pred_class = np.argmax(y_pred, axis=1)  # Extracting the class with the highest probability\n",
        "\n",
        "    # Assuming y_true is a Nx2 array where each row contains the true class probabilities\n",
        "    y_true_class = np.argmax(y_test, axis=1)  # Extracting the class with the highest true probability\n",
        "\n",
        "    accuracy = accuracy_score(y_true_class, y_pred_class)\n",
        "    print(f'Accuracy: {accuracy}')\n",
        "\n",
        "    cm = confusion_matrix(y_true_class, y_pred_class)\n",
        "    print(f'Confusion Matrix: {cm}')\n",
        "\n",
        "    return accuracy, cm\n",
        "\n",
        "# Example usage:\n",
        "# accuracy, confusion_matrix = evaluate_model(trained_model, test_data, true_labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess_y_eloss(y_train,y_test):\n",
        "    # One-hot encode the categorical variable for eloss\n",
        "    # print(y_train)\n",
        "    y_train_0_categorical = np.array(y_train).reshape(-1, 1)\n",
        "    y_test_0_categorical = np.array(y_test).reshape(-1, 1)\n",
        "\n",
        "    encoder = OneHotEncoder(sparse_output=False)\n",
        "    y_train_0_categorical_encoded = encoder.fit_transform(y_train_0_categorical)\n",
        "    y_test_0_categorical_encoded = encoder.transform(y_test_0_categorical)\n",
        "    return y_train_0_categorical_encoded,y_test_0_categorical_encoded\n",
        "\n",
        "def convert_y_to_float(y_train,y_test):\n",
        "    y_train = y_train.astype(np.float32)\n",
        "    y_test= y_test.astype(np.float32)\n",
        "    return y_train,y_test\n",
        "\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "def convert_y_to_categorical(y_train,y_test):\n",
        "    # Step 1: Convert string labels to integer labels\n",
        "    label_encoder = LabelEncoder()\n",
        "    y_train_encoded = label_encoder.fit_transform(y_train)\n",
        "    y_test_encoded= label_encoder.fit_transform(y_test)\n",
        "    # Step 2: Convert integer labels to one-hot encoded format\n",
        "    num_classes = len(label_encoder.classes_)\n",
        "    y_train_categorical = to_categorical(y_train_encoded, num_classes=num_classes)\n",
        "    y_test_categorical = to_categorical(y_test_encoded, num_classes=num_classes)\n",
        "\n",
        "    return y_train_categorical,y_test_categorical\n",
        "\n",
        "\n",
        "\n",
        "def preprocess_y_alpha_s(y_train,y_test):\n",
        "    y_train_categorical,y_test_categorical=convert_y_to_categorical(y_train,y_test)\n",
        "    return y_train_categorical,y_test_categorical\n",
        "\n",
        "def preprocess_y_q_0(y_train,y_test):\n",
        "    y_train_categorical,y_test_categorical=convert_y_to_categorical(y_train,y_test)\n",
        "    return y_train_categorical,y_test_categorical"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "Jbi8LtS3bOtH"
      },
      "outputs": [],
      "source": [
        "def preprocess_dataset(dataset_x, dataset_y):\n",
        "  print(\"Pre-processing\")\n",
        "  # Example usage:\n",
        "  x_train, x_test, y_train, y_test= \\\n",
        "    split_dataset(dataset_x, dataset_y, test_size=0.2, random_state=None)\n",
        "  print(\"deleting the original dataset after splitting ...\")\n",
        "  del dataset_x,dataset_y\n",
        "  \n",
        "  print(\"x_train:\",type(x_train), x_train.size, x_train.shape)\n",
        "  print(\"y_train:\",type(y_train), y_train.size,y_train.shape)\n",
        "\n",
        "  print(\"x_test:\",type(x_test), x_test.size, x_test.shape)\n",
        "  print(\"y_test:\",type(y_test), y_test.size,y_test.shape)\n",
        "  \n",
        "  # train_labels, test_labels = parse_dataset(x_train,x_test)\n",
        "  # print(train_labels.size,test_labels.size)\n",
        "\n",
        "  # BATCH_SIZE = 32\n",
        "\n",
        "  # train_dataset = train_dataset.shuffle(len(x_train)).map(augment).batch(BATCH_SIZE)\n",
        "  # test_dataset = test_dataset.shuffle(len(x_test)).batch(BATCH_SIZE)\n",
        "\n",
        "  print(\"Preprocess y_train and y_test\")\n",
        "  y_train_eloss, y_test_eloss,y_train_alpha_s, y_test_alpha_s,y_train_q_0, y_test_q_0=([],[],[],[],[],[])\n",
        "\n",
        "  \n",
        "  print(\"y_train[:,0]:\",type(y_train[:,0]), y_train[:,0].size,y_train[:,0].shape)\n",
        "  y_train_eloss, y_test_eloss=preprocess_y_eloss(y_train[:,0],y_test[:,0])\n",
        "  print(y_train[:4,0])\n",
        "  print(y_train_eloss[:4])\n",
        "\n",
        "  print(\"y_train[:,1]:\",type(y_train[:,1]), y_train[:,1].size,y_train[:,1].shape)\n",
        "  y_train_alpha_s, y_test_alpha_s=preprocess_y_alpha_s(y_train[:,1],y_test[:,1])\n",
        "  print(y_train[:4,1])\n",
        "  print(y_train_alpha_s[:4])\n",
        "\n",
        "  print(\"y_train[:,2]:\",type(y_train[:,2]), y_train[:,2].size,y_train[:,2].shape)\n",
        "  y_train_q_0, y_test_q_0=preprocess_y_q_0(y_train[:,2],y_test[:,2])\n",
        "  print(y_train[:4,1])\n",
        "  print(y_train_alpha_s[:4])\n",
        "\n",
        "  return (x_train, y_train_eloss,y_train_alpha_s,y_train_q_0, \\\n",
        "          x_test, y_test_eloss,y_test_alpha_s,y_test_q_0)\n",
        "# (dataset_x, dataset_y)= get_dataset(1000)\n",
        "# (x_train, y_train_eloss,y_train_alpha_s,y_train_q_0, \\\n",
        "#   x_test, y_test_eloss,y_test_alpha_s,y_test_q_0)=\\\n",
        "# preprocess_dataset(dataset_x, dataset_y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "ILdXu--e2P_6"
      },
      "outputs": [],
      "source": [
        "def plot_training_history(history,simulation_path):\n",
        "    # Plot training & validation accuracy values\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['accuracy'])\n",
        "    plt.plot(history.history['val_accuracy'])\n",
        "    plt.title('Model Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "    # Set ticks on the epoch axis to display only integer values\n",
        "    plt.xticks(range(0, len(history.history['accuracy'])+1,5))\n",
        "\n",
        "    # Plot training & validation loss values\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.plot(history.history['val_loss'])\n",
        "    plt.title('Model Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "    # Set ticks on the epoch axis to display only integer values\n",
        "    plt.xticks(range(0, len(history.history['accuracy'])+1,5))\n",
        "\n",
        "    # Adjust layout and show the plot\n",
        "    plt.tight_layout()\n",
        "\n",
        "\n",
        "    # Save the plot with high resolution (300 dpi)\n",
        "    file_name='_accuracy_loss.png'\n",
        "    file_path=simulation_path+file_name\n",
        "    plt.savefig(file_path, dpi=300)\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "    return file_path\n",
        "\n",
        "# Plot the training history\n",
        "# plot_training_history_path=plot_training_history(history,simulation_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "pUyg0Teluaa0"
      },
      "outputs": [],
      "source": [
        "def save_training_history(history,simulation_path):\n",
        "  # Save the training history to a file (e.g., JSON format)\n",
        "\n",
        "  training_history_file_path =simulation_path+'_training_history'\n",
        "  # training_history_file_path  =simulation_directory_path+training_history_file_name\n",
        "\n",
        "  training_history_file_path_json=training_history_file_path+'.json'\n",
        "  with open(training_history_file_path_json, 'w') as f:\n",
        "      json.dump(history.history, f)\n",
        "  print(training_history_file_path_json)\n",
        "\n",
        "  training_history_file_path_csv=training_history_file_path+'.csv'\n",
        "  pd.DataFrame.from_dict(history.history).to_csv(training_history_file_path_csv,index=False)\n",
        "  print(training_history_file_path_csv)\n",
        "\n",
        "  training_history_file_path_npy=training_history_file_path+'.npy'\n",
        "  np.save(training_history_file_path_npy,history.history)\n",
        "  print(training_history_file_path_npy)\n",
        "  return training_history_file_path_json,training_history_file_path_csv,training_history_file_path_csv\n",
        "\n",
        "# training_history_file_path_json,training_history_file_path_csv,training_history_file_path_csv = \\\n",
        "#   save_training_history(history,simulation_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "4Txe-hJQNEZx"
      },
      "outputs": [],
      "source": [
        "# train and evaluate classifiers\n",
        "# This method shall get the cloud points as the trainset, to be trained by pointnet\n",
        "def train_and_evaluate_classifier_kfold(model, x_train,y_train , x_test, y_test, n_epochs, monitor, k_folds,simulation_path):\n",
        "\n",
        "    # Use KFold for k-fold cross-validation\n",
        "    kfold = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
        "\n",
        "    accuracies = []\n",
        "    cms = []\n",
        "    train_times = []\n",
        "    all_histories = []  # Store histories for each fold\n",
        "    plots=[]\n",
        "    models=[]\n",
        "    for fold, (train_index, val_index) in enumerate(kfold.split(x_train)):\n",
        "    # train_index, val_index in kfold.split(x_train):\n",
        "        x_train_fold, x_val_fold = x_train[train_index], x_train[val_index]\n",
        "        y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n",
        "\n",
        "        # Include both ModelCheckpoint and History callbacks in the callbacks list\n",
        "        # callbacks=[checkpoint_callback]\n",
        "        fold_path=f'{simulation_path}_fold_{fold}'\n",
        "        # Use ModelCheckpoint callback to save the best model\n",
        "        best_model_file_path = f'{fold_path}_best_model.keras'\n",
        "        models.append(best_model_file_path)\n",
        "        checkpoint_callback = ModelCheckpoint(\n",
        "            best_model_file_path,\n",
        "            monitor=monitor,\n",
        "            save_best_only=True,\n",
        "            mode=\"max\" if monitor == \"val_accuracy\" else \"min\",\n",
        "            verbose=1\n",
        "        )\n",
        "\n",
        "        # Use History callback to retrieve the training history\n",
        "        history_callback = History()\n",
        "        \n",
        "        start = time()\n",
        "        history = model.fit(\n",
        "            x_train_fold,\n",
        "            y_train_fold,\n",
        "            epochs=n_epochs,\n",
        "            validation_data=(x_val_fold, y_val_fold),\n",
        "            callbacks=[checkpoint_callback]\n",
        "        )\n",
        "        train_time = (time()-start)/60.0\n",
        "        train_times.append(train_time)\n",
        "        # Append history to the list\n",
        "        all_histories.append(history.history)\n",
        "\n",
        "        plot=plot_training_history(history,fold_path)\n",
        "        training_history_file_path_json,training_history_file_path_csv,training_history_file_path_csv = \\\n",
        "         save_training_history(history,fold_path)\n",
        "        plots.append(plot)\n",
        "\n",
        "        accuracy, confusion_matrix = evaluate_model(model, x_test, y_test)\n",
        "        accuracies.append(accuracy)\n",
        "        cms.append(confusion_matrix)\n",
        "\n",
        "    return accuracies, cms, train_times, all_histories, plots,models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "lzP5rLoMLRqp"
      },
      "outputs": [],
      "source": [
        "# Load the DataFrame from the saved file\n",
        "def load_csv_into_dataframe(file_path):\n",
        "  df_results = pd.read_csv(file_path, sep='\\t')\n",
        "  return df_results\n",
        "#example usage\n",
        "# df_results=load_csv_into_dataframe(\"/content/drive/MyDrive/Colab Notebooks/binary_classification_results_kfold_errorbar.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "3lYKLbf1TxpA"
      },
      "outputs": [],
      "source": [
        "def plot_save_mean_error_bar(df_results,simulation_path):\n",
        "    # df_results= df_results_kfold_errorbar\n",
        "    print(df_results)\n",
        "    # Set a seaborn style (optional)\n",
        "    sns.set(style=\"whitegrid\")\n",
        "    \n",
        "\n",
        "\n",
        "    # Define a dictionary to map classifiers to markers\n",
        "    marker_dict = {\n",
        "\n",
        "        'ElossNet': 'o',\n",
        "        'AlphaSNet': 's',\n",
        "        'Q0Net': '^'\n",
        "    }\n",
        "    plt.figure(figsize=(10, 6))\n",
        "\n",
        "    for clf_name, group in df_results.groupby('Classifier'):\n",
        "        plt.errorbar(\n",
        "            group['Dataset_Size'],\n",
        "            group['Mean_Accuracy'],\n",
        "            yerr=group['Std_Accuracy'],\n",
        "            label=clf_name,\n",
        "            marker=marker_dict.get(clf_name, 'o'),  # Use 'o' as default marker if not found in the dictionary\n",
        "            capsize=5\n",
        "        )\n",
        "\n",
        "    plt.xscale('log')  # Set x-axis to logarithmic scale\n",
        "    plt.xlabel('Dataset Size (log scale)')\n",
        "    plt.ylabel('Mean Accuracy')\n",
        "    plt.title('Energy loss, alpha_s, and Q_0 Classification Accuracy with Error Bars for Different Dataset Sizes')\n",
        "    plt.legend()\n",
        "    # plt.grid(True)\n",
        "    # Save the plot with high resolution (300 dpi)\n",
        "    accuracy_errorbar_plot_path=simulation_path+'_accuracy_errorbar_plot.png'\n",
        "    plt.savefig(accuracy_errorbar_plot_path, dpi=300)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "lFHDNMtlQ6Zh"
      },
      "outputs": [],
      "source": [
        "def train_and_evaluate_classifier_for_all_datasets(dataset_sizes,classifiers,simulation_path,n_epochs, monitor, k_folds):\n",
        "  print(simulation_path)\n",
        "  # Results storage\n",
        "  results_kfold = []\n",
        "  results_kfold_errorbar=[]\n",
        "  # Loop through different dataset sizes\n",
        "  for size in dataset_sizes:\n",
        "      current_simulation_name=f'_size_{size}'\n",
        "      current_simulation_path=simulation_path+current_simulation_name\n",
        "      print(current_simulation_path)\n",
        "      # Generate dataset\n",
        "      # x, y = get_dataset(size)\n",
        "      (dataset_x, dataset_y)= get_dataset(size)\n",
        "      (x_train, y_train_eloss,y_train_alpha_s,y_train_q_0, \\\n",
        "       x_test, y_test_eloss,y_test_alpha_s,y_test_q_0)=\\\n",
        "        preprocess_dataset(dataset_x, dataset_y)\n",
        "\n",
        "      # Loop through classifiers\n",
        "      for clf_name, clf in classifiers.items():\n",
        "        # Choose the appropriate y_train and y_test based on clf_name\n",
        "        if clf_name == 'ElossNet':\n",
        "            y_train, y_test = y_train_eloss, y_test_eloss\n",
        "        elif clf_name == 'AlphaSNet':\n",
        "            y_train, y_test = y_train_alpha_s, y_test_alpha_s\n",
        "        elif clf_name == 'Q0Net':\n",
        "            y_train, y_test = y_train_q_0, y_test_q_0\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown classifier name: {clf_name}\")\n",
        "        # Evaluate classifier using k-fold cross-validation\n",
        "        fold_accuracies, fold_conf_matrices, fold_train_times, all_histories, plots, models = \\\n",
        "        train_and_evaluate_classifier_kfold(clf, x_train,  y_train,x_test,  y_test, n_epochs, monitor, k_folds,current_simulation_path)\n",
        "\n",
        "        # Store results for each fold\n",
        "        for fold_num, (accuracy, cm,train_time,plot,model) in enumerate(zip(fold_accuracies, fold_conf_matrices,fold_train_times,plots,models), start=1):\n",
        "            results_kfold.append({\n",
        "                'Dataset Size': size,\n",
        "                'Classifier': clf_name,\n",
        "                'Fold Number': fold_num,\n",
        "                'Accuracy': accuracy,\n",
        "                'Confusion Matrix': cm,\n",
        "                'Train Time': train_time,\n",
        "                'Loss/Accuracy Plot Path': plot,\n",
        "                'Best Model Path': model\n",
        "            })\n",
        "            print(f'Average Train Time: {np.mean(fold_train_times)} minutes')\n",
        "            # Calculate mean and standard deviation of accuracy scores\n",
        "            mean_accuracy = np.mean(fold_accuracies)\n",
        "            std_accuracy = np.std(fold_accuracies)\n",
        "\n",
        "            # Store results\n",
        "            results_kfold_errorbar.append({\n",
        "                'Dataset_Size': size,\n",
        "                'Classifier': clf_name,\n",
        "                'Mean_Accuracy': mean_accuracy,\n",
        "                'Std_Accuracy': std_accuracy\n",
        "            })\n",
        "  # Create a DataFrame from k-fold results\n",
        "  df_results_kfold = pd.DataFrame(results_kfold)\n",
        "  # Save the DataFrame to a text file\n",
        "  results_kfold_path=simulation_path+'_results_kfold.txt'\n",
        "  df_results_kfold.to_csv(results_kfold_path, index=False, sep='\\t')\n",
        "  # Display results in a table\n",
        "  print(df_results_kfold)\n",
        "\n",
        "  # Create a DataFrame from k-fold results\n",
        "  df_results_kfold_errorbar = pd.DataFrame(results_kfold_errorbar)\n",
        "  # Save the DataFrame to a text file\n",
        "  results_kfold_errorbar_path=simulation_path+'_results_kfold_errorbar.txt'\n",
        "  df_results_kfold_errorbar.to_csv(results_kfold_errorbar_path, index=False, sep='\\t')\n",
        "  # Display results in a table\n",
        "  print(df_results_kfold_errorbar)\n",
        "  plot_save_mean_error_bar(df_results_kfold_errorbar,simulation_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "hXGQAvpJtN8t"
      },
      "outputs": [],
      "source": [
        "# Classifiers 'ElossNet','AlphaSNet','Q0Net'\n",
        "classifiers = {\n",
        "    # 'ElossNet': eloss_net,\n",
        "    'AlphaSNet':alpha_s_net,\n",
        "    # 'Q0Net':q_0_net\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XHqJCbwcA1oq",
        "outputId": "49434047-9bc5-4c28-b908-bea6cf73dcad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "G:\\My Drive\\Projects\\110_JetscapeMl\\hm.jetscapeml.data\\simulation_results\\\n",
            "G:\\My Drive\\Projects\\110_JetscapeMl\\hm.jetscapeml.data\\simulation_results\\jetml_classification_alpha_s_MMAT_MLBT\n",
            "G:\\My Drive\\Projects\\110_JetscapeMl\\hm.jetscapeml.data\\simulation_results\\jetml_classification_alpha_s_MMAT_MLBT_size_1000\n",
            "dataset_file_name: G:\\My Drive\\Projects\\110_JetscapeMl\\hm.jetscapeml.data\\simulation_results\\jet_ml_benchmark_config_01_to_09_alpha_0.2_0.3_0.4_q0_1.5_2.0_2.5_MMAT_MLBT_size_1000_shuffled.pkl\n",
            "dataset.x: <class 'numpy.ndarray'> 1024000 (1000, 32, 32)\n",
            "dataset.y: <class 'numpy.ndarray'> 3000 (1000, 3)\n",
            "Extract the first column for binary classification\n",
            "dataset.x: <class 'numpy.ndarray'> 1024000 (1000, 32, 32)\n",
            "dataset.y: <class 'numpy.ndarray'> 3000 (1000, 3)\n",
            "Pre-processing\n",
            "deleting the original dataset after splitting ...\n",
            "x_train: <class 'numpy.ndarray'> 819200 (800, 32, 32)\n",
            "y_train: <class 'numpy.ndarray'> 2400 (800, 3)\n",
            "x_test: <class 'numpy.ndarray'> 204800 (200, 32, 32)\n",
            "y_test: <class 'numpy.ndarray'> 600 (200, 3)\n",
            "Preprocess y_train and y_test\n",
            "y_train[:,0]: <class 'numpy.ndarray'> 800 (800,)\n",
            "['MLBT' 'MLBT' 'MMAT' 'MLBT']\n",
            "[[1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]]\n",
            "y_train[:,1]: <class 'numpy.ndarray'> 800 (800,)\n",
            "['0.4' '0.4' '0.3' '0.2']\n",
            "[[0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]]\n",
            "y_train[:,2]: <class 'numpy.ndarray'> 800 (800,)\n",
            "['0.4' '0.4' '0.3' '0.2']\n",
            "[[0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]]\n"
          ]
        },
        {
          "ename": "InvalidArgumentError",
          "evalue": "Graph execution error:\n\nDetected at node sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits defined at (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n\n  File \"<frozen runpy>\", line 88, in _run_code\n\n  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n\n  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\traitlets\\config\\application.py\", line 992, in launch_instance\n\n  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 736, in start\n\n  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 195, in start\n\n  File \"c:\\ProgramData\\anaconda3\\Lib\\asyncio\\base_events.py\", line 607, in run_forever\n\n  File \"c:\\ProgramData\\anaconda3\\Lib\\asyncio\\base_events.py\", line 1922, in _run_once\n\n  File \"c:\\ProgramData\\anaconda3\\Lib\\asyncio\\events.py\", line 80, in _run\n\n  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 516, in dispatch_queue\n\n  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 505, in process_one\n\n  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 412, in dispatch_shell\n\n  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 740, in execute_request\n\n  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 422, in do_execute\n\n  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 546, in run_cell\n\n  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3024, in run_cell\n\n  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3079, in _run_cell\n\n  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n\n  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3284, in run_cell_async\n\n  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3466, in run_ast_nodes\n\n  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3526, in run_code\n\n  File \"C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_12616\\2500325420.py\", line 17, in <module>\n\n  File \"C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_12616\\264592449.py\", line 31, in train_and_evaluate_classifier_for_all_datasets\n\n  File \"C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_12616\\4126548032.py\", line 37, in train_and_evaluate_classifier_kfold\n\n  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n\n  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1807, in fit\n\n  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1401, in train_function\n\n  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1384, in step_function\n\n  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1373, in run_step\n\n  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1151, in train_step\n\n  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1209, in compute_loss\n\n  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\compile_utils.py\", line 277, in __call__\n\n  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py\", line 143, in __call__\n\n  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py\", line 270, in call\n\n  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py\", line 2454, in sparse_categorical_crossentropy\n\n  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py\", line 5775, in sparse_categorical_crossentropy\n\nlabels must be 1-D, but got shape [32,3]\n\t [[{{node sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits}}]] [Op:__inference_train_function_9352]",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[91], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(simulation_directory_path)\n\u001b[0;32m     16\u001b[0m simulation_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msimulation_directory_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mjetml_classification_alpha_s_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_labels_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 17\u001b[0m train_and_evaluate_classifier_for_all_datasets(dataset_sizes,classifiers,simulation_path,n_epochs, monitor, k_folds)\n",
            "Cell \u001b[1;32mIn[90], line 31\u001b[0m, in \u001b[0;36mtrain_and_evaluate_classifier_for_all_datasets\u001b[1;34m(dataset_sizes, classifiers, simulation_path, n_epochs, monitor, k_folds)\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown classifier name: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclf_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Evaluate classifier using k-fold cross-validation\u001b[39;00m\n\u001b[0;32m     30\u001b[0m fold_accuracies, fold_conf_matrices, fold_train_times, all_histories, plots, models \u001b[38;5;241m=\u001b[39m \\\n\u001b[1;32m---> 31\u001b[0m train_and_evaluate_classifier_kfold(clf, x_train,  y_train,x_test,  y_test, n_epochs, monitor, k_folds,current_simulation_path)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Store results for each fold\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m fold_num, (accuracy, cm,train_time,plot,model) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(fold_accuracies, fold_conf_matrices,fold_train_times,plots,models), start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n",
            "Cell \u001b[1;32mIn[19], line 37\u001b[0m, in \u001b[0;36mtrain_and_evaluate_classifier_kfold\u001b[1;34m(model, x_train, y_train, x_test, y_test, n_epochs, monitor, k_folds, simulation_path)\u001b[0m\n\u001b[0;32m     34\u001b[0m history_callback \u001b[38;5;241m=\u001b[39m History()\n\u001b[0;32m     36\u001b[0m start \u001b[38;5;241m=\u001b[39m time()\n\u001b[1;32m---> 37\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[0;32m     38\u001b[0m     x_train_fold,\n\u001b[0;32m     39\u001b[0m     y_train_fold,\n\u001b[0;32m     40\u001b[0m     epochs\u001b[38;5;241m=\u001b[39mn_epochs,\n\u001b[0;32m     41\u001b[0m     validation_data\u001b[38;5;241m=\u001b[39m(x_val_fold, y_val_fold),\n\u001b[0;32m     42\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m[checkpoint_callback]\n\u001b[0;32m     43\u001b[0m )\n\u001b[0;32m     44\u001b[0m train_time \u001b[38;5;241m=\u001b[39m (time()\u001b[38;5;241m-\u001b[39mstart)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m60.0\u001b[39m\n\u001b[0;32m     45\u001b[0m train_times\u001b[38;5;241m.\u001b[39mappend(train_time)\n",
            "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
            "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[1;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits defined at (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n\n  File \"<frozen runpy>\", line 88, in _run_code\n\n  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n\n  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\traitlets\\config\\application.py\", line 992, in launch_instance\n\n  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 736, in start\n\n  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 195, in start\n\n  File \"c:\\ProgramData\\anaconda3\\Lib\\asyncio\\base_events.py\", line 607, in run_forever\n\n  File \"c:\\ProgramData\\anaconda3\\Lib\\asyncio\\base_events.py\", line 1922, in _run_once\n\n  File \"c:\\ProgramData\\anaconda3\\Lib\\asyncio\\events.py\", line 80, in _run\n\n  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 516, in dispatch_queue\n\n  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 505, in process_one\n\n  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 412, in dispatch_shell\n\n  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 740, in execute_request\n\n  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 422, in do_execute\n\n  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 546, in run_cell\n\n  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3024, in run_cell\n\n  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3079, in _run_cell\n\n  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n\n  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3284, in run_cell_async\n\n  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3466, in run_ast_nodes\n\n  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3526, in run_code\n\n  File \"C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_12616\\2500325420.py\", line 17, in <module>\n\n  File \"C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_12616\\264592449.py\", line 31, in train_and_evaluate_classifier_for_all_datasets\n\n  File \"C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_12616\\4126548032.py\", line 37, in train_and_evaluate_classifier_kfold\n\n  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n\n  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1807, in fit\n\n  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1401, in train_function\n\n  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1384, in step_function\n\n  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1373, in run_step\n\n  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1151, in train_step\n\n  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1209, in compute_loss\n\n  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\compile_utils.py\", line 277, in __call__\n\n  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py\", line 143, in __call__\n\n  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py\", line 270, in call\n\n  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py\", line 2454, in sparse_categorical_crossentropy\n\n  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py\", line 5775, in sparse_categorical_crossentropy\n\nlabels must be 1-D, but got shape [32,3]\n\t [[{{node sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits}}]] [Op:__inference_train_function_9352]"
          ]
        }
      ],
      "source": [
        "monitor = 'val_accuracy'  # 'val_accuracy' or 'val_loss'\n",
        "n_epochs = 1\n",
        "k_folds = 10  # You can adjust the number of folds\n",
        "\n",
        "# defining dataset sizes and classifiers\n",
        "\n",
        "# Sizes of datasets\n",
        "dataset_sizes = [1000]\n",
        "# dataset_sizes = [100000]\n",
        "# dataset_sizes = [1000000]\n",
        "# dataset_sizes = [1000, 10000]\n",
        "#dataset_sizes = [1000, 10000,100000]\n",
        "# dataset_sizes = [1000, 10000, 100000, 1000000]\n",
        "\n",
        "print(simulation_directory_path)\n",
        "simulation_path=f'{simulation_directory_path}jetml_classification_alpha_s_{class_labels_str}'\n",
        "train_and_evaluate_classifier_for_all_datasets(dataset_sizes,classifiers,simulation_path,n_epochs, monitor, k_folds)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
