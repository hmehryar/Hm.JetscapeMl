{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1,'/wsu/home/gy/gy40/gy4065/hm.jetscapeml.source')\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Loading/Installing Package => Begin\\n\\n')\n",
    "# from jet_ml_dataset_builder_utilities import install\n",
    "# #reading/writing into files\n",
    "# # !pip3 install pickle5\n",
    "# install(\"pickle5\")\n",
    "import jet_ml_dataset_builder.jet_ml_dataset_builder_utilities as util\n",
    "\n",
    "print('\\n########################################################################')\n",
    "print('Checking the running platforms\\n')\n",
    "\n",
    "from jet_ml_dataset_builder.jet_ml_dataset_builder_utilities import set_directory_paths\n",
    "# Call the function and retrieve the dataset_directory_path and simulation_directory_path\n",
    "dataset_directory_path, simulation_directory_path = set_directory_paths()\n",
    "\n",
    "# Access the dataset_directory_path and simulation_directory_path\n",
    "print(\"Dataset Directory Path:\", dataset_directory_path)\n",
    "print(\"Simulation Directory Path:\", simulation_directory_path)\n",
    "print('########################################################################\\n')\n",
    "\n",
    "\n",
    "print('\\nLoading/Installing Package => End\\n\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jet_ml_dataset_builder.jet_ml_dataset_builder_utilities import  parse_parameters\n",
    "\n",
    "# Call the function and retrieve the tokenized parameters\n",
    "tokenized_arguments, tokenized_values = parse_parameters()\n",
    "\n",
    "# Access the tokenized arguments and values\n",
    "print(\"Tokenized Arguments:\")\n",
    "for argument in tokenized_arguments:\n",
    "    print(argument)\n",
    "\n",
    "print(\"\\nTokenized Values:\")\n",
    "for argument, value in tokenized_values.items():\n",
    "    print(f\"{argument}: {value}\")\n",
    "\n",
    "y_class_label_items=['MMAT','MLBT']\n",
    "alpha_s_items=[0.2 ,0.3 ,0.4]\n",
    "q0_items=[1.5 ,2.0 ,2.5]\n",
    "\n",
    "print(\"y_class_label_items:\",y_class_label_items)\n",
    "print(\"alpha_s_items:\",alpha_s_items)\n",
    "print(\"q0_items:\",q0_items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Building required params for the loading the dataset file\")\n",
    "\n",
    "class_labels_str = '_'.join(y_class_label_items)\n",
    "alpha_s_items_str='_'.join(map(str, alpha_s_items))\n",
    "q0_items_str='_'.join(map(str, q0_items))\n",
    "total_size=9*1200000\n",
    "dataset_file_name = f\"jet_ml_benchmark_config_01_to_09_alpha_{alpha_s_items_str}_q0_{q0_items_str}_{class_labels_str}_size_{total_size}_shuffled.pkl\"\n",
    "dataset_file_name=simulation_directory_path+dataset_file_name\n",
    "print(\"dataset_file_name:\",dataset_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from jet_ml_dataset_builder.jet_ml_dataset_builder_utilities import load_dataset\n",
    "# dataset=load_dataset (dataset_file_name)\n",
    "# ((x_train,y_train),(x_test,y_test))=dataset\n",
    "# print(\"dataset y_train values:\\n\", y_train[1:100])\n",
    "# print(\"dataset y_test values:\\n\", y_test[1:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing y Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "\n",
    "# # Assuming you have the dataset stored in variables: x_train, x_test, y_train, y_test\n",
    "\n",
    "# # Preprocess y_train and y_test\n",
    "# # One-hot encode the categorical variable\n",
    "# y_train_categorical = np.array(y_train[:, 0]).reshape(-1, 1)\n",
    "# y_test_categorical = np.array(y_test[:, 0]).reshape(-1, 1)\n",
    "\n",
    "# encoder = OneHotEncoder(sparse=False)\n",
    "# y_train_categorical_encoded = encoder.fit_transform(y_train_categorical)\n",
    "# y_test_categorical_encoded = encoder.transform(y_test_categorical)\n",
    "\n",
    "\n",
    "# # Standardize the numerical variables\n",
    "# scaler = StandardScaler()\n",
    "# y_train_numerical = np.array(y_train[:, 1:])\n",
    "# y_test_numerical = np.array(y_test[:, 1:])\n",
    "\n",
    "# y_train_numerical_scaled = scaler.fit_transform(y_train_numerical)\n",
    "# y_test_numerical_scaled = scaler.transform(y_test_numerical)\n",
    "\n",
    "# # Combine the encoded categorical and scaled numerical columns\n",
    "# y_train_processed = np.hstack((y_train_categorical_encoded, y_train_numerical_scaled))\n",
    "# y_test_processed = np.hstack((y_test_categorical_encoded, y_test_numerical_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y processed dataset file name\n",
    "dataset_file_name = f\"jet_ml_benchmark_config_01_to_09_alpha_{alpha_s_items_str}_q0_{q0_items_str}_{class_labels_str}_size_{total_size}_shuffled_y_processed.pkl\"\n",
    "dataset_file_name=simulation_directory_path+dataset_file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print (\"Saving the processed dataset\")\n",
    "# from jet_ml_dataset_builder.jet_ml_dataset_builder_utilities import save_dataset\n",
    "\n",
    "# dataset_processed=((x_train,y_train_processed),(x_test,y_test_processed))\n",
    "# save_dataset(dataset_file_name,dataset_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print (\"Loading the processed dataset\")\n",
    "from jet_ml_dataset_builder.jet_ml_dataset_builder_utilities import load_dataset\n",
    "# dataset=load_dataset(dataset_file_name)\n",
    "# ((x_train,y_train_processed),(x_test,y_test_processed))=dataset\n",
    "((x_train,y_train_processed),(x_test,y_test_processed)) = load_dataset(dataset_file_name)\n",
    "print (\"Loaded the processed dataset\")\n",
    "print(\"Now that you're done with dataset , release memory\")\n",
    "# del dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from jet_ml_dataset_builder.jet_ml_dataset_builder_utilities import save_dataset\n",
    "# Assuming you have the training dataset as x_train_resized and y_train_processed\n",
    "# Determine the number of pieces you want to split the dataset into\n",
    "def split_dataset_into_pieces_and_store():\n",
    "    num_pieces = 64\n",
    "    batch_size = len(x_train) // num_pieces\n",
    "\n",
    "    # Create a directory to store the split datasets\n",
    "    output_directory = f\"jet_ml_benchmark_config_01_to_09_alpha_{alpha_s_items_str}_q0_{q0_items_str}_{class_labels_str}_size_{total_size}_split_train_datasets/\"\n",
    "    output_directory=simulation_directory_path+output_directory\n",
    "\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "    # Split the dataset and save each piece\n",
    "    for i in range(num_pieces):\n",
    "        start = i * batch_size\n",
    "        end = (i + 1) * batch_size if i < num_pieces - 1 else len(x_train)\n",
    "        x_split = x_train[start:end]\n",
    "        y_split = y_train_processed[start:end]\n",
    "\n",
    "        # Save the split dataset\n",
    "        print (f'Saving {output_directory}train_split_{i}.pkl')\n",
    "        save_dataset(f'{output_directory}train_split_{i}.pkl',(x_split,y_split))\n",
    "\n",
    "    print(\"Training dataset split into 64 pieces and saved.\")\n",
    "exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split the data into training and validation sets\n",
    "x_train_processed, x_val_processed, y_train_processed, y_val_processed = train_test_split(\n",
    "    x_train, y_train_processed, test_size=0.1, random_state=42\n",
    ")\n",
    "# Print the shapes of processed data for verification\n",
    "print(\"Shape of x_train_processed:\", x_train_processed.shape)\n",
    "print(\"Shape of y_train_processed:\", y_train_processed.shape)\n",
    "print(\"Shape of x_val_processed:\", x_val_processed.shape)\n",
    "print(\"Shape of y_val_processed:\", y_val_processed.shape)\n",
    "print(\"Shape of x_test:\", x_test.shape)\n",
    "print(\"Shape of y_test_processed:\", y_test_processed.shape)\n",
    "\n",
    "print(\"Now that you're done with x_train , release memory\")\n",
    "del x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # a parallel implementation\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.image import resize\n",
    "\n",
    "# # Assuming you have the dataset stored in variables: x_train_processed, x_val_processed, x_test\n",
    "\n",
    "# # Convert the NumPy arrays to TensorFlow Tensors\n",
    "# x_train_processed = tf.convert_to_tensor(x_train_processed, dtype=tf.float32)\n",
    "# x_val_processed = tf.convert_to_tensor(x_val_processed, dtype=tf.float32)\n",
    "# x_test = tf.convert_to_tensor(x_test, dtype=tf.float32)\n",
    "\n",
    "# # Resize training set\n",
    "# print(\"Resize training set\")\n",
    "# x_train_resized = resize(x_train_processed, (224, 224), method='bilinear')\n",
    "# x_train_resized = tf.image.grayscale_to_rgb(x_train_resized)  # Ensure it has 3 channels\n",
    "# x_train_resized = x_train_resized.numpy()  # Convert back to NumPy array\n",
    "\n",
    "# # Resize validation set\n",
    "# print(\"Resize validation set\")\n",
    "# x_val_resized = resize(x_val_processed, (224, 224), method='bilinear')\n",
    "# x_val_resized = tf.image.grayscale_to_rgb(x_val_resized)  # Ensure it has 3 channels\n",
    "# x_val_resized = x_val_resized.numpy()  # Convert back to NumPy array\n",
    "\n",
    "# # Resize test set\n",
    "# print(\"Resize test set\")\n",
    "# x_test_resized = resize(x_test, (224, 224), method='bilinear')\n",
    "# x_test_resized = tf.image.grayscale_to_rgb(x_test_resized)  # Ensure it has 3 channels\n",
    "# x_test_resized = x_test_resized.numpy()  # Convert back to NumPy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from jet_ml_dataset_builder.jet_ml_dataset_builder_utilities import save_dataset\n",
    "# print(\"Saving dataset processes and resized\")\n",
    "# dataset_processed_resized=((x_train_resized,y_train_processed),(x_val_resized,y_val_processed),(x_test_resized,y_test_processed))\n",
    "# save_dataset(dataset_file_name,dataset_processed_resized)\n",
    "# print(\"Saved dataset processes and resized\")\n",
    "# del dataset_processed_resized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "To build a deep neural model based on the VGG16 architecture for predicting the target parameters in the \"y\" side of your dataset, you can use transfer learning. Transfer learning involves using pre-trained models and fine-tuning them for your specific task.\n",
    "\n",
    "In this case, we will use the pre-trained VGG16 model, remove its top layers (which are specific to the original classification task), and add new layers for our multi-task prediction. Since you have three target parameters in the \"y\" side, we will create three output layers, each predicting one of the target parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from keras.applications import VGG16\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Replace these with your actual preprocessed dataset\n",
    "# x_train_processed \n",
    "# x_val_processed \n",
    "# y_train_processed \n",
    "# y_val_processed \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cnn_model(input_shape,lr):\n",
    "    base_model = VGG16(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    predictions = Dense(9, activation='softmax')(x)  # Assuming 9 total categories (2 for \"MMAT\", \"MLBT\" + 4 for \"q0\" + 3 for \"alpha_s\")\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "    optimizer = Adam(learning_rate=lr)\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path, makedirs\n",
    "save_dir = simulation_directory_path+'simulation_result_vgg16_synthesis_10800k'\n",
    "if not path.exists(save_dir):\n",
    "    makedirs(save_dir)\n",
    "print('Directory to save models: {}'.format(save_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor='val_accuracy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "def get_callbacks(monitor, save_dir):\n",
    "    mode = None\n",
    "    if 'loss' in monitor:\n",
    "        mode = 'min'\n",
    "    elif 'accuracy' in monitor:\n",
    "        mode = 'max'\n",
    "    assert mode != None, 'Check the monitor parameter!'\n",
    "\n",
    "    # es = EarlyStopping(monitor=monitor, mode=mode, patience=10,\n",
    "    #                   min_delta=0., verbose=1)\n",
    "    es = EarlyStopping(monitor=monitor, mode=mode, patience=3, restore_best_weights=True)\n",
    "    # rlp = ReduceLROnPlateau(monitor=monitor, mode=mode, factor=0.2, patience=5,\n",
    "    #                         min_lr=0.001, verbose=1)\n",
    "    mcp = ModelCheckpoint(path.join(save_dir, 'hm_jetscape_ml_model_best.h5'), monitor=monitor, \n",
    "                          save_best_only=True, mode=mode, verbose=1)\n",
    "    \n",
    "    # return [es, rlp, mcp]\n",
    "    return [es, mcp]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# early_stopping = EarlyStopping(patience=3, restore_best_weights=True)\n",
    "import tensorflow as tf\n",
    "callbacks = get_callbacks(monitor, save_dir)\n",
    "n_epochs=30\n",
    "batch_size=32\n",
    "lr=0.001\n",
    "input_shape=(32, 32, 3)\n",
    "\n",
    "from time import time\n",
    "def train_network(train_set, val_set, n_epochs, batch_size, monitor):\n",
    "    tf.keras.backend.clear_session()\n",
    "    model = cnn_model(input_shape, lr)\n",
    "    callbacks = get_callbacks(monitor, save_dir)\n",
    "\n",
    "    model.summary()\n",
    "    \n",
    "    start = time()\n",
    "\n",
    "    history = model.fit(\n",
    "        train_set[0], train_set[1],\n",
    "        batch_size=batch_size,\n",
    "        epochs=n_epochs,\n",
    "        validation_data=val_set,\n",
    "        callbacks=callbacks\n",
    "        )\n",
    "    train_time = (time()-start)/60.\n",
    "    return history, train_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, val_set = (x_train_resized, y_train_processed), (x_val_resized, y_val_processed)\n",
    "\n",
    "history, train_time = train_network(train_set, val_set, n_epochs, lr, batch_size, monitor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_name='hm_jetscape_ml_model_history.csv'\n",
    "file_path=save_dir+file_name\n",
    "pd.DataFrame.from_dict(history.history).to_csv(file_path,index=False)\n",
    "\n",
    "\n",
    "file_name='hm_jetscape_ml_model_history.npy'\n",
    "file_path=save_dir+file_name\n",
    "np.save(file_path,history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Replace these with your actual test set\n",
    "x_test_resized\n",
    "y_test_processed \n",
    "\n",
    "test_loss, test_accuracy = model.evaluate(x_test_resized, y_test_processed)\n",
    "print(\"Test Loss:\", test_loss)\n",
    "print(\"Test Accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# # Plot the loss and accuracy curves\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# plt.subplot(1, 2, 1)\n",
    "# plt.plot(history.history['loss'], label='Training Loss')\n",
    "# plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.legend()\n",
    "\n",
    "# plt.subplot(1, 2, 2)\n",
    "# plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "# plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.legend()\n",
    "\n",
    "# plt.tight_layout()\n",
    "\n",
    "# # Save the plot as an image file\n",
    "# plt.savefig('loss_accuracy_plot.png')\n",
    "\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
