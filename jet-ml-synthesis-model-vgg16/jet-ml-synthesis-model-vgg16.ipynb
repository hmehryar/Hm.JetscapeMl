{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "\n",
    "sys.path.insert(1,'/wsu/home/gy/gy40/gy4065/hm.jetscapeml.source')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Loading/Installing Package => Begin\\n\\n')\n",
    "# from jet_ml_dataset_builder_utilities import install\n",
    "# #reading/writing into files\n",
    "# # !pip3 install pickle5\n",
    "# install(\"pickle5\")\n",
    "import jet_ml_dataset_builder.jet_ml_dataset_builder_utilities as util\n",
    "\n",
    "print('\\n########################################################################')\n",
    "print('Checking the running platforms\\n')\n",
    "\n",
    "from jet_ml_dataset_builder.jet_ml_dataset_builder_utilities import set_directory_paths\n",
    "# Call the function and retrieve the dataset_directory_path and simulation_directory_path\n",
    "dataset_directory_path, simulation_directory_path = set_directory_paths()\n",
    "\n",
    "# Access the dataset_directory_path and simulation_directory_path\n",
    "print(\"Dataset Directory Path:\", dataset_directory_path)\n",
    "print(\"Simulation Directory Path:\", simulation_directory_path)\n",
    "print('########################################################################\\n')\n",
    "\n",
    "\n",
    "print('\\nLoading/Installing Package => End\\n\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jet_ml_dataset_builder.jet_ml_dataset_builder_utilities import  parse_parameters\n",
    "\n",
    "# Call the function and retrieve the tokenized parameters\n",
    "tokenized_arguments, tokenized_values = parse_parameters()\n",
    "\n",
    "# Access the tokenized arguments and values\n",
    "print(\"Tokenized Arguments:\")\n",
    "for argument in tokenized_arguments:\n",
    "    print(argument)\n",
    "\n",
    "print(\"\\nTokenized Values:\")\n",
    "for argument, value in tokenized_values.items():\n",
    "    print(f\"{argument}: {value}\")\n",
    "\n",
    "y_class_label_items=['MMAT','MLBT']\n",
    "alpha_s_items=[0.2 ,0.3 ,0.4]\n",
    "q0_items=[1.5 ,2.0 ,2.5]\n",
    "\n",
    "print(\"y_class_label_items:\",y_class_label_items)\n",
    "print(\"alpha_s_items:\",alpha_s_items)\n",
    "print(\"q0_items:\",q0_items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Building required params for the loading the dataset file\")\n",
    "\n",
    "class_labels_str = '_'.join(y_class_label_items)\n",
    "alpha_s_items_str='_'.join(map(str, alpha_s_items))\n",
    "q0_items_str='_'.join(map(str, q0_items))\n",
    "total_size=9*1200000\n",
    "dataset_file_name = f\"jet_ml_benchmark_config_01_to_09_alpha_{alpha_s_items_str}_q0_{q0_items_str}_{class_labels_str}_size_{total_size}_shuffled.pkl\"\n",
    "dataset_file_name=simulation_directory_path+dataset_file_name\n",
    "print(\"dataset_file_name:\",dataset_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jet_ml_dataset_builder.jet_ml_dataset_builder_utilities import load_dataset\n",
    "dataset=load_dataset (dataset_file_name)\n",
    "((x_train,y_train),(x_test,y_test))=dataset\n",
    "print(\"dataset y_train values:\\n\", y_train[1:100])\n",
    "print(\"dataset y_test values:\\n\", y_test[1:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing y Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # Assuming you have the dataset stored in variables: x_train, x_test, y_train, y_test\n",
    "\n",
    "# # Preprocess y_train and y_test\n",
    "# # One-hot encode the categorical variable\n",
    "# y_train_categorical = np.array(y_train[:, 0]).reshape(-1, 1)\n",
    "# y_test_categorical = np.array(y_test[:, 0]).reshape(-1, 1)\n",
    "\n",
    "# encoder = OneHotEncoder(sparse=False)\n",
    "# y_train_categorical_encoded = encoder.fit_transform(y_train_categorical)\n",
    "# y_test_categorical_encoded = encoder.transform(y_test_categorical)\n",
    "\n",
    "\n",
    "# # Standardize the numerical variables\n",
    "# scaler = StandardScaler()\n",
    "# y_train_numerical = np.array(y_train[:, 1:])\n",
    "# y_test_numerical = np.array(y_test[:, 1:])\n",
    "\n",
    "# y_train_numerical_scaled = scaler.fit_transform(y_train_numerical)\n",
    "# y_test_numerical_scaled = scaler.transform(y_test_numerical)\n",
    "\n",
    "# # Combine the encoded categorical and scaled numerical columns\n",
    "# y_train_processed = np.hstack((y_train_categorical_encoded, y_train_numerical_scaled))\n",
    "# y_test_processed = np.hstack((y_test_categorical_encoded, y_test_numerical_scaled))\n",
    "\n",
    "# print (\"Saving the processed dataset\")\n",
    "# from jet_ml_dataset_builder.jet_ml_dataset_builder_utilities import save_dataset\n",
    "# dataset_file_name = f\"jet_ml_benchmark_config_01_to_09_alpha_{alpha_s_items_str}_q0_{q0_items_str}_{class_labels_str}_size_{total_size}_shuffled_y_processed.pkl\"\n",
    "# dataset_file_name=simulation_directory_path+dataset_file_name\n",
    "# dataset_processed=((x_train,y_train_processed),(x_test,y_test_processed))\n",
    "# save_dataset(dataset_file_name,dataset_processed)\n",
    "\n",
    "\n",
    "# # Split the data into training and validation sets\n",
    "# x_train_processed, x_val_processed, y_train_processed, y_val_processed = train_test_split(\n",
    "#     x_train, y_train_processed, test_size=0.1, random_state=42\n",
    "# )\n",
    "\n",
    "# # Print the shapes of processed data for verification\n",
    "# print(\"Shape of x_train_processed:\", x_train_processed.shape)\n",
    "# print(\"Shape of x_val_processed:\", x_val_processed.shape)\n",
    "# print(\"Shape of y_train_processed:\", y_train_processed.shape)\n",
    "# print(\"Shape of y_val_processed:\", y_val_processed.shape)\n",
    "# print(\"Shape of x_test:\", x_test.shape)\n",
    "# print(\"Shape of y_test_processed:\", y_test_processed.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from skimage.transform import resize\n",
    "\n",
    "# # Assuming you have the dataset stored in variables: x_train_processed, x_val_processed, x_test\n",
    "\n",
    "# # Resize training set\n",
    "# x_train_resized = np.array([resize(img, (224, 224, 3)) for img in x_train_processed])\n",
    "# # x_train_processed=x_train_resized\n",
    "# # Resize validation set\n",
    "# x_val_resized = np.array([resize(img, (224, 224, 3)) for img in x_val_processed])\n",
    "# # x_val_processed=x_val_resized\n",
    "# # Resize test set\n",
    "# x_test_resized = np.array([resize(img, (224, 224, 3)) for img in x_test])\n",
    "# # x_test=x_test_resized\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "To build a deep neural model based on the VGG16 architecture for predicting the target parameters in the \"y\" side of your dataset, you can use transfer learning. Transfer learning involves using pre-trained models and fine-tuning them for your specific task.\n",
    "\n",
    "In this case, we will use the pre-trained VGG16 model, remove its top layers (which are specific to the original classification task), and add new layers for our multi-task prediction. Since you have three target parameters in the \"y\" side, we will create three output layers, each predicting one of the target parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import numpy as np\n",
    "# from keras.applications import VGG16\n",
    "# from keras.models import Model\n",
    "# from keras.layers import Dense, GlobalAveragePooling2D\n",
    "# from keras.optimizers import Adam\n",
    "# from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Replace these with your actual preprocessed dataset\n",
    "# x_train_processed \n",
    "# x_val_processed \n",
    "# y_train_processed \n",
    "# y_val_processed \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_model = VGG16(weights='imagenet', include_top=False, input_shape=(32, 32, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# x = base_model.output\n",
    "# x = GlobalAveragePooling2D()(x)\n",
    "# x = Dense(512, activation='relu')(x)\n",
    "# predictions = Dense(9, activation='softmax')(x)  # Assuming 9 total categories (2 for \"MMAT\", \"MLBT\" + 3 for \"q0\" + 4 for \"alpha_s\")\n",
    "# model = Model(inputs=base_model.input, outputs=predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(optimizer=Adam(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# early_stopping = EarlyStopping(patience=3, restore_best_weights=True)\n",
    "\n",
    "# history = model.fit(\n",
    "#     x_train_resized, y_train_processed,\n",
    "#     batch_size=32,\n",
    "#     epochs=20,\n",
    "#     validation_data=(x_val_resized, y_val_processed),\n",
    "#     callbacks=[early_stopping]\n",
    "# )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Replace these with your actual test set\n",
    "# x_test_resized\n",
    "# y_test_processed \n",
    "\n",
    "# test_loss, test_accuracy = model.evaluate(x_test_resized, y_test_processed)\n",
    "# print(\"Test Loss:\", test_loss)\n",
    "# print(\"Test Accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# # Plot the loss and accuracy curves\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# plt.subplot(1, 2, 1)\n",
    "# plt.plot(history.history['loss'], label='Training Loss')\n",
    "# plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.legend()\n",
    "\n",
    "# plt.subplot(1, 2, 2)\n",
    "# plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "# plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.legend()\n",
    "\n",
    "# plt.tight_layout()\n",
    "\n",
    "# # Save the plot as an image file\n",
    "# plt.savefig('loss_accuracy_plot.png')\n",
    "\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
