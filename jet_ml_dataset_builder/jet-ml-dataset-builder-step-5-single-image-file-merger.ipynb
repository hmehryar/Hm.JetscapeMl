{"cells":[{"cell_type":"markdown","id":"hcsKVBPS1Ruj","metadata":{"id":"hcsKVBPS1Ruj"},"source":["\n","## Part 0: Prerequisites:\n","\n","\n"]},{"cell_type":"code","execution_count":null,"id":"34L2uh7E1P75","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":37057,"status":"ok","timestamp":1651011075958,"user":{"displayName":"Haydar Mehryar","userId":"09990703679773155769"},"user_tz":240},"id":"34L2uh7E1P75","outputId":"6ca0e65b-7fce-407d-d5bc-96b8723d92d9"},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mFailed to start the Kernel. \n","Unable to start Kernel 'tensorflow_gpuenv_v2 (Python 3.7.7)' due to connection timeout. \n","View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."]}],"source":["import sys\n","# sys.stdout = open(\"output.txt\", \"w\")\n","\n","print('Loading/Installing Package => Begin\\n\\n')\n","# Commonly used modules\n","import numpy as np\n","import os\n","from os import path, makedirs\n","import time\n","from time import time\n","import subprocess\n","\n","\n","def install(package):\n","  print(\"Installing \"+package) \n","  subprocess.check_call([sys.executable,\"-m\" ,\"pip\", \"install\", package])\n","  print(\"Installed \"+package+\"\\n\") \n","\n","\n","\n","# Images, plots, display, and visualization\n","import matplotlib.cm as cm\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","\n","#reading/writing into files\n","# !pip3 install pickle5\n","install(\"pickle5\")\n","import pickle5 as pickle\n","\n","print('\\n########################################################################')\n","print('Checking the running platforms\\n')\n","import platform\n","running_os=platform.system()\n","print(\"OS: \"+running_os)\n","print(\"OS version: \"+platform.release())\n","\n","try:\n","  from google.colab import drive\n","  COLAB = True\n","except:\n","  COLAB = False\n","print(\"running on Colab: \"+str(COLAB))\n","\n","# if 'google.colab' in str(get_ipython()):\n","#   print('Running on CoLab')\n","#   install(\"google.colab\")\n","#   from google.colab import drive\n","#   drive.mount('/content/drive')\n","# else:\n","#   print('Not running on CoLab')\n","\n","\n","print(\"Python version: \"+platform.python_version())\n","\n","\n","dataset_directory_path=''\n","simulation_directory_path=''\n","\n","if COLAB == True:\n","  drive.mount('/content/drive')\n","  dataset_directory_path='/content/drive/MyDrive/Projects/110_JetscapeMl/hm.jetscapeml.data/'\n","  simulation_directory_path=dataset_directory_path+'simulation_results/'\n","elif 'Linux' in running_os:\n","  dataset_directory_path='/wsu/home/gy/gy40/gy4065/hm.jetscapeml.data/'\n","  simulation_directory_path=dataset_directory_path+'simulation_results/'\n","else:\n","  dataset_directory_path= 'G:\\\\My Drive\\\\Projects\\\\110_JetscapeMl\\\\hm.jetscapeml.data\\\\'\n","  simulation_directory_path=dataset_directory_path+'simulation_results\\\\'\n","print('Dataset Directory Path: '+dataset_directory_path)\n","\n","\n","\n","if not path.exists(simulation_directory_path):\n","    makedirs(simulation_directory_path)\n","print('Simulation Results Path: '+simulation_directory_path)\n","print('########################################################################\\n')\n","\n","\n","print('\\nLoading/Installing Package => End\\n\\n')"]},{"cell_type":"code","execution_count":null,"id":"29370d58","metadata":{},"outputs":[],"source":["class DatasetBuilderSingleFileAnalyzer:\n","   # class attribute\n","  \n","    # Instance attribute\n","    def __init__(self, input_file_name_hadrons,data_size,y_class_label_items,output_dataset_file_name):\n","        self.input_file_name_hadrons=x_train\n","        self.data_size=y_train\n","        self.y_class_label_items=x_test\n","        self.output_dataset_file_name=y_test"]},{"cell_type":"markdown","id":"fdc50e81","metadata":{},"source":["## Part 0: Input Params:"]},{"cell_type":"markdown","id":"645e0a0d","metadata":{},"source":["getting inputs parameters from command line"]},{"cell_type":"code","execution_count":null,"id":"e7b3b682","metadata":{},"outputs":[],"source":["print('########################################################################\\n')\n","print(\"Parsing parameters from command line and initializing the input parameters\")\n","# Python program to demonstrate\n","# command line arguments\n"," \n"," \n","import getopt, sys\n"," \n"," \n","# Remove 1st argument from the\n","# list of command line arguments\n","argumentList = sys.argv[1:]\n"," \n","# Options\n","options = \"hi:d:y:o:n:c:p:\"\n"," \n","# Long options\n","long_options = [\"Help\", \"Input_file_name_hadrons\",\"Data_size\",\"Y_class_label_items\",\"output_dataset_file_name=\", \"number_of_partition\",\"configuration_directory\",\"configuration_number\"]\n"," \n","try:\n","    # Parsing argument\n","    arguments, values = getopt.getopt(argumentList, options, long_options)\n","    print(arguments)\n","    print(values)\n","    # checking each argument\n","    for currentArgument, currentValue in arguments:\n","        print(currentArgument)\n","        if currentArgument in (\"-h\", \"--Help\"):\n","            print (\"Displaying Help\")   \n","        elif currentArgument in (\"-i\", \"--Input_file_name_hadrons\"):\n","            print (\"Input_file_name_hadrons: \", currentValue)\n","            file_name_hadrons=currentValue\n","            print('simulated events final state hadron file: '+file_name_hadrons)\n","        elif currentArgument in (\"-d\", \"--Data_size\"):\n","            print (\"Data_size: \", currentValue) \n","            data_size=int(currentValue)\n","            print('data_size: {} '.format(data_size))\n","        elif currentArgument in (\"-y\", \"--Y_class_label_items\"):\n","            print (\"Y_class_label_items: \", currentValue)\n","            y_class_label_items=[currentValue]     \n","            print(\"y_class_label_items\")\n","            print(y_class_label_items)\n","        elif currentArgument in (\"-o\", \"--output_dataset_file_name\"):\n","            print (\"output_dataset_file_name: \",currentValue)\n","            dataset_file_name=currentValue\n","            print(\"Dataset file name: \"+dataset_file_name)\n","        elif currentArgument in (\"-n\", \"--number_of_partition\"):\n","            print (\"number_of_partition: \",currentValue)\n","            number_of_partition=int(currentValue)\n","            print('Number of partition for splitting the events: {} '.format(number_of_partition))\n","        elif currentArgument in (\"-c\", \"--configuration_directory\"):\n","            print (\"configuration_directory: \",currentValue)\n","            configuration_directory=currentValue\n","            print('Configuration directory: ',configuration_directory)\n","        elif currentArgument in (\"-p\", \"--configuration_number\"):\n","            print (\"configuration_number: \",currentValue)\n","            configuration_number=int(currentValue)\n","            print('Configuration number to reference which dataset it is: {} '.format(configuration_number))\n","except getopt.error as err:\n","    # output error, and return with an error code\n","    print (str(err))\n","print('########################################################################\\n')"]},{"cell_type":"code","execution_count":null,"id":"4915eca9","metadata":{},"outputs":[],"source":["def save_event_items_chunk(file_name,event_items_chunks_item):\n","    with open(file_name, 'wb') as dataset_file:\n","        pickle.dump(event_items_chunks_item,dataset_file, protocol=pickle.HIGHEST_PROTOCOL)\n","\n","        \n","def load__event_items_chunk(file_name):\n","    with open(file_name, 'rb') as dataset_file:\n","        event_items = pickle.load(dataset_file, encoding='latin1')\n","        return event_items"]},{"cell_type":"markdown","id":"568a6a65","metadata":{},"source":["Loading Events Image Item Chunck Item from Fies and Merge into one file\n"]},{"cell_type":"code","execution_count":null,"id":"843b7884","metadata":{},"outputs":[],"source":["image_grid_count=32\n","def load_merge_image_items_into_singular_image_file():\n","  print('\\n########################################################################')\n","  start = time()\n","  print(\"Loading Events Image Chunks from File and Merge Into \")\n","  # number_of_partition=20\n","  \n","  number_of_events_per_partition=int( data_size/number_of_partition)\n","  event_items_image_array =np.array(np.zeros((1,image_grid_count,image_grid_count))) \n","  event_items_image = np.array(np.zeros((1,image_grid_count,image_grid_count))) \n","  for partition_index in range(number_of_partition):\n","    print(\"Partition#\",partition_index,\" Loading Partition \",partition_index, \"in the file\")\n","    file_name=\"config-0\"+str(configuration_number)+\"-\"+y_class_label_items[0]+\"-simulationsize\"+str(data_size)+\"-partition\"+str(partition_index)+\"-numofevents\"+str(number_of_events_per_partition)+\"-img-chunk.pkl\"\n","    file_name=simulation_directory_path+file_name\n","    # file_name=y_class_label_items[0]+\"-simulationsize\"+str(data_size)+\"-partition\"+str(partition_index)+\"-numofevents\"+str(number_of_events_per_partition)+\"-img-chunk.pkl\"\n","    # file_name=simulation_directory_path+file_name\n","    event_items_image=load__event_items_chunk(file_name)\n","    print (\"Partition#\",partition_index,\" Loaded: \",file_name)\n","    print(\"Partition#\",partition_index,\" event_items_chunks_item type: \", type(event_items_image))\n","    print(\"Partition#\",partition_index,\" event_items_chunks_item content: \", len(event_items_image))\n","    \n","    print(\"Partition#\",partition_index,\" Before Appending the new file to events array: \", len(event_items_image_array))\n","    if partition_index==0:\n","        event_items_image_array=event_items_image\n","    else:\n","        event_items_image_array=np.insert(event_items_image_array,0,event_items_image,axis=0)\n","    print(\"Partition#\",partition_index,\" After Appending the new file to events array: \", len(event_items_image_array))\n","  else:\n","    print(\"Finally loading events items partitioning is done!\")\n","  print('\\n########################################################################')\n","  print(\"Storing Image Items Chunck into File begins\")\n","  file_name=\"config-0\"+str(configuration_number)+\"-\"+y_class_label_items[0]+\"-simulationsize\"+str(data_size)+\"-img-chunk.pkl\"\n","  file_name=simulation_directory_path+file_name\n","  # file_name=y_class_label_items[0]+\"-simulationsize\"+str(data_size)+\"-img-chunk.pkl\"\n","  # file_name=simulation_directory_path+file_name\n","  print (\"Dataset Filename: \",file_name,)\n","  save_event_items_chunk(file_name,event_items_image_array)\n","  print (\"Dataset Stored at : \",file_name)\n","  print(\"Storing Image Items Chunck into File ends\")\n","  print('\\n########################################################################')\n","  elapsed = time() - start\n","  print('Loading / Merging /Storing event image chunk into Singular Elapsed %.3f seconds.' % elapsed)\n","  print('\\n########################################################################')\n","\n","load_merge_image_items_into_singular_image_file()"]},{"cell_type":"code","execution_count":null,"id":"8ce73da7","metadata":{},"outputs":[],"source":["def save_dataset(file_name,dataset):\n","    with open(file_name, 'wb') as dataset_file:\n","        pickle.dump(dataset,dataset_file, protocol=pickle.HIGHEST_PROTOCOL)\n","\n","        \n","def load_dataset(file_name):\n","    with open(file_name, 'rb') as dataset_file:\n","        (x_train, y_train), (x_test, y_test) = pickle.load(dataset_file, encoding='latin1')\n","        dataset=((x_train, y_train), (x_test, y_test))\n","        return dataset"]},{"cell_type":"code","execution_count":null,"id":"ddb751dd","metadata":{},"outputs":[],"source":["def adding_missing_images():\n","    print('\\n########################################################################')\n","    start = time()\n","    print (\"Loading Lbt dataset\")\n","    file_name=y_class_label_items[0]+\"-simulationsize\"+str(data_size)+\"-img-chunk.pkl\"\n","    file_name=simulation_directory_path+file_name\n","    event_items_image=load__event_items_chunk(file_name)\n","    print (\"Loaded: \",file_name)\n","    print(\"Dataset type: \", type(event_items_image))\n","    print(\"Dataset length: \", len(event_items_image))\n","\n","    print (\"Loading Lbt dataset\")\n","    file_name=\"jetscape-ml-benchmark-dataset-matter-vs-lbt-200k-shuffled-01.pkl\"\n","    file_name=dataset_directory_path+file_name\n","    (x_train, y_train), (x_test, y_test) =load_dataset(file_name)\n","    print (\"Loaded: \",file_name)\n","    print(\"Dataset type: \", type(x_train))\n","    print(\"Dataset length: \", len(x_train))\n","\n","    event_items_image_array =np.array(np.zeros((1,image_grid_count,image_grid_count))) \n","    print(\"Before Appending the new file events to array: \", len(event_items_image_array))\n","    train_index=0\n","    count=0\n","    while count!=9:\n","        if y_train[train_index]==y_class_label_items[0]:\n","            if count==0:\n","                event_items_image_array=[x_train[train_index]]\n","            else:\n","                event_items_image_array=np.insert(event_items_image_array,0,x_train[train_index],axis=0)\n","            count=count+1\n","            print(\"Label \", y_train[train_index])\n","            print(len(event_items_image_array))\n","        train_index=train_index+1          \n","    print(\"After Appending the new events to array: \", len(event_items_image_array))\n","    print(\"After Appending the new events to array: \", event_items_image_array.size)\n","    print(\"After Appending the new events to array: \",type(event_items_image_array)) \n","    print(event_items_image_array)  \n","\n","    print(\"Appending 9 missing events to dataset\") \n","    print (\"Before Appending size:\",len(event_items_image))\n","    event_items_image=np.insert(event_items_image,0,event_items_image_array,axis=0)\n","    print (\"After Appending size:\",len(event_items_image))\n","\n","    print('\\n########################################################################')\n","    print(\"Storing Image Items Chunck into File begins\")\n","    file_name=y_class_label_items[0]+\"-simulationsize\"+str(data_size)+\"-img-chunk1.pkl\"\n","    file_name=simulation_directory_path+file_name\n","    print (\"Dataset Filename: \",file_name,)\n","    save_event_items_chunk(file_name,event_items_image)\n","    print (\"Dataset Stored at : \",file_name)\n","    print(\"Storing Image Items Chunck into File ends\")\n","    print('\\n########################################################################')\n","    \n","    elapsed = time() - start\n","    print('Loading Dataset Elapsed %.3f seconds.' % elapsed)\n","    print('\\n########################################################################')\n","# adding_missing_images()\n"]},{"cell_type":"markdown","id":"d0645573","metadata":{"id":"d0645573"},"source":["**Construncting proportional train and test sets** by events' images"]},{"cell_type":"code","execution_count":null,"id":"10d0f4ae","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":235,"status":"ok","timestamp":1638550370540,"user":{"displayName":"Haydar Mehryar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiV2PbrE4NQbmWZlQz1WDM6jCKRcmLwN1DLu24x5g=s64","userId":"09990703679773155769"},"user_tz":300},"id":"10d0f4ae","outputId":"551439d9-fc7a-4fcd-dd9e-03ec3f24271d"},"outputs":[{"name":"stdout","output_type":"stream","text":["x_train_matter: <class 'numpy.ndarray'> 819200 (800, 32, 32)\n","x_test_matter: <class 'numpy.ndarray'> 204800 (200, 32, 32)\n","x_train_matter_lbt: <class 'numpy.ndarray'> 819200 (800, 32, 32)\n","x_test_matter_lbt: <class 'numpy.ndarray'> 204800 (200, 32, 32)\n"]}],"source":["def get_data_splitting_index(slice_total,slice_train,data_size):\n","    slice_test=slice_total-slice_train\n","    data_splitting_index=int(data_size*(slice_train/slice_total))\n","    return data_splitting_index\n","def get_x_train_test_data_by_proportion(slice_total,slice_train,data):\n","    data_size=len(data)\n","    data_splitting_index=get_data_splitting_index(slice_total,slice_train,data_size)\n","    x_train=data[0:data_splitting_index]\n","    x_test=data[data_splitting_index:data_size]\n","    return (x_train,x_test)\n","# slice_total=5\n","# slice_train=4\n","\n","slice_total=10\n","slice_train=9\n","\n","# print('\\n########################################################################')\n","# print('Construncting proportional train and test sets by events'' images')\n","# (x_train,x_test)=get_x_train_test_data_by_proportion(slice_total,slice_train,event_items_image)\n","# # (x_train_matter,x_test_matter)=get_x_train_test_data_by_proportion(slice_total,slice_train,event_items_image_matter)\n","# # (x_train_matter_lbt,x_test_matter_lbt)=get_x_train_test_data_by_proportion(slice_total,slice_train,event_items_image_matter_lbt)\n","\n","\n","# print(\"x_train:\",type(x_train), x_train.size, x_train.shape)\n","# print(\"x_test:\",type(x_test), x_test.size, x_test.shape)\n","# # print(\"x_train_matter:\",type(x_train_matter), x_train_matter.size, x_train_matter.shape)\n","# # print(\"x_test_matter:\",type(x_test_matter), x_test_matter.size, x_test_matter.shape)\n","# # print(\"x_train_matter_lbt:\",type(x_train_matter_lbt), x_train_matter_lbt.size, x_train_matter_lbt.shape)\n","# # print(\"x_test_matter_lbt:\",type(x_test_matter_lbt), x_test_matter_lbt.size, x_test_matter_lbt.shape)\n","# print('\\n########################################################################')"]},{"cell_type":"markdown","id":"658af74c","metadata":{"id":"658af74c"},"source":["**Concatenate list of prortional data and create X side of the dataset**"]},{"cell_type":"code","execution_count":null,"id":"051da949","metadata":{"id":"051da949","outputId":"f977a0e9-4551-409d-f2c1-97b92b4cab92"},"outputs":[{"name":"stdout","output_type":"stream","text":["x_train: <class 'numpy.ndarray'> 1638400 (1600, 32, 32)\n","x_test: <class 'numpy.ndarray'> 409600 (400, 32, 32)\n"]}],"source":["def load_construct_proportional_x_sets():\n","    print('\\n########################################################################')\n","    start = time()\n","    print (\"Loading X dataset\")\n","    file_name=\"config-0\"+str(configuration_number)+\"-\"+y_class_label_items[0]+\"-simulationsize\"+str(data_size)+\"-img-chunk.pkl\"\n","    file_name=simulation_directory_path+file_name\n","    event_items_image=load__event_items_chunk(file_name)\n","    \n","    print (\"Loaded: \",file_name)\n","    print(\"Dataset type: \", type(event_items_image))\n","    print(\"Dataset length: \", len(event_items_image))\n","    print('\\n########################################################################')\n","    print('Construncting proportional train and test sets by events'' images')\n","    (x_train,x_test)=get_x_train_test_data_by_proportion(slice_total,slice_train,event_items_image)\n","    print(\"x_train:\",type(x_train), x_train.size, x_train.shape)\n","    print(\"x_test:\",type(x_test), x_test.size, x_test.shape)\n","    print('\\n########################################################################')\n","    elapsed = time() - start\n","    print('Loading/Proportionalizing x Dataset Elapsed %.3f seconds.' % elapsed)\n","    print('\\n########################################################################')\n","    return (x_train,x_test)\n","(x_train,x_test)=load_construct_proportional_x_sets()\n","\n","\n","print(\"x_train:\",type(x_train), x_train.size, x_train.shape)\n","print(\"x_test:\",type(x_test), x_test.size, x_test.shape)"]},{"cell_type":"markdown","id":"27dd49e0","metadata":{"id":"27dd49e0"},"source":["**Building Y side of the dataset**"]},{"cell_type":"code","execution_count":null,"id":"9682882a","metadata":{"id":"9682882a","outputId":"10a8d324-3b02-49d7-e055-6509bd1b7b62"},"outputs":[{"name":"stdout","output_type":"stream","text":["y_train: <class 'numpy.ndarray'> 1600 (1600,)\n","y_test: <class 'numpy.ndarray'> 400 (400,)\n"]}],"source":["def dataset_y_builder(y_size,y_class_label_items):\n","    class_size=int(y_size/len(y_class_label_items))\n","    y=[]\n","    for class_label_item in y_class_label_items:\n","        y = np.append (y, [class_label_item]*class_size)\n","    return y\n","\n","def build_y_train_test_data_by_proportion(slice_total,slice_train,y_class_label_items, data_size):\n","    train_size=get_data_splitting_index(slice_total,slice_train,data_size)\n","    test_size=data_size-train_size\n","    y_train=dataset_y_builder(train_size,y_class_label_items)\n","    y_test=dataset_y_builder(test_size,y_class_label_items)\n","    return (y_train,y_test)\n","\n","\n","# data_size=2000\n","# y_class_label_items=['MVAC','MLBT']\n","def construct_proportional_y_set():\n","    print('\\n########################################################################')\n","    start = time()\n","    print('Building Y side of the dataset')\n","    (y_train,y_test)=build_y_train_test_data_by_proportion(slice_total,slice_train,y_class_label_items, data_size)\n","    print(\"y_train:\",type(y_train), y_train.size, y_train.shape)\n","    print(\"y_test:\",type(y_test), y_test.size, y_test.shape)\n","    print('\\n########################################################################')\n","    elapsed = time() - start\n","    print('Proportionalizing y Dataset Elapsed %.3f seconds.' % elapsed)\n","    print('\\n########################################################################')\n","    return (y_train,y_test)\n","(y_train,y_test)=construct_proportional_y_set()"]},{"cell_type":"markdown","id":"4acb6609","metadata":{"id":"4acb6609"},"source":["**Saving Constructed Benchmark Dataset as a file**"]},{"cell_type":"code","execution_count":null,"id":"5a663f75","metadata":{"id":"5a663f75"},"outputs":[],"source":["def store_into_dataset_file():\n","    file_name=\"config-0\"+str(configuration_number)+\"-\"+y_class_label_items[0]+\"-simulationsize\"+str(data_size)+\"-dataset.pkl\"\n","    file_name=simulation_directory_path+file_name\n","    # file_name=y_class_label_items[0]+\"-simulationsize\"+str(data_size)+\"-dataset.pkl\"\n","    # file_name=simulation_directory_path+file_name\n","\n","    print('\\n########################################################################')\n","    print('Saving Constructed Benchmark Dataset as a file')\n","    dataset=((x_train,y_train),(x_test,y_test))\n","    save_dataset(file_name,dataset)\n","    print('\\n########################################################################')\n","store_into_dataset_file()"]},{"cell_type":"code","execution_count":null,"id":"9d3d7292","metadata":{"id":"9d3d7292"},"outputs":[],"source":["sys.stdout.close()"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"jetscape-ml-tensorflow-nn-dataset-builder.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3.7.6 ('tensorflow_env')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"vscode":{"interpreter":{"hash":"ada828d16365d2b22d3899327f52f8feba3feb56b4fde7279c1cd0b9201605e0"}}},"nbformat":4,"nbformat_minor":5}
