{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade tensorflow\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1,'/wsu/home/gy/gy40/gy4065/hm.jetscapeml.source')\n",
    "sys.path.insert(1,'/content/drive/My Drive/Projects/110_JetscapeMl/hm.jetscapeml.source')\n",
    "sys.path.insert(1,'/content/drive/MyDrive/Projects/110_JetscapeMl/hm.jetscapeml.source')\n",
    "sys.path.insert(1,'/g/My Drive/Projects/110_JetscapeMl/hm.jetscapeml.source')\n",
    "sys.path.insert(1,'G:\\\\My Drive\\\\Projects\\\\110_JetscapeMl\\\\hm.jetscapeml.source')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "########################################################################\n",
      "Checking the running platforms and setting the directory path\n",
      "\n",
      "Python version: 3.11.5\n",
      "OS: Windows\n",
      "OS version: 10\n",
      "running on Colab: False\n",
      "Dataset Directory Path: D:\\Projects\\110_JetscapeMl\\hm.jetscapeml.data\\\n",
      "Simulation Results Path: D:\\Projects\\110_JetscapeMl\\hm.jetscapeml.data\\simulation_results\\\n",
      "########################################################################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from jet_ml_dataset_builder.jet_ml_dataset_builder_utilities import set_directory_paths\n",
    "dataset_directory_path, simulation_directory_path = set_directory_paths()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregatring all parameters values\n",
      "label_items:\n",
      " {'y_class_label_items': ['MMAT', 'MLBT'], 'alpha_s_items': [0.2, 0.3, 0.4], 'q0_items': [1.5, 2.0, 2.5]}\n",
      "Building required params for the loading the dataset file\n",
      "labels_str:\n",
      " {'class_labels_str': 'MMAT_MLBT', 'alpha_s_items_str': '0.2_0.3_0.4', 'q0_items_str': '1.5_2.0_2.5'}\n"
     ]
    }
   ],
   "source": [
    "from jet_ml_dataset_builder.jet_ml_dataset_builder_utilities import get_labels_str\n",
    "label_str_dict=get_labels_str()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the whole dataset\n",
      "dataset.x: <class 'numpy.ndarray'> 1024000 (1000, 32, 32)\n",
      "dataset.y: <class 'numpy.ndarray'> 3000 (1000, 3)\n"
     ]
    }
   ],
   "source": [
    "from jet_ml_dataset_builder.jet_ml_dataset_builder_utilities import load_dataset\n",
    "size=1000\n",
    "\n",
    "dataset_file_name = f\"jet_ml_benchmark_config_01_to_09_alpha_{label_str_dict['alpha_s_items_str']}_q0_{label_str_dict['q0_items_str']}_{label_str_dict['class_labels_str']}_size_{size}_shuffled.pkl\"\n",
    "\n",
    "dataset_file_name = dataset_directory_path + dataset_file_name\n",
    "print(\"Loading the whole dataset\")\n",
    "dataset = load_dataset(dataset_file_name, has_test=False)\n",
    "(dataset_x, dataset_y) = dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "No conversion path for dtype: dtype('<U32')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 28\u001b[0m\n\u001b[0;32m     25\u001b[0m hdf5_file\u001b[38;5;241m.\u001b[39mcreate_dataset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m'\u001b[39m, data\u001b[38;5;241m=\u001b[39mimages, compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgzip\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Create datasets for labels\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m hdf5_file\u001b[38;5;241m.\u001b[39mcreate_dataset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m, data\u001b[38;5;241m=\u001b[39mlabels)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Store metadata as attributes or separate datasets\u001b[39;00m\n\u001b[0;32m     31\u001b[0m metadata_group \u001b[38;5;241m=\u001b[39m hdf5_file\u001b[38;5;241m.\u001b[39mcreate_group(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\h5py\\_hl\\group.py:183\u001b[0m, in \u001b[0;36mGroup.create_dataset\u001b[1;34m(self, name, shape, dtype, data, **kwds)\u001b[0m\n\u001b[0;32m    180\u001b[0m         parent_path, name \u001b[38;5;241m=\u001b[39m name\u001b[38;5;241m.\u001b[39mrsplit(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    181\u001b[0m         group \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequire_group(parent_path)\n\u001b[1;32m--> 183\u001b[0m dsid \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mmake_new_dset(group, shape, dtype, data, name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    184\u001b[0m dset \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mDataset(dsid)\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dset\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\h5py\\_hl\\dataset.py:86\u001b[0m, in \u001b[0;36mmake_new_dset\u001b[1;34m(parent, shape, dtype, data, name, chunks, compression, shuffle, fletcher32, maxshape, compression_opts, fillvalue, scaleoffset, track_times, external, track_order, dcpl, dapl, efile_prefix, virtual_prefix, allow_unknown_filter, rdcc_nslots, rdcc_nbytes, rdcc_w0)\u001b[0m\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     85\u001b[0m         dtype \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39mdtype(dtype)\n\u001b[1;32m---> 86\u001b[0m     tid \u001b[38;5;241m=\u001b[39m h5t\u001b[38;5;241m.\u001b[39mpy_create(dtype, logical\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m# Legacy\u001b[39;00m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m((compression, shuffle, fletcher32, maxshape, scaleoffset)) \u001b[38;5;129;01mand\u001b[39;00m chunks \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n",
      "File \u001b[1;32mh5py\\\\h5t.pyx:1663\u001b[0m, in \u001b[0;36mh5py.h5t.py_create\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mh5py\\\\h5t.pyx:1687\u001b[0m, in \u001b[0;36mh5py.h5t.py_create\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mh5py\\\\h5t.pyx:1753\u001b[0m, in \u001b[0;36mh5py.h5t.py_create\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: No conversion path for dtype: dtype('<U32')"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "\n",
    "# Assume dataset is a dictionary with keys 'x' for images and 'y' for labels\n",
    "images = dataset_x  # This should be a (1000, 32, 32) NumPy array\n",
    "labels = dataset_y  # This should be a (1000, 3) NumPy array\n",
    "\n",
    "# Step 2: Prepare metadata\n",
    "metadata = {\n",
    "    'eloss': ['MMAT', 'MLBT'],\n",
    "    'alpha_s_items': [0.2, 0.3, 0.4],\n",
    "    'q0_items': [1.0, 1.5, 2.0, 2.5]\n",
    "}\n",
    "\n",
    "dataset_file_name = f\"jet_ml_benchmark_config_01_to_09_alpha_{label_str_dict['alpha_s_items_str']}_q0_{label_str_dict['q0_items_str']}_{label_str_dict['class_labels_str']}_size_{size}_shuffled.h5\"\n",
    "\n",
    "dataset_file_name = dataset_directory_path + dataset_file_name\n",
    "# Step 3: Create an HDF5 file\n",
    "hdf5_file = h5py.File(dataset_file_name, 'w')\n",
    "\n",
    "# Step 4: Write data to HDF5\n",
    "# Create a dataset for images\n",
    "hdf5_file.create_dataset('images', data=images, compression='gzip')\n",
    "\n",
    "# Create datasets for labels\n",
    "hdf5_file.create_dataset('labels', data=labels)\n",
    "\n",
    "# Store metadata as attributes or separate datasets\n",
    "metadata_group = hdf5_file.create_group('metadata')\n",
    "metadata_group.attrs['eloss'] = np.string_(metadata['eloss'])  # Convert list to bytes for HDF5 compatibility\n",
    "metadata_group.attrs['alpha_s'] = metadata['alpha_s_items']\n",
    "metadata_group.attrs['q_0'] = metadata['q0_items']\n",
    "\n",
    "# Optionally, store metadata as separate datasets if needed for clarity\n",
    "# metadata_group.create_dataset('eloss', data=np.string_(metadata['eloss']))\n",
    "# metadata_group.create_dataset('alpha_s_items', data=metadata['alpha_s_items'])\n",
    "# metadata_group.create_dataset('q0_items', data=metadata['q0_items'])\n",
    "\n",
    "# Step 5: Close the HDF5 file\n",
    "hdf5_file.close()\n",
    "\n",
    "print(\"Conversion to HDF5 completed successfully.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
