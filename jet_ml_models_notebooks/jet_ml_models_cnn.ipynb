{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1,'/wsu/home/gy/gy40/gy4065/hm.jetscapeml.source')\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Loading/Installing Package => Begin\\n\\n')\n",
    "\n",
    "import jet_ml_dataset_builder.jet_ml_dataset_builder_utilities as util\n",
    "\n",
    "print('\\n########################################################################')\n",
    "print('Checking the running platforms\\n')\n",
    "\n",
    "from jet_ml_dataset_builder.jet_ml_dataset_builder_utilities import set_directory_paths\n",
    "# Call the function and retrieve the dataset_directory_path and simulation_directory_path\n",
    "dataset_directory_path, simulation_directory_path = set_directory_paths()\n",
    "\n",
    "# Access the dataset_directory_path and simulation_directory_path\n",
    "print(\"Dataset Directory Path:\", dataset_directory_path)\n",
    "print(\"Simulation Directory Path:\", simulation_directory_path)\n",
    "print('########################################################################\\n')\n",
    "\n",
    "\n",
    "print('\\nLoading/Installing Package => End\\n\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jet_ml_dataset_builder.jet_ml_dataset_builder_utilities import  parse_parameters\n",
    "\n",
    "# Call the function and retrieve the tokenized parameters\n",
    "tokenized_arguments, tokenized_values = parse_parameters()\n",
    "\n",
    "# Access the tokenized arguments and values\n",
    "print(\"Tokenized Arguments:\")\n",
    "for argument in tokenized_arguments:\n",
    "    print(argument)\n",
    "\n",
    "print(\"\\nTokenized Values:\")\n",
    "for argument, value in tokenized_values.items():\n",
    "    print(f\"{argument}: {value}\")\n",
    "\n",
    "y_class_label_items=['MMAT','MLBT']\n",
    "alpha_s_items=[0.2 ,0.3 ,0.4]\n",
    "q0_items=[1.5 ,2.0 ,2.5]\n",
    "\n",
    "print(\"y_class_label_items:\",y_class_label_items)\n",
    "print(\"alpha_s_items:\",alpha_s_items)\n",
    "print(\"q0_items:\",q0_items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Building required params for the loading the dataset file\")\n",
    "\n",
    "class_labels_str = '_'.join(y_class_label_items)\n",
    "alpha_s_items_str='_'.join(map(str, alpha_s_items))\n",
    "q0_items_str='_'.join(map(str, q0_items))\n",
    "total_size=9*1200000\n",
    "# for shuffled_y_processed\n",
    "# dataset_file_name = f\"jet_ml_benchmark_config_01_to_09_alpha_{alpha_s_items_str}_q0_{q0_items_str}_{class_labels_str}_size_{total_size}_split_train_datasets/train_split_0.pkl\"\n",
    "# for shuffled\n",
    "dataset_file_name = f\"jet_ml_benchmark_config_01_to_09_alpha_{alpha_s_items_str}_q0_{q0_items_str}_{class_labels_str}_size_{1000}_shuffled.pkl\"\n",
    "# dataset_file_name = f\"jet_ml_benchmark_config_01_to_09_alpha_{alpha_s_items_str}_q0_{q0_items_str}_{class_labels_str}_size_{10000}_shuffled.pkl\"\n",
    "# dataset_file_name = f\"jet_ml_benchmark_config_01_to_09_alpha_{alpha_s_items_str}_q0_{q0_items_str}_{class_labels_str}_size_{100000}_shuffled.pkl\"\n",
    "# dataset_file_name = f\"jet_ml_benchmark_config_01_to_09_alpha_{alpha_s_items_str}_q0_{q0_items_str}_{class_labels_str}_size_{1000000}_shuffled.pkl\"\n",
    "# dataset_file_name = f\"jet_ml_benchmark_config_01_to_09_alpha_{alpha_s_items_str}_q0_{q0_items_str}_{class_labels_str}_size_{total_size}_shuffled.pkl\"\n",
    "\n",
    "dataset_file_name=simulation_directory_path+dataset_file_name\n",
    "print(\"dataset_file_name:\",dataset_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from jet_ml_dataset_builder.jet_ml_dataset_builder_utilities import load_dataset\n",
    "# dataset=load_dataset(dataset_file_name,has_test=False)\n",
    "# # ((x_train, y_train),(x_test,y_test))=dataset\n",
    "# # dataset_x=[x_train,x_test]\n",
    "# # dataset_y=[x_test,y_test]\n",
    "# (dataset_x, dataset_y) = dataset\n",
    "# print(\"dataset.x:\",type(dataset_x), dataset_x.size, dataset_x.shape)\n",
    "# print(\"dataset.y:\",type(dataset_y), dataset_y.size,dataset_y.shape)\n",
    "# # print(\"dataset y_train values:\\n\", dataset_x[1:10])\n",
    "# print(\"dataset y_test values:\\n\", dataset_y[1:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The first column of `dataset_y` is extracted (`dataset_y_binary`) for binary classification.\n",
    "- The dataset is split into training and testing sets using `train_test_split`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Assuming x and y are defined\n",
    "# # x should be a 2D array (e.g., (1000, 32*32))\n",
    "# # y should be a 2D array with three columns (e.g., (1000, 3))\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# test_size = 0.1  # Adjust the test_size as needed\n",
    "# # Extract the first column for binary classification\n",
    "# dataset_y_binary = dataset_y[:, 0]\n",
    "\n",
    "# # Split the dataset into training and testing sets\n",
    "# x_train, x_test, y_train, y_test = train_test_split(dataset_x, dataset_y_binary, test_size=test_size, random_state=42)\n",
    "\n",
    "# # Display the shapes of the training and test datasets\n",
    "# print(\"Training set shapes - x:\", x_train.shape, \" y:\", y_train.shape)\n",
    "# print(\"Test set shapes - x:\", x_test.shape, \" y:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from jet_ml_dataset_builder.jet_ml_dataset_builder_utilities import load_dataset\n",
    "# Function to load datasets of different sizes\n",
    "def get_dataset(size):\n",
    "\n",
    "    dataset_file_name = f\"jet_ml_benchmark_config_01_to_09_alpha_{alpha_s_items_str}_q0_{q0_items_str}_{class_labels_str}_size_{size}_shuffled.pkl\"\n",
    "    \n",
    "    dataset_file_name=simulation_directory_path+dataset_file_name\n",
    "    print(\"dataset_file_name:\",dataset_file_name)\n",
    "    \n",
    "    dataset=load_dataset(dataset_file_name,has_test=False)\n",
    "    (dataset_x, dataset_y) = dataset\n",
    "    # Extract the first column for binary classification\n",
    "    dataset_y = dataset_y[:, 0]\n",
    "    print(\"dataset.x:\",type(dataset_x), dataset_x.size, dataset_x.shape)\n",
    "    print(\"dataset.y:\",type(dataset_y), dataset_y.size,dataset_y.shape)\n",
    "    return dataset_x, dataset_y\n",
    "\n",
    "# Function to train and evaluate classifiers\n",
    "def train_and_evaluate_classifier(model, x_train, y_train, x_test, y_test):\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam',\n",
    "                loss='binary_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "    # Train the model\n",
    "    model.fit(x_train, y_train, epochs=10, validation_split=0.2)\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    y_pred_probs = model.predict(x_test)\n",
    "    y_pred = (y_pred_probs > 0.5).astype(int).flatten()  # Assuming threshold of 0.5 for binary classification\n",
    "\n",
    "    # Evaluate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    return accuracy, cm\n",
    "\n",
    "# # Sizes of datasets\n",
    "dataset_sizes = [1000]\n",
    "# dataset_sizes = [1000, 10000]\n",
    "# dataset_sizes = [1000, 10000, 100000, 1000000]\n",
    "\n",
    "def model_cnn():\n",
    "    # Build the CNN model\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 1)),\n",
    "        keras.layers.MaxPooling2D((2, 2)),\n",
    "        keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        keras.layers.MaxPooling2D((2, 2)),\n",
    "        keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        keras.layers.Flatten(),\n",
    "        keras.layers.Dense(64, activation='relu'),\n",
    "        keras.layers.Dense(1, activation='sigmoid')  # Binary classification, so using sigmoid activation\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "# Classifiers\n",
    "classifiers = {\n",
    "    'CNN': model_cnn(),\n",
    "}\n",
    "\n",
    "# Results storage\n",
    "results = []\n",
    "\n",
    "# Loop through different dataset sizes\n",
    "for size in dataset_sizes:\n",
    "    # Generate dataset\n",
    "    x, y = get_dataset(size)\n",
    "    \n",
    "\n",
    "    # Assuming dataset_y is a NumPy array or a Pandas Series with string labels\n",
    "    label_encoder = LabelEncoder()\n",
    "    dataset_y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "    # Print the mapping of original labels to encoded labels\n",
    "    label_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
    "    print(\"Label Mapping:\", label_mapping)\n",
    "\n",
    "    # Now, dataset_y_encoded can be used for training your CNN\n",
    "\n",
    "    # Split dataset\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Calculate the maximum value in the training set\n",
    "    max_value = np.max(x_train)\n",
    "\n",
    "    # Normalize the pixel values to be between 0 and 1\n",
    "    x_train, x_test = x_train / max_value, x_test / max_value\n",
    "\n",
    "    # Loop through classifiers\n",
    "    for clf_name, clf in classifiers.items():\n",
    "        # Train and evaluate classifier\n",
    "        accuracy, cm = train_and_evaluate_classifier(clf, x_train, y_train, x_test, y_test)\n",
    "        results.append({'Dataset Size': size, 'Classifier': clf_name, 'Accuracy': accuracy, 'Confusion Matrix': cm})\n",
    "        # Create a DataFrame from results\n",
    "        df_results = pd.DataFrame(results)\n",
    "\n",
    "        # Save the DataFrame to a text file\n",
    "        df_results.to_csv('binary_classification_results.txt', index=False, sep='\\t')\n",
    "\n",
    "\n",
    "        # Plotting with different markers for each classifier\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        markers = ['o', 's', '^', 'D', 'v']  # You can customize the markers here\n",
    "\n",
    "        for clf_name, group, marker in zip(classifiers.keys(), df_results.groupby('Classifier'), markers):\n",
    "            plt.plot(group[1]['Dataset Size'], group[1]['Accuracy'], label=clf_name, marker=marker)\n",
    "\n",
    "        # # Plotting\n",
    "        # plt.figure(figsize=(12, 8))\n",
    "        # for clf_name, group in df_results.groupby('Classifier'):\n",
    "        #     plt.plot(group['Dataset Size'], group['Accuracy'], label=clf_name, marker='o')\n",
    "\n",
    "        plt.title('Binary Classification Accuracy for Different Dataset Sizes')\n",
    "        plt.xlabel('Dataset Size')\n",
    "        plt.xscale('log')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend()\n",
    "\n",
    "        # Save the plot with high resolution (300 dpi)\n",
    "        plt.savefig('binary_classification_accuracy_plot.png', dpi=300)\n",
    "        plt.show()\n",
    "\n",
    "        # Display results in a table\n",
    "        print(df_results.pivot_table(index='Dataset Size', columns='Classifier', values='Accuracy'))\n",
    "\n",
    "        # Define the module labels\n",
    "        module_labels = ['MMATTER', 'MLBT']\n",
    "\n",
    "        # Save confusion matrices\n",
    "        for index, row in df_results.iterrows():\n",
    "            clf_name = row['Classifier']\n",
    "            dataset_size = row['Dataset Size']\n",
    "            cm = row['Confusion Matrix']\n",
    "            plt.figure()\n",
    "            plt.imshow(cm, interpolation='nearest', cmap='Oranges') #plt.cm.Blue\n",
    "\n",
    "            # Annotate each cell with the value\n",
    "            for i in range(len(module_labels)):\n",
    "                for j in range(len(module_labels)):\n",
    "                    plt.text(j, i, str(cm[i, j]), ha='center', va='center', color='black')\n",
    "\n",
    "            plt.title(f'Confusion Matrix - {clf_name} - {dataset_size} samples')\n",
    "            plt.colorbar()\n",
    "            # Set tick labels\n",
    "            plt.xticks(np.arange(len(module_labels)), module_labels)\n",
    "            plt.yticks(np.arange(len(module_labels)), module_labels)\n",
    "\n",
    "            plt.xlabel('Predicted Label')\n",
    "            plt.ylabel('True Label')\n",
    "            # Remove tick marks\n",
    "            plt.tick_params(axis='both', which='both', bottom=False, top=False, left=False, right=False)\n",
    "            plt.savefig(f'confusion_matrix_{clf_name}_{dataset_size}.png', dpi=300, bbox_inches='tight')\n",
    "            plt.show()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
